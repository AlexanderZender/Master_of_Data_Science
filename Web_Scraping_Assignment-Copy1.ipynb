{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#Question 1\n",
    "# design and implement the solution to crawl all the unique URLs for the detailed\n",
    "#publication pages. \n",
    "############################################\n",
    "\n",
    "##############################\n",
    "#Get page and import libraries\n",
    "##############################\n",
    "#import beautiful soup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#Get the page which has all urls within it\n",
    "MainUrl = \"https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_characteranimation.htm\"\n",
    "page = requests.get(MainUrl)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"pbl_tvcg2021motionsynthesis.htm\"><img alt=\"Webpage\" class=\"ImgIconPublication\" src=\"publications/tvcg2021deeplearning/images/icon.png\" style=\"display:block\"/></a>\n",
      "pbl_tvcg2021motionsynthesis.htm\n"
     ]
    }
   ],
   "source": [
    "# Getting the links through the divs which hold images\n",
    "body = soup.find(\"body\", class_=\"PageDefault\")\n",
    "imageDiv = body.find(\"div\", class_ =\"ImgIconPublicationDiv\")\n",
    "#find the beginning of the href\n",
    "url = imageDiv.find(\"a\")\n",
    "print(url)\n",
    "#check we can find the url\n",
    "if (url != None):\n",
    "    urlPublication = url[\"href\"]\n",
    "    print(urlPublication)\n",
    "else:\n",
    "    print(\"Cannot find the link in the publication page!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pbl_tvcg2021motionsynthesis.htm\n",
      "pbl_tcsvt2021motionprediction.htm\n",
      "pbl_cag2021reactivemotion.htm\n",
      "pbl_cavw2019emotionanalysis.htm\n",
      "pbl_mig2019dancesynthesis.htm\n",
      "pbl_ace2017dancedj3d.htm\n",
      "pbl_d2at2017emotionsynthesis.htm\n",
      "pbl_siggraph2016dancegeneration.htm\n",
      "pbl_hhm2016depthsensor.htm\n",
      "pbl_pg2015latticemodel.htm\n",
      "pbl_casa2014variationsynthesis.htm\n",
      "pbl_pg2013topologyik.htm\n",
      "pbl_cavw2013preparationsynthesis.htm\n",
      "pbl_casa2013preparationsynthesis.htm\n",
      "pbl_tvcg2012interactionsynthesis.htm\n",
      "pbl_vrst2012physicalmodel.htm\n",
      "pbl_patent2010interactionpatent.htm\n",
      "pbl_thesis2010interactioncharacter.htm\n",
      "pbl_casa2009angularmomentum.htm\n",
      "pbl_siggraphasia2008interactionpatches.htm\n",
      "pbl_i3d2008interactionavatars.htm\n",
      "pbl_mig2008interactioncharacters.htm\n",
      "pbl_vrst2007interactionsingly.htm\n",
      "pbl_sca2006fightinggametree.htm\n",
      "pbl_arxiv2006gametree.htm\n"
     ]
    }
   ],
   "source": [
    "#define variable which finds the body and the class we are interested in\n",
    "body = soup.find(\"body\", class_=\"PageDefault\")\n",
    "#Create an empty List to store the urls\n",
    "urlPublications = []\n",
    "#Define variable that finds the title of the movie(the section we are interested in) through tag and class\n",
    "imageDivs = body.find_all(\"div\", class_ =\"ImgIconPublicationDiv\")\n",
    "\n",
    "for imageDiv in imageDivs:\n",
    "    url = imageDiv.find(\"a\")\n",
    "    if (url != None):\n",
    "        urlPublications.append(url[\"href\"])\n",
    "        print(urlPublications[len(urlPublications)-1])\n",
    "    else:\n",
    "        print(\"Cannot find the link in the publication page!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tvcg2021motionsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tcsvt2021motionprediction.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_cag2021reactivemotion.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_cavw2019emotionanalysis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_mig2019dancesynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_ace2017dancedj3d.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_d2at2017emotionsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_siggraph2016dancegeneration.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_hhm2016depthsensor.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_pg2015latticemodel.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_casa2014variationsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_pg2013topologyik.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_cavw2013preparationsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_casa2013preparationsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tvcg2012interactionsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_vrst2012physicalmodel.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_patent2010interactionpatent.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_thesis2010interactioncharacter.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_casa2009angularmomentum.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_siggraphasia2008interactionpatches.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_i3d2008interactionavatars.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_mig2008interactioncharacters.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_vrst2007interactionsingly.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_sca2006fightinggametree.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_arxiv2006gametree.htm\n"
     ]
    }
   ],
   "source": [
    "# Getting the host name\n",
    "hostUrl = \"https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_characteranimation.htm\"\n",
    "urlWithoutHttps = hostUrl[8:] # \"www.imdb.com/chart/moviemeter/?sort=i\n",
    "iFirstSlash = urlWithoutHttps.find(\"/\")\n",
    "hostname = \"/hubert.shum/comp42315/\"\n",
    "urlFolder = hostUrl[:8] + urlWithoutHttps[:iFirstSlash]\n",
    "#create the global url\n",
    "urlGlobal = []\n",
    "for urlPublication in urlPublications:\n",
    "    urlGlobal.append(urlFolder + hostname + urlPublication)\n",
    "    print(urlGlobal[len(urlGlobal)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tvcg2021motionsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tcsvt2021motionprediction.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_cag2021reactivemotion.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_cavw2019emotionanalysis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_mig2019dancesynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_ace2017dancedj3d.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_d2at2017emotionsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_siggraph2016dancegeneration.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_hhm2016depthsensor.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_pg2015latticemodel.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_casa2014variationsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_pg2013topologyik.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_cavw2013preparationsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_casa2013preparationsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tvcg2012interactionsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_vrst2012physicalmodel.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_patent2010interactionpatent.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_thesis2010interactioncharacter.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_casa2009angularmomentum.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_siggraphasia2008interactionpatches.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_i3d2008interactionavatars.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_mig2008interactioncharacters.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_vrst2007interactionsingly.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_sca2006fightinggametree.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_arxiv2006gametree.htm\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "#Question 2\n",
    "#design and implement the solution to crawl all the text-based information of each\n",
    "#publication from the website, to convert such information into a suitable data format, and to\n",
    "#store it in a data file\n",
    "##############################\n",
    "\n",
    "for link in urlGlobal:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spatio-temporal Manifold Learning for Human Motions via Long-horizon Modeling', 'A Quadruple Diffusion Convolutional Recurrent Network for Human Motion Prediction', 'GAN-based Reactive Motion Synthesis with Class-aware Discriminators for Human-human Interaction', 'A Generic Framework for Editing and Synthesizing Multimodal Data with Relative Emotion Strength', 'Automatic Sign Dance Synthesis from Gesture-based Sign Language', 'DanceDJ: A 3D Dance Animation Authoring System for Live Performance', 'Synthesizing Motion with Relative Emotion Strength', 'Automatic Dance Generation System Considering Sign Language Information', 'Depth Sensor based Facial and Body Animation Control', 'Multi-layer Lattice Model for Real-Time Dynamic Character Deformation', 'Human Motion Variation Synthesis with Multivariate Gaussian Processes', 'Topology Aware Data-Driven Inverse Kinematics', 'Natural Preparation Behavior Synthesis', 'Preparation Behaviour Synthesis with Reinforcement Learning', 'Simulating Multiple Character Interactions with Collaborative and Adversarial Goals', 'Real-time Physical Modelling of Character Movements with Microsoft Kinect', 'Manufacturing Video Graphics', 'Simulating Interactions Among Multiple Characters', 'Angular Momentum Guided Motion Concatenation', 'Interaction Patches for Multi-Character Animation', 'Simulating Interactions of Avatars in High Dimensional State Space', 'Simulating Interactions of Characters', 'Simulating Competitive Interactions using Singly Captured Motions', 'Generating Realistic Fighting Scenes by Game Tree', 'Technical Note: Generating Realistic Fighting Scenes by Game Tree']\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#Title\n",
    "###################################\n",
    "#Define empty list for the titles\n",
    "Titles = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    #find the h1 (Only the titles are contained in H1 tags in this case) \n",
    "    titleH1 = soup.find(\"h1\")\n",
    "    title = titleH1.text\n",
    "    #print(title)\n",
    "    Titles.append(title)\n",
    "\n",
    "print(Titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data-driven modeling of human motions is ubiquitous in computer graphics and computer vision applications, such as synthesizing realistic motions or recognizing actions. Recent research has shown that such problems can be approached by learning a natural motion manifold using deep learning on a large amount data, to address the shortcomings of traditional data-driven approaches. However, previous deep learning methods can be sub-optimal for two reasons. First, the skeletal information has not been fully utilized for feature extraction. Unlike images, it is difficult to define spatial proximity in skeletal motions in the way that deep networks can be applied for feature extraction. Second, motion is time-series data with strong multi-modal temporal correlations between frames. On the one hand, a frame could be followed by several candidate frames leading to different motions; on the other hand, long-range dependencies exist where a number of frames in the beginning correlate to a number of frames later.  Ineffective temporal modeling would either under-estimate the multi-modality and variance, resulting in featureless mean motion or over-estimate them resulting in jittery motions, which is a major source of visual artifacts. In this paper, we propose a new deep network to tackle these challenges by creating a natural motion manifold that is versatile for many applications. The network has a new spatial component for feature extraction. It is also equipped with a new batch prediction model that predicts a large number of frames at once, such that long-term temporally-based objective functions can be employed to correctly learn the motion multi-modality and variances. With our system, long-duration motions can be predicted/synthesized using an open-loop setup where the motion retains the dynamics accurately. It can also be used for denoising corrupted motions and synthesizing new motions with given control signals. We demonstrate that our system can create superior results comparing to existing work in multiple applications.', 'Recurrent neural network (RNN) has become popular for human motion prediction thanks to its ability to capture temporal dependencies. However, it has limited capacity in modeling the complex spatial relationship in the human skeletal structure. In this work, we present a novel diffusion convolutional recurrent predictor for spatial and temporal movement forecasting, with multi-step random walks traversing bidirectionally along an adaptive graph to model interdependency among body joints. In the temporal domain, existing methods rely on a single forward predictor with the produced motion deflecting to the drift route, which leads to error accumulations over time. We propose to supplement the forward predictor with a forward discriminator to alleviate such motion drift in the long term under adversarial training. The solution is further enhanced by a backward predictor and a backward discriminator to effectively reduce the error, such that the system can also look into the past to improve the prediction at early frames. The two-way spatial diffusion convolutions and two-way temporal predictors together form a quadruple network. Furthermore, we train our framework by modeling the velocity from observed motion dynamics instead of static poses to predict future movements that effectively reduces the discontinuity problem at early prediction. Our method outperforms the state of the arts on both 3D and 2D datasets, including the Human3.6M, CMU Motion Capture and Penn Action datasets. The results also show that our method correctly predicts both high-dynamic and low-dynamic moving trends with less motion drift.', \"Creating realistic characters that can react to the users' or another character's movement can benefit computer graphics, games and virtual reality hugely. However, synthesizing such reactive motions in human-human interactions is a challenging task due to the many different ways two humans can interact. While there are a number of successful researches in adapting the generative adversarial network (GAN) in synthesizing single human actions, there are very few on modelling human-human interactions. In this paper, we propose a semi-supervised GAN system that synthesizes the reactive motion of a character given the active motion from another character. Our key insights are two-fold. First, to effectively encode the complicated spatial-temporal information of a human motion, we empower the generator with a part-based long short-term memory (LSTM) module, such that the temporal movement of different limbs can be effectively modelled. We further include an attention module such that the temporal significance of the interaction can be learned, which enhances the temporal alignment of the active-reactive motion pair. Second, as the reactive motion of different types of interactions can be significantly different, we introduce a discriminator that not only tells if the generated movement is realistic or not, but also tells the class label of the interaction. This allows the use of such labels in supervising the training of the generator. We experiment with the SBU and the HHOI datasets. The high quality of the synthetic motion demonstrates the effective design of our generator, and the discriminability of the synthesis also demonstrates the strength of our discriminator.\", 'Emotion is considered to be a core element in performances [1]. In computer animation, both body motions and facial expressions are two popular mediums for a character to express the emotion. However, there has been limited research in studying how to effectively synthesize these two types of character movements using different levels of emotion strength with intuitive control, which is difficult to be modelled effectively. In this work, we explore a common model that can be used to represent the emotion for the applications of body motions and facial expressions synthesis. Unlike previous work which encode emotions into discrete motion style descriptors, we propose a continuous control indicator called emotion strength, by controlling which a data-driven approach is presented to synthesize motions with fine control over emotions. Rather than interpolating motion features to synthesize new motion as in existing work, our method explicitly learns a model mapping low-level motion features to the emotion strength. Since the motion synthesis model is learned in the training stage, the computation time required for synthesizing motions at run-time is very low. We further demonstrate the generality of our proposed framework by editing 2D face images using relative emotion strength. As a result, our method can be applied to interactive applications such as computer games, image editing tools and virtual reality applications, as well as offline applications such as animation and movie production. ', 'Automatic dance synthesis has become more and more popular due to the increasing demand in computer games and animations. Existing research generates dance motions without much consideration for the context of the music. In reality, professional dancers make choreography according to the lyrics and music features. In this research, we focus on a particular genre of dance known as sign dance, which combines gesture-based sign language with full body dance motion. We propose a system to automatically generate sign dance from a piece of music and its corresponding sign gesture. The core of the system is a Sign Dance Model trained by multiple regression analysis to represent the correlations between sign dance and sign gesture/music, as well as a set of objective functions to evaluate the quality of the sign dance. Our system can be applied to music visualization, allowing people with hearing difficulties to understand and enjoy music. ', 'Dance is an important component of live performance for expressing emotion and presenting visual context. Human dance performances typically require expert knowledge of dance choreography and professional rehearsal, which are too costly for casual entertainment venues and clubs. Recent advancements in character animation and motion synthesis have made it possible to synthesize virtual 3D dance characters in real-time. The major problem in existing systems is a lack of an intuitive interfaces to control the animation for real-time dance controls. We propose a new system called the DanceDJ to solve this problem. Our system consists of two parts. The first part is an underlying motion analysis system that evaluates motion features including dance features such as the postures and movement tempo, as well as audio features such as the music tempo and structure. As a pre-process, given a dancing motion database, our system evaluates the quality of possible timings to connect and switch different dancing motions. During run-time, we propose a control interface that provides visual guidance. We observe that disk jockeys (DJs) effectively control the mixing of music using the DJ controller, and therefore propose a DJ controller for controlling dancing characters. This allows DJs to transfer their skills from music control to dance control using a similar hardware setup. We map different motion control functions onto the DJ controller, and visualize the timing of natural connection points, such that the DJ can effectively govern the synthesized dance motion. We conducted two user experiments to evaluate the user experience and the quality of the dance character. Quantitative analysis shows that our system performs well in both motion control and simulation quality.', 'With the advancement in motion sensing technology, acquiring high-quality human motions for creating realistic character animation is much easier than before. Since motion data itself is not the main obstacle anymore, more and more effort goes into enhancing the realism of character animation, such as motion styles and control. In this paper, we explore a less studied area: the emotion of motions. Unlike previous work which encode emotions into discrete motion style descriptors, we propose a continuous control indicator called motion strength, by controlling which a data-driven approach is presented to synthesize motions with fine control over emotions. Rather than interpolating motion features to synthesize new motion as in existing work, our method explicitly learns a model mapping low-level motion features to the emotion strength. Since the motion synthesis model is learned in the training stage, the computation time required for synthesizing motions at run-time is very low. As a result, our method can be applied to interactive applications such as computer games and virtual reality applications, as well as offline applications such as animation and movie production.', \"In recent years, thanks to the development of 3DCG animation editing tools (e.g. MikuMikuDance), a lot of 3D character dance animation movies are created by amateur users. However, it is very difficult to create choreography from scratch without any technical knowledge. Shiratori et al. [2006] produced the dance automatic generation system considering rhythm and intensity of dance motions. However, each segment is selected randomly from database, so the generated dance motion has no linguistic or emotional meanings. Takano et al. [2010] produced a human motion generation system considering motion labels. However, they use simple motion labels like 'running' or 'jump', so they cannot generate motions that express emotions. In reality, professional dancers make choreography based on music features or lyrics in music, and express emotion or how they feel in music. In our work, we aim at generating more emotional dance motion easily. Therefore, we use linguistic information in lyrics, and generate dance motion.\", 'Depth sensors have become one of the most popular means of generating human facial and posture information in the past decade. By coupling a depth camera and computer vision based recognition algorithms, these sensors can detect human facial and body features in real time. Such a breakthrough has fused many new research directions in animation creation and control, which also has opened up new challenges. In this chapter, we explain how depth sensors obtain human facial and body information. We then discuss on the main challenge on depth sensor-based systems, which is the inaccuracy of the obtained data, and explain how the problem is tackled. Finally, we point out the emerging applications in the field, in which human facial and body feature modeling and understanding is a key research problem.', 'Due to the recent advancement of computer graphics hardware and software algorithms, deformable characters have become more and more popular in real-time applications such as computer games. While there are mature techniques to generate primary deformation from skeletal movement, simulating realistic and stable secondary deformation such as jiggling of fats remains challenging. On one hand, traditional volumetric approaches such as the finite element method require higher computational cost and are infeasible for limited hardware such as game consoles. On the other hand, while shape matching based simulations can produce plausible deformation in real-time, they suffer from a stiffness problem in which particles either show unrealistic deformation due to high gains, or cannot catch up with the body movement. In this paper, we propose a unified multi-layer lattice model to simulate the primary and secondary deformation of skeleton-driven characters. The core idea is to voxelize the input character mesh into multiple anatomical layers including the bone, muscle, fat and skin. Primary deformation is applied on the bone voxels with lattice-based skinning. The movement of these voxels is propagated to other voxel layers using lattice shape matching simulation, creating a natural secondary deformation. Our multi-layer lattice framework can produce simulation quality comparable to those from other volumetric approaches with a significantly smaller computational cost. It is best to be applied in real-time applications such as console games or interactive animation creation. ', 'Human motion variation synthesis is important for crowd simulation and interactive applications to enhance synthesis quality. In this paper, we propose a novel generative probabilistic model to synthesize variations of human motion. Our key idea is to model the conditional distribution of each joint via a multivariate Gaussian process model, namely semi-parametric latent factor model (SLFM). SLFM can effectively model the correlations between degrees of freedom (DOFs) of joints rather than dealing with each DOF separately as implemented in existing methods. A detailed evaluation is performed to show that the proposed approach can effectively synthesize variations of different types of motions. Motions generated by our method show a richer variation compared with existing ones. Finally, our user study shows that the synthesized motion has a similar level of naturalness to captured human motions. Our method is best applied in computer games and animations to introduce motion variations.', 'Creating realistic human movement is a time consuming and labour intensive task. The major difficulty is that the user has to edit individual joints while maintaining an overall realistic and collision free posture. Previous research suggests the use of data-driven inverse kinematics, such that one can focus on the control of a few joints, while the system automatically composes a natural posture. However, as a common problem of kinematics synthesis, penetration of body parts is difficult to avoid in complex movements. In this paper, we propose a new data-driven inverse kinematics framework that conserves the topology of the synthesizing postures. Our system monitors and regulates the topology changes using the Gauss Linking Integral (GLI), such that penetration can be efficiently prevented. As a result, complex motions with tight body movements, as well as those involving interaction with external objects, can be simulated with minimal manual intervention. Experimental results show that using our system, the user can create high quality human motion in real-time by controlling a few joints using a mouse or a multi-touch screen. The movement generated is both realistic and penetration free. Our system is best applied for interactive motion design in computer animations and games.', 'Humans adjust their movements in advance to prepare for the forthcoming action, resulting in an efficient and smooth transition. However, traditional computer animation approaches such as motion graphs simply concatenates a series of actions without taking into account the following one. In this paper, we propose a new method to produce preparation behaviours using reinforcement learning. As an offline process, the system learns the optimal way to approach a target and prepare for interaction. A scalar value called the level of preparation is introduced, which represents the degree of transition from the initial action to the interacting action. To synthesize the movements of preparation, we propose a customized motion blending scheme based on the level of preparation, which is followed by an optimization framework that adjusts the posture to keep the balance. During run-time, the trained controller drives the character to move to a target with the appropriate level of preparation, resulting in a human-like behaviour. We create scenes in which the character has to move in a complex environment and interacts with objects, such as crawling under and jumping over obstacles while walking. The method is useful not only for computer animation, but also for real-time applications such as computer games, in which the characters need to accomplish a series of tasks in a given environment.', 'When humans perform a series of motions, they prepare for the next motion in advance so as to enhance the response time of their movements. This kind of preparation behaviour results in a natural and smooth transition of the overall movement. In this paper, we propose a new method to synthesize the behaviour using reinforcement learning. To create preparation movements, we propose a customized motion blending algorithm that is governed by a single numerical value, which we called the level of preparation. During the offline process, the system learns the optimal way to approach a target, as well as the realistic behaviour to prepare for interaction considering the level of preparation. At run-time, the trained controller indicates the character to move to a target with the appropriate level of preparation, resulting in human-like movements. We synthesized scenes in which the character has to move in a complex environment and interact with objects, such as a character crawling under and jumping over obstacles while walking. The method is useful not only for computer animation, but also for real-time applications such as computer games, in which the characters need to accomplish a series of tasks in a given environment.', 'This paper proposes a new methodology for synthesizing animations of multiple characters, allowing them to intelligently compete with one another in dense environments, while still satisfying requirements set by an animator. To achieve these two conflicting objectives simultaneously, our method separately evaluates the competition and collaboration of the interactions, integrating the scores to select an action that maximizes both criteria. We extend the idea of min-max search, normally used for strategic games such as chess. Using our method, animators can efficiently produce scenes of dense character interactions such as those in collective sports or martial arts. The method is especially effective for producing animations along story lines, where the characters must follow multiple objectives, while still accommodating geometric and kinematic constraints from the environment.', 'With the advancement of motion tracking hardware such as the Microsoft Kinect, synthesizing human-like characters with real-time captured movements becomes increasingly important. Traditional kinematics and dynamics approaches perform sub-optimally when the captured motion is noisy or even incomplete. In this paper, we proposed a unified framework to control physically simulated characters with live captured motion from Kinect. Our framework can synthesize any posture in a physical environment using external forces and torques computed by a PD controller. The major problem of Kinect is the incompleteness of the captured posture, with some degree of freedom (DOF) missing due to occlusions and noises. We propose to search for a best matched posture from a motion database constructed in a dimensionality reduced space, and substitute the missing DOF to the live captured data. Experimental results show that our method can synthesize realistic character movements from noisy captured motion. The proposed algorithm is computationally efficient and can be applied to a wide variety of interactive virtual reality applications such as motion-based gaming, rehabilitation and sport training.', 'An apparatus comprising: a memory system storing a plurality of sequences, each sequence comprising data for reproducing a different pattern of interactions between a respective plurality of moving characters and storing a combination data structure defining for each sequence connectability of that sequence with other ones of the plurality of sequences; a processor configured to determine pair-wise combination of stored sequences, by selecting sequences for pair-wise combination that are defined as connectable by the stored combination data structure, wherein each pair-wise combination has in common at least one of their respective plurality of moving characters and configured to use determined pair-wise combinations of the stored sequences to produce and output video graphics comprising a series of sequences in which movable characters repetitively interact in different combinations.', 'In this thesis, we attack a challenging problem in the field of character animation: synthesizing interactions among multiple virtual characters in real-time. Although there are heavy demands in the gaming and animation industries, no systemic solution has been proposed due to the difficulties to model the complex behaviors of the characters.We represent the continuous interactions among characters as a discrete Markov Decision Process, and design a general objective function to evaluate the immediate rewards of launching an action. By applying game theory such as tree expansion and min-max search, the optimal actions that benefit the character the most in the future are selected. The simulated characters can interact competitively while achieving the requests from animators cooperatively.Since the interactions between two characters depend on a lot of criteria, it is difficult to exhaustively precompute the optimal actions for all variations of these criteria. We design an off-policy approach that samples and precomputes only meaningful interactions. With the precomputed policy, the optimal movements under different situations can be evaluated in real-time.To simulate the interactions for a large number of characters with minimal computational overhead, we propose a method to precompute short durations of interactions between two characters as connectable patches. The patches are concatenated spatially to generate interactions with multiple characters, and temporally to generate longer interactions. Based on the optional instructions given by the animators, our system automatically applies concatenations to create a huge scene of interacting crowd.We demonstrate our system by creating scenes with high quality interactions. On one hand, our algorithm can automatically generate artistic scenes of interactions such as the fighting scenes in movies that involve hundreds of characters. On the other hand, it can create controllable, intelligent characters that interact with the opponents for real-time applications such as 3D computer games.', 'In this paper, we propose a new method to concatenate two dynamic full-body motions such as punches, kicks and flips by using the angular momentum as a cue. Through the observation of real humans, we have identified two patterns of angular momentum that make the transition of such motions efficient. Based on these observations, we propose a new method to concatenate two full-body motions in a natural manner. Our method is useful for applications where dynamic, full-body motions are required, such as 3D computer games and animations.', 'We propose a data-driven approach to automatically generate a scene where tens to hundreds of characters densely interact with each other. During offline processing, the close interactions between characters are precomputed by expanding a game tree, and these are stored as data structures called interaction patches. Then, during run-time, the system spatio-temporally concatenates the interaction patches to create scenes where a large number of characters closely interact with one another. Using our method, it is possible to automatically or interactively produce animations of crowds interacting with each other in a stylized way. The method can be used for a variety of applications including TV programs, advertisements and movies.', 'Efficient computation of strategic movements is essential to control virtual avatars intelligently in computer games and 3D virtual environments. Such a module is needed to control non-player characters (NPCs) to fight, play team sports or move through a mass crowd. Reinforcement learning is an approach to achieve real-time optimal control. However, the huge state space of human interactions makes it difficult to apply existing learning methods to control avatars when they have dense interactions with other characters. In this research, we propose a new methodology to ef?ciently plan the movements of an avatar interacting with another. We make use of the fact that the subspace of meaningful interactions is much smaller than the whole state space of two avatars. We efficiently collect samples by exploring the subspace where dense interactions between the avatars occur and favor samples that have high connectivity with the other samples. Using the collected samples, a ?nite state machine (FSM) called Interaction Graph is composed. At run-time, we compute the optimal action of each avatar by min-max search or dynamic programming on the Interaction Graph. The methodology is applicable to control NPCs in ?ghting and ball-sports games.', 'It is difficult to create scenes where multiple characters densely interact with each other. Manually creating the motions of characters is time consuming due to the correlation of the movements between the characters. Capturing the motions of multiple characters is also difficult as it requires a huge amount of post-processing of the data. In this paper, we explain the methods we have proposed to simulate close interactions of characters based on singly captured motions. We propose methods to (1) control characters intelligently to cooperatively/competitively interact with the other characters, and (2) generate movements that include close interactions such as tangling the segments with the others by taking into account the topological relationship of the characters.', 'It is difficult to create scenes where multiple avatars are fighting / competing with each other. Manually creating the motions of avatars is time consuming due to the correlation of the movements between the avatars. Capturing the motions of multiple avatars is also difficult as it requires a huge amount of post-processing. In this paper, we propose a new method to generate a realistic scene of avatars densely interacting in a competitive environment. The motions of the avatars are considered to be captured individually, which will increase the easiness of obtaining the data. We propose a new algorithm called the temporal expansion approach which maps the continuous time action plan to a discrete space such that turnbased evaluation methods can be used. As a result, many mature algorithms in game such as the min-max search and alpha?beta pruning can be applied.Using our method, avatars will plan their strategies taking into account the reaction of the opponent. Fighting scenes with multiple avatars are generated to demonstrate the effectiveness of our algorithm. The proposed method can also be applied to other kinds of continuous activities that require strategy planning such as sport games.', 'Recently, there have been a lot of researches to synthesize/edit the motion of a single avatar in the virtual environment. However, there has not been so much work of simulating continuous interactions of multiple avatars such as fighting. In this paper, we propose a new method to generate a realistic fighting scene based on motion capture data. We propose a new algorithm called the temporal expansion approach which maps the continuous time action plan to a discrete causality space such that turn-based evaluation methods can be used. As a result, it is possible to use many mature algorithms available in strategy games such as the Minimax algorithm and α-β pruning. We also propose a method to generate and use an offense/defense table, which illustrates the spatialtemporal relationship of attacks and dodges, to incorporate tactical maneuvers of defense into the scene. Using our method, avatars will plan their strategies taking into account the reaction of the opponent. Fighting scenes with multiple avatars are generated to demonstrate the effectiveness of our algorithm. The proposed method can also be applied to other kinds of continuous activities that require strategy planning such as sport games.', 'Recently, there have been a lot of researches to synthesize/edit the motion of a single avatar in the virtual environment. However, there has not been so much work of simulating continuous interactions of multiple avatars such as fighting. In this paper, we propose a new method to generate a realistic fighting scene based on motion capture data. We propose a new algorithm called the temporal expansion approach which maps the continuous time action plan to a discrete causality space such that turn-based evaluation methods can be used. As a result, it is possible to use many mature algorithms available in strategy games such as the Minimax algorithm and α-β pruning. We also propose a method to generate and use an offense/defense table, which illustrates the spatialtemporal relationship of attacks and dodges, to incorporate tactical maneuvers of defense into the scene. Using our method, avatars will plan their strategies taking into account the reaction of the opponent. Fighting scenes with multiple avatars are generated to demonstrate the effectiveness of our algorithm. The proposed method can also be applied to other kinds of continuous activities that require strategy planning such as sport games.']\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#Abstract\n",
    "###################################\n",
    "#Define empty list for the abstracts\n",
    "Abstracts = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    #find the abstract \n",
    "    abstractDiv = soup.find(\"div\", attrs = {\"style\": \"margin-left: var(--size-marginleft)\"})\n",
    "    #print(abstractDiv) # check this works\n",
    "    abstract = abstractDiv.p.text\n",
    "    #print(abstract)\n",
    "    Abstracts.append(abstract)\n",
    "print(Abstracts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum', 'Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  Shigeo Morishima']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum', 'Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  Shigeo Morishima', 'Liuyang Zhou  Lifeng Shang  Hubert P H Shum  Howard Leung']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum', 'Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  Shigeo Morishima', 'Liuyang Zhou  Lifeng Shang  Hubert P H Shum  Howard Leung', 'Edmond S L Ho  Hubert P H Shum  Yiuming Cheung  P C Yuen']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum', 'Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  Shigeo Morishima', 'Liuyang Zhou  Lifeng Shang  Hubert P H Shum  Howard Leung', 'Edmond S L Ho  Hubert P H Shum  Yiuming Cheung  P C Yuen', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum', 'Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  Shigeo Morishima', 'Liuyang Zhou  Lifeng Shang  Hubert P H Shum  Howard Leung', 'Edmond S L Ho  Hubert P H Shum  Yiuming Cheung  P C Yuen', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum', 'Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  Shigeo Morishima', 'Liuyang Zhou  Lifeng Shang  Hubert P H Shum  Howard Leung', 'Edmond S L Ho  Hubert P H Shum  Yiuming Cheung  P C Yuen', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum', 'Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  Shigeo Morishima', 'Liuyang Zhou  Lifeng Shang  Hubert P H Shum  Howard Leung', 'Edmond S L Ho  Hubert P H Shum  Yiuming Cheung  P C Yuen', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki', 'Hubert P H Shum  Edmond S L Ho']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum', 'Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  Shigeo Morishima', 'Liuyang Zhou  Lifeng Shang  Hubert P H Shum  Howard Leung', 'Edmond S L Ho  Hubert P H Shum  Yiuming Cheung  P C Yuen', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki', 'Hubert P H Shum  Edmond S L Ho', 'Taku Komura  Hubert P H Shum']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum', 'Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  Shigeo Morishima', 'Liuyang Zhou  Lifeng Shang  Hubert P H Shum  Howard Leung', 'Edmond S L Ho  Hubert P H Shum  Yiuming Cheung  P C Yuen', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki', 'Hubert P H Shum  Edmond S L Ho', 'Taku Komura  Hubert P H Shum', 'Hubert P H Shum']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum', 'Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  Shigeo Morishima', 'Liuyang Zhou  Lifeng Shang  Hubert P H Shum  Howard Leung', 'Edmond S L Ho  Hubert P H Shum  Yiuming Cheung  P C Yuen', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki', 'Hubert P H Shum  Edmond S L Ho', 'Taku Komura  Hubert P H Shum', 'Hubert P H Shum', 'Hubert P H Shum  Taku Komura  Pranjul Yadav']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum', 'Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  Shigeo Morishima', 'Liuyang Zhou  Lifeng Shang  Hubert P H Shum  Howard Leung', 'Edmond S L Ho  Hubert P H Shum  Yiuming Cheung  P C Yuen', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki', 'Hubert P H Shum  Edmond S L Ho', 'Taku Komura  Hubert P H Shum', 'Hubert P H Shum', 'Hubert P H Shum  Taku Komura  Pranjul Yadav', 'Hubert P H Shum  Taku Komura  Masashi Shiraishi  Shuntaro Yamazaki']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum', 'Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  Shigeo Morishima', 'Liuyang Zhou  Lifeng Shang  Hubert P H Shum  Howard Leung', 'Edmond S L Ho  Hubert P H Shum  Yiuming Cheung  P C Yuen', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki', 'Hubert P H Shum  Edmond S L Ho', 'Taku Komura  Hubert P H Shum', 'Hubert P H Shum', 'Hubert P H Shum  Taku Komura  Pranjul Yadav', 'Hubert P H Shum  Taku Komura  Masashi Shiraishi  Shuntaro Yamazaki', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum', 'Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  Shigeo Morishima', 'Liuyang Zhou  Lifeng Shang  Hubert P H Shum  Howard Leung', 'Edmond S L Ho  Hubert P H Shum  Yiuming Cheung  P C Yuen', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki', 'Hubert P H Shum  Edmond S L Ho', 'Taku Komura  Hubert P H Shum', 'Hubert P H Shum', 'Hubert P H Shum  Taku Komura  Pranjul Yadav', 'Hubert P H Shum  Taku Komura  Masashi Shiraishi  Shuntaro Yamazaki', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki', 'Taku Komura  Hubert P H Shum  Edmond S L Ho']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum', 'Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  Shigeo Morishima', 'Liuyang Zhou  Lifeng Shang  Hubert P H Shum  Howard Leung', 'Edmond S L Ho  Hubert P H Shum  Yiuming Cheung  P C Yuen', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki', 'Hubert P H Shum  Edmond S L Ho', 'Taku Komura  Hubert P H Shum', 'Hubert P H Shum', 'Hubert P H Shum  Taku Komura  Pranjul Yadav', 'Hubert P H Shum  Taku Komura  Masashi Shiraishi  Shuntaro Yamazaki', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki', 'Taku Komura  Hubert P H Shum  Edmond S L Ho', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum', 'Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  Shigeo Morishima', 'Liuyang Zhou  Lifeng Shang  Hubert P H Shum  Howard Leung', 'Edmond S L Ho  Hubert P H Shum  Yiuming Cheung  P C Yuen', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki', 'Hubert P H Shum  Edmond S L Ho', 'Taku Komura  Hubert P H Shum', 'Hubert P H Shum', 'Hubert P H Shum  Taku Komura  Pranjul Yadav', 'Hubert P H Shum  Taku Komura  Masashi Shiraishi  Shuntaro Yamazaki', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki', 'Taku Komura  Hubert P H Shum  Edmond S L Ho', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki', 'Hubert P H Shum  Taku Komura']\n",
      "['He Wang  Edmond S L Ho  Hubert P H Shum  Zhanxing Zhu', 'Qianhui Men  Edmond S L Ho  Hubert P H Shum  Howard Leung', 'Qianhui Men  Hubert P H Shum  Edmond S L Ho  Howard Leung', 'Jacky C P Chan  Hubert P H Shum  He Wang  Li Yi  Wei Wei  Edmond S L Ho', 'Naoya Iwamoto  Hubert P H Shum  Wakana Asahina  Shigeo Morishima', 'Naoya Iwamoto  Takuya Kato  Hubert P H Shum  Ryo Kakitsuka  Kenta Hara  Shigeo Morishima', 'Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi', 'Wakana Asahina  Naoya Iwamoto  Hubert P H Shum  Shigeo Morishima', 'Yijun Shen  Jingtian Zhang  Longzhi Yang  Hubert P H Shum', 'Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  Shigeo Morishima', 'Liuyang Zhou  Lifeng Shang  Hubert P H Shum  Howard Leung', 'Edmond S L Ho  Hubert P H Shum  Yiuming Cheung  P C Yuen', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho  Taku Komura  Franck Multon', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki', 'Hubert P H Shum  Edmond S L Ho', 'Taku Komura  Hubert P H Shum', 'Hubert P H Shum', 'Hubert P H Shum  Taku Komura  Pranjul Yadav', 'Hubert P H Shum  Taku Komura  Masashi Shiraishi  Shuntaro Yamazaki', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki', 'Taku Komura  Hubert P H Shum  Edmond S L Ho', 'Hubert P H Shum  Taku Komura  Shuntaro Yamazaki', 'Hubert P H Shum  Taku Komura', 'Hubert P H Shum  Taku Komura']\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#Authors\n",
    "###################################\n",
    "#Define empty list for the authors\n",
    "Authors = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    #find the div where the authors are\n",
    "    authorDiv = soup.find(\"div\",  attrs = {\"style\": \"display:block;min-width:280px;max-width:300px;float:left;text-align:left;overflow:hidden;margin-left:var(--size-marginleft);\"} )\n",
    "    #print(authorDiv) #check this works\n",
    "    authors = authorDiv.p.text\n",
    "    #print(authors) check this works\n",
    "    authorEnding = authors.find(', \"')\n",
    "    listAuthors = authors[:authorEnding]\n",
    "    #print(listAuthors) #check this works #may have to change this later for question about authors, maybe need new line\n",
    "    authorsWithoutComma = listAuthors.replace(',', ' |')\n",
    "    #print(authorsWithoutComma) #check this works\n",
    "    authorList = authorsWithoutComma.replace('and', '|')\n",
    "    \n",
    "    # Removing punctuations in string using regex\n",
    "    import re\n",
    "    # initializing string\n",
    "    test_str = authorList\n",
    "    # Removing punctuations in string\n",
    "    # Using regex\n",
    "    author = re.sub(r'[^\\w\\s]', '', test_str)\n",
    "    Authors.append(author)\n",
    "  \n",
    "    print(Authors)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021']\n",
      "['2021', '2021']\n",
      "['2021', '2021', '2021']\n",
      "['2021', '2021', '2021', '2019']\n",
      "['2021', '2021', '2021', '2019', '2019']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016', '2015']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016', '2015', '2014']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016', '2015', '2014', '2013']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016', '2015', '2014', '2013', '2013']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016', '2015', '2014', '2013', '2013', '2013']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016', '2015', '2014', '2013', '2013', '2013', '2012']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016', '2015', '2014', '2013', '2013', '2013', '2012', '2012']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016', '2015', '2014', '2013', '2013', '2013', '2012', '2012', '2010']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016', '2015', '2014', '2013', '2013', '2013', '2012', '2012', '2010', '2010']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016', '2015', '2014', '2013', '2013', '2013', '2012', '2012', '2010', '2010', '2009']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016', '2015', '2014', '2013', '2013', '2013', '2012', '2012', '2010', '2010', '2009', '2008']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016', '2015', '2014', '2013', '2013', '2013', '2012', '2012', '2010', '2010', '2009', '2008', '2008']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016', '2015', '2014', '2013', '2013', '2013', '2012', '2012', '2010', '2010', '2009', '2008', '2008', '2008']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016', '2015', '2014', '2013', '2013', '2013', '2012', '2012', '2010', '2010', '2009', '2008', '2008', '2008', '2007']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016', '2015', '2014', '2013', '2013', '2013', '2012', '2012', '2010', '2010', '2009', '2008', '2008', '2008', '2007', '2006']\n",
      "['2021', '2021', '2021', '2019', '2019', '2017', '2017', '2016', '2016', '2015', '2014', '2013', '2013', '2013', '2012', '2012', '2010', '2010', '2009', '2008', '2008', '2008', '2007', '2006', '2006']\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#Dates\n",
    "###################################\n",
    "#Define empty list for the dates\n",
    "Dates = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    #find the date\n",
    "    dateDiv = soup.find(\"div\",  attrs = {\"style\": \"display:block;min-width:280px;max-width:300px;float:left;text-align:left;overflow:hidden;margin-left:var(--size-marginleft);\"} )\n",
    "    #print(dateDiv) #check this works\n",
    "    dateWithDot = dateDiv.p.text.split()[-1]\n",
    "    #print(dateWithDot) #check this works\n",
    "    date = dateWithDot[:-1] #remove the fullstop\n",
    "    Dates.append(date)\n",
    "    print(Dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IEEE Transactions on Visualization and Computer Graphics', 'IEEE Transactions on Circuits and Systems for Video Technology', 'Computers and Graphics', 'Computer Animation and Virtual Worlds', ' MIG  ', ' ACE  ', ' D2AT  ', ' SIGGRAPH  ', ' Motion  ', 'Computer Graphics Forum', 'Computer Animation and Virtual Worlds', 'Computer Graphics Forum', 'Computer Animation and Virtual Worlds', ' CASA  ', 'IEEE Transactions on Visualization and Computer Graphics', ' VRST  ', 'NA', 'NA', 'Computer Animation and Virtual Worlds', 'ACM Transactions on Graphics', ' I3D  ', ' MIG  ', ' VRST  ', ' SCA  ', 'arXiv ']\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#Journal\n",
    "###################################\n",
    "#Define empty list for the journals\n",
    "Journals = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    #find journal article- this is more complex need some if clauses and a list\n",
    "    journalDiv = soup.find(\"div\",  attrs = {\"style\": \"display:block;min-width:280px;max-width:300px;float:left;text-align:left;overflow:hidden;margin-left:var(--size-marginleft);\"} )\n",
    "    #print(journalDiv) #check this works\n",
    "    journals = journalDiv.em\n",
    "    #print(journals)\n",
    "    #remove em\n",
    "    if (journals != None):\n",
    "        journal = journals.text\n",
    "        journal = journal.replace(\"'08:\", \"\")\n",
    "        journal = journal.replace(\"'21:\", \"\")\n",
    "        journal = journal.replace(\"'17:\", \"\")\n",
    "        journal = journal.replace(\"'19:\", \"\")\n",
    "        journal = journal.replace(\"'12:\", \"\")\n",
    "        journal = journal.replace(\"'16:\", \"\")\n",
    "        journal = journal.replace(\"'13:\", \"\")\n",
    "        journal = journal.replace(\"'06:\", \"\")\n",
    "        journal = journal.replace(\"'07:\", \"\")\n",
    "        journal = journal.replace(\"preprint arXiv:2006.11620\", \"\")\n",
    "        Journals.append(journal)\n",
    "    else:\n",
    "        Journals.append(\"NA\")\n",
    "        #Journals.append(journal)\n",
    "print(Journals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 31, 'NA', 30, 'NA', 'NA', 'NA', 'NA', 'NA', 34, 25, 32, 25, 'NA', 18, 'NA', 'NA', 'NA', 20, 27, 'NA', 'NA', 'NA', 'NA', 'NA']\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#Volume\n",
    "###################################\n",
    "#Define empty list for the volumes\n",
    "Volumes = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    #Find VolumeNumber\n",
    "    volDivNumber = soup.find(\"td\")\n",
    "    #print(journalDivNumber)\n",
    "    volClass = volDivNumber.find(\"p\", class_=\"TextSmallDefault\")\n",
    "    #print(journalClass)\n",
    "    volDivText = volClass.text\n",
    "    #print(journalDivText)\n",
    "    \n",
    "    import re\n",
    "    txt = volDivText\n",
    "    x = re.findall(\"volume={\", txt)\n",
    "    if (x):\n",
    "        iBeginning = volDivText.find(\"volume\")\n",
    "        iEnding = volDivText.find(\"}, number\")\n",
    "        #print(journalDivText[iBeginning+8:iEnding])\n",
    "        vNumber= volDivText[volDivText.find(\"volume\")+8:volDivText.find(\"}, number\")]\n",
    "        #print(jNumber)\n",
    "        split_string = vNumber.split(\"}\", 1)\n",
    "        substring = split_string[0]\n",
    "        #print(substring)\n",
    "        volNum = int(substring)\n",
    "        #print(volNum)\n",
    "        Volumes.append(volNum) \n",
    "    else:\n",
    "        Volumes.append(\"NA\")\n",
    "print(Volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IEEE', 'IEEE', 'Elsevier', 'John Wiley and Sons Ltd.', 'ACM', 'NA', 'NA', 'ACM', 'Springer International Publishing', 'John Wiley and Sons Ltd.', 'John Wiley and Sons Ltd.', 'John Wiley and Sons Ltd.', 'John Wiley and Sons Ltd.', 'John Wiley and Sons Ltd.', 'IEEE', 'ACM', 'WO Patent WO/2010/057897', 'University of Edinburgh', 'John Wiley and Sons Ltd.', 'ACM', 'ACM', 'Springer-Verlag', 'ACM', 'Eurographics Association', 'NA']\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#Publisher\n",
    "###################################\n",
    "#Define empty list for the publishers\n",
    "Publishers = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    #Find Publisher\n",
    "    journalDivPublisher = soup.find(\"td\")\n",
    "    #print(journalDivNumber)\n",
    "    journalPublisherClass = journalDivPublisher.find(\"p\", class_=\"TextSmallDefault\")\n",
    "    #print(journalClass)\n",
    "    publisherDivText = journalPublisherClass.text\n",
    "    #print(publisherDivText)\n",
    "    \n",
    "    import re\n",
    "    text = publisherDivText\n",
    "    i = re.findall(\"publisher={\", text)\n",
    "    \n",
    "    if (i):\n",
    "        iBeginning = publisherDivText.find(\"publisher\")\n",
    "        iEnding = publisherDivText.find(\"},}\")\n",
    "        #print(publisherDivText[iBeginning+8:iEnding])\n",
    "        jPublisher= publisherDivText[publisherDivText.find(\"publisher\")+11:publisherDivText.find(\"},}\")]\n",
    "        #print(jPublisher)\n",
    "        split_string_pub = jPublisher.split(\"}\", 1)\n",
    "        substring_pub = split_string_pub[0]\n",
    "        #print(substring_pub)\n",
    "        Publishers.append(substring_pub)\n",
    "    else:\n",
    "        Publishers.append(\"NA\")\n",
    "print(Publishers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['216--227', '3417--3432', '12', 'e1871', '18:1--18:9', '653--670', '8', '23:1--23:2', '16', '99--109', '301--309', '61--70', '531--542', '10', '741--752', '17--24', 'NA', '151', '385--394', '114:1--114:8', '131--138', '94--103', '65--72', '2', '7']\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#Page numbers\n",
    "###################################\n",
    "#Define empty list for the page numbers\n",
    "PageNumbers = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    #Find Pages\n",
    "    journalDivpages = soup.find(\"td\")\n",
    "    #print(journalDivNumber)\n",
    "    journalpagesClass = journalDivpages.find(\"p\", class_=\"TextSmallDefault\")\n",
    "    #print(journalClass)\n",
    "    pagesDivText = journalpagesClass.text\n",
    "    #print(pagesDivText)\n",
    "    \n",
    "    import re\n",
    "    num = pagesDivText\n",
    "    k = re.findall(\"pages={\", num)\n",
    "    \n",
    "    if (k):\n",
    "        iBeginning = pagesDivText.find(\"pages={\")\n",
    "        iEnding = pagesDivText.find(\"}, \")\n",
    "        #print(publisherDivText[iBeginning+8:iEnding])\n",
    "        jpages = pagesDivText[pagesDivText.find(\"pages={\")+7:pagesDivText.find(\"}, \")]\n",
    "        #print(jpages)\n",
    "        split_string_pages = jpages.split(\"},\", 1)\n",
    "        substring_pages = split_string_pages[0]\n",
    "        #print(substring_pages)\n",
    "        PageNumbers.append(substring_pages)\n",
    "    else:\n",
    "        #print(\"NA\")\n",
    "        PageNumbers.append(\"NA\")\n",
    "print(PageNumbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12', '16', '12', '20', '9', '18', '8', '2', '16', '11', '9', '10', '10', '10', '12', '8', 'NA', '151', '10', '8', '8', '10', '8', '2', '7']\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#Number of pages\n",
    "###################################\n",
    "#Define empty list for the number of page numbers\n",
    "NumberOfPages = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    #Find Number of Pages\n",
    "    journalDivpagesNum = soup.find(\"td\")\n",
    "    #print(journalDivNumber)\n",
    "    journalpagesNumClass = journalDivpagesNum.find(\"p\", class_=\"TextSmallDefault\")\n",
    "    #print(journalClass)\n",
    "    pagesNumDivText = journalpagesNumClass.text\n",
    "    #print(pagesDivText)\n",
    "         \n",
    "        \n",
    "    import re\n",
    "    num = pagesNumDivText\n",
    "    l = re.findall(\"numpages={\", num)\n",
    "    \n",
    "    if (l):\n",
    "        iBeginning = pagesNumDivText.find(\"numpages={\")\n",
    "        iEnding = pagesNumDivText.find(\"}, \")\n",
    "        #print(publisherDivText[iBeginning+8:iEnding])\n",
    "        jpagesNum = pagesNumDivText[pagesNumDivText.find(\"numpages={\")+10:pagesDivText.find(\"}, \")]\n",
    "        #print(jpages)\n",
    "        split_string_pagesNum = jpagesNum.split(\"},\", 1)\n",
    "        substring_pagesNum = split_string_pagesNum[0]\n",
    "        #print(substring_pagesNum)\n",
    "        NumberOfPages.append(substring_pagesNum)\n",
    "    else:\n",
    "        NumberOfPages.append(\"NA\")\n",
    "print(NumberOfPages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74, 13, 0, 6, 0, 4, 3, 6, 1, 14, 6, 26, 1, 2, 68, 68, 0, 1, 11, 125, 45, 0, 45, 6, 0]\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#Citation\n",
    "###################################\n",
    "#Define empty list for the citations\n",
    "Citations = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    citationSpan = soup.find(\"span\", class_=\"TextHighlightDefault\")\n",
    "    #print(citationSpan)\n",
    "    \n",
    "    import re\n",
    "    citationtext = citationSpan.text\n",
    "    #print(citationtext)\n",
    "    x = re.findall(\"Citation:\", citationtext)\n",
    "    \n",
    "    if (x):\n",
    "        iBeginning = citationtext.find(\"Citation:\")\n",
    "        iEnding = citationtext.find(\"##\")\n",
    "        #print(journalDivText[iBeginning+8:iEnding])\n",
    "        cite = citationtext[citationtext.find(\"Citation:\"):citationtext.find(\"##\")]\n",
    "        #print(cite)\n",
    "        split_string = cite.split(\": \", 1)\n",
    "        substring = split_string[1]\n",
    "        #print(substring)\n",
    "        integer = int(substring)\n",
    "        Citations.append(integer)\n",
    "    else:\n",
    "        zero = int(0)\n",
    "        Citations.append(zero)\n",
    "print(Citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4.579', '4.685', '1.936', '1.020', 'NA', 'NA', 'NA', 'NA', 'NA', '2.078', '1.020', '2.078', '1.020', 'NA', '4.579', 'NA', 'NA', 'NA', '1.020', '5.414', 'NA', 'NA', 'NA', 'NA', 'NA']\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#Impact Factor\n",
    "###################################\n",
    "#Define empty list for the impact factor\n",
    "ImpactFactors = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    impactSpan = soup.find(\"span\", class_=\"TextHighlightDefault\")\n",
    "    #print(impactSpan)\n",
    "    \n",
    "    import re\n",
    "    impactText = impactSpan.text\n",
    "    #print(impactText)\n",
    "    x = re.findall(\"Impact Factor\", impactText)\n",
    "    \n",
    "    if (x):\n",
    "        iBeginning = impactText.find(\"Impact Factor\")\n",
    "        iEnding = impactText.find(\"#\")\n",
    "        #print(journalDivText[iBeginning+8:iEnding])\n",
    "        impact = impactText[impactText.find(\"Impact Factor\"):impactText.find(\"#\")]\n",
    "        #print(impact)\n",
    "        split_string = impact.split(\": \", 2)\n",
    "        substring = split_string[1]\n",
    "        #print(substring) \n",
    "        ImpactFactors.append(substring)\n",
    "    else:\n",
    "        ImpactFactors.append(\"NA\")\n",
    "print(ImpactFactors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10.1109/TVCG.2019.2936810', '10.1109/TCSVT.2020.3038145', '10.1016/j.cag.2021.09.014', '10.1002/cav.1871', '10.1145/3359566.3360069', '10.1007/978-3-319-76270-8_46', 'NA', '10.1145/2945078.2945101', '10.1007/978-3-319-30808-1_7-1', '10.1111/cgf.12749', '10.1002/cav.1599', '10.1111/cgf.12212', '10.1002/cav.1546', 'NA', '10.1109/TVCG.2010.257', '10.1145/2407336.2407340', 'NA', 'NA', '10.1002/cav.v20:2/3', '10.1145/1409060.1409067', '10.1145/1342250.1342271', '10.1007/978-3-540-89220-5_10', '10.1145/1315184.1315194', 'NA', 'NA']\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#Dois\n",
    "###################################\n",
    "#Define empty list for the dois\n",
    "Dois = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    #Find Doi\n",
    "    journalDivdoi = soup.find(\"td\")\n",
    "    #print(journalDivNumber)\n",
    "    journaldoiClass = journalDivdoi.find(\"p\", class_=\"TextSmallDefault\")\n",
    "    #print(journalClass)\n",
    "    doiDivText = journaldoiClass.text\n",
    "    #print(doiDivText)\n",
    "    \n",
    "    import re\n",
    "    d = doiDivText\n",
    "    i = re.findall(\"doi={\", d)\n",
    "    \n",
    "    if (i):\n",
    "        iBeginning = doiDivText.find(\"doi={\")\n",
    "        iEnding = doiDivText.find(\"}, \")\n",
    "        #print(publisherDivText[iBeginning+8:iEnding])\n",
    "        jdoi = doiDivText[doiDivText.find(\"doi={\")+5:doiDivText.find(\"}, \")]\n",
    "        #:doiDivText.find(\"}, \")\n",
    "        #print(jdoi)\n",
    "        split_string_doi = jdoi.split(\"},\", 1)\n",
    "        substring_doi = split_string_doi[0]\n",
    "        #print(substring_doi)\n",
    "        Dois.append(substring_doi)\n",
    "    else:\n",
    "        #print(\"NA\")\n",
    "        Dois.append(\"NA\")\n",
    "print(Dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin creating the data file by importing csv\n",
    "import csv\n",
    "#Lists to be used in the data file\n",
    "Title = Titles\n",
    "Abstract = Abstracts\n",
    "Author = Authors\n",
    "Date = Dates\n",
    "Journal = Journals\n",
    "Volume = Volumes\n",
    "Publisher = Publishers\n",
    "Pages = PageNumbers\n",
    "No_Pages = NumberOfPages\n",
    "Citation = Citations\n",
    "ImpactFactor = ImpactFactors\n",
    "DOI = Dois\n",
    "\n",
    "file = open(\"publicationData.csv\", \"w\", newline =\"\")\n",
    "writer = csv.writer(file)\n",
    "\n",
    "for w in range(25):\n",
    "\n",
    "    writer.writerow([Title[w],Abstract[w], Author[w], Date[w], Journal[w], Volume[w], Publisher[w], Pages[w], No_Pages[w], \n",
    "                     Citation[w], ImpactFactor[w], DOI[w]])\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = 'publicationData.csv'\n",
    "column_names = ['Title', 'Abstract', 'Author/s', 'Date', 'Journal', 'Volume', 'Publisher', 'Pages', 'NumberOfPages',\n",
    "               'Citation', 'Impact Factor', 'DOI']\n",
    "\n",
    "raw_dataset = pd.read_csv(data, names=column_names,\n",
    "                          na_values='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Author/s</th>\n",
       "      <th>Date</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Pages</th>\n",
       "      <th>NumberOfPages</th>\n",
       "      <th>Citation</th>\n",
       "      <th>Impact Factor</th>\n",
       "      <th>DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spatio-temporal Manifold Learning for Human Mo...</td>\n",
       "      <td>Data-driven modeling of human motions is ubiqu...</td>\n",
       "      <td>He Wang  Edmond S L Ho  Hubert P H Shum  Zhanx...</td>\n",
       "      <td>2021</td>\n",
       "      <td>IEEE Transactions on Visualization and Compute...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>216--227</td>\n",
       "      <td>12.0</td>\n",
       "      <td>74</td>\n",
       "      <td>4.579</td>\n",
       "      <td>10.1109/TVCG.2019.2936810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Quadruple Diffusion Convolutional Recurrent ...</td>\n",
       "      <td>Recurrent neural network (RNN) has become popu...</td>\n",
       "      <td>Qianhui Men  Edmond S L Ho  Hubert P H Shum  H...</td>\n",
       "      <td>2021</td>\n",
       "      <td>IEEE Transactions on Circuits and Systems for ...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>3417--3432</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13</td>\n",
       "      <td>4.685</td>\n",
       "      <td>10.1109/TCSVT.2020.3038145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GAN-based Reactive Motion Synthesis with Class...</td>\n",
       "      <td>Creating realistic characters that can react t...</td>\n",
       "      <td>Qianhui Men  Hubert P H Shum  Edmond S L Ho  H...</td>\n",
       "      <td>2021</td>\n",
       "      <td>Computers and Graphics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.936</td>\n",
       "      <td>10.1016/j.cag.2021.09.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Generic Framework for Editing and Synthesizi...</td>\n",
       "      <td>Emotion is considered to be a core element in ...</td>\n",
       "      <td>Jacky C P Chan  Hubert P H Shum  He Wang  Li Y...</td>\n",
       "      <td>2019</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>30.0</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>e1871</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.020</td>\n",
       "      <td>10.1002/cav.1871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Automatic Sign Dance Synthesis from Gesture-ba...</td>\n",
       "      <td>Automatic dance synthesis has become more and ...</td>\n",
       "      <td>Naoya Iwamoto  Hubert P H Shum  Wakana Asahina...</td>\n",
       "      <td>2019</td>\n",
       "      <td>MIG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACM</td>\n",
       "      <td>18:1--18:9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1145/3359566.3360069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DanceDJ: A 3D Dance Animation Authoring System...</td>\n",
       "      <td>Dance is an important component of live perfor...</td>\n",
       "      <td>Naoya Iwamoto  Takuya Kato  Hubert P H Shum  R...</td>\n",
       "      <td>2017</td>\n",
       "      <td>ACE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>653--670</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1007/978-3-319-76270-8_46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Synthesizing Motion with Relative Emotion Stre...</td>\n",
       "      <td>With the advancement in motion sensing technol...</td>\n",
       "      <td>Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi</td>\n",
       "      <td>2017</td>\n",
       "      <td>D2AT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Automatic Dance Generation System Considering ...</td>\n",
       "      <td>In recent years, thanks to the development of ...</td>\n",
       "      <td>Wakana Asahina  Naoya Iwamoto  Hubert P H Shum...</td>\n",
       "      <td>2016</td>\n",
       "      <td>SIGGRAPH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACM</td>\n",
       "      <td>23:1--23:2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1145/2945078.2945101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Depth Sensor based Facial and Body Animation C...</td>\n",
       "      <td>Depth sensors have become one of the most popu...</td>\n",
       "      <td>Yijun Shen  Jingtian Zhang  Longzhi Yang  Hube...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Motion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Springer International Publishing</td>\n",
       "      <td>16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1007/978-3-319-30808-1_7-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Multi-layer Lattice Model for Real-Time Dynami...</td>\n",
       "      <td>Due to the recent advancement of computer grap...</td>\n",
       "      <td>Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  ...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Computer Graphics Forum</td>\n",
       "      <td>34.0</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>99--109</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14</td>\n",
       "      <td>2.078</td>\n",
       "      <td>10.1111/cgf.12749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Human Motion Variation Synthesis with Multivar...</td>\n",
       "      <td>Human motion variation synthesis is important ...</td>\n",
       "      <td>Liuyang Zhou  Lifeng Shang  Hubert P H Shum  H...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>25.0</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>301--309</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.020</td>\n",
       "      <td>10.1002/cav.1599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Topology Aware Data-Driven Inverse Kinematics</td>\n",
       "      <td>Creating realistic human movement is a time co...</td>\n",
       "      <td>Edmond S L Ho  Hubert P H Shum  Yiuming Cheung...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Computer Graphics Forum</td>\n",
       "      <td>32.0</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>61--70</td>\n",
       "      <td>10.0</td>\n",
       "      <td>26</td>\n",
       "      <td>2.078</td>\n",
       "      <td>10.1111/cgf.12212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Natural Preparation Behavior Synthesis</td>\n",
       "      <td>Humans adjust their movements in advance to pr...</td>\n",
       "      <td>Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho ...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>25.0</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>531--542</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.020</td>\n",
       "      <td>10.1002/cav.1546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Preparation Behaviour Synthesis with Reinforce...</td>\n",
       "      <td>When humans perform a series of motions, they ...</td>\n",
       "      <td>Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho ...</td>\n",
       "      <td>2013</td>\n",
       "      <td>CASA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Simulating Multiple Character Interactions wit...</td>\n",
       "      <td>This paper proposes a new methodology for synt...</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Shuntaro Yamazaki</td>\n",
       "      <td>2012</td>\n",
       "      <td>IEEE Transactions on Visualization and Compute...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>741--752</td>\n",
       "      <td>12.0</td>\n",
       "      <td>68</td>\n",
       "      <td>4.579</td>\n",
       "      <td>10.1109/TVCG.2010.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Real-time Physical Modelling of Character Move...</td>\n",
       "      <td>With the advancement of motion tracking hardwa...</td>\n",
       "      <td>Hubert P H Shum  Edmond S L Ho</td>\n",
       "      <td>2012</td>\n",
       "      <td>VRST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACM</td>\n",
       "      <td>17--24</td>\n",
       "      <td>8.0</td>\n",
       "      <td>68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1145/2407336.2407340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Manufacturing Video Graphics</td>\n",
       "      <td>An apparatus comprising: a memory system stori...</td>\n",
       "      <td>Taku Komura  Hubert P H Shum</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WO Patent WO/2010/057897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Simulating Interactions Among Multiple Characters</td>\n",
       "      <td>In this thesis, we attack a challenging proble...</td>\n",
       "      <td>Hubert P H Shum</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>University of Edinburgh</td>\n",
       "      <td>151</td>\n",
       "      <td>151.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Angular Momentum Guided Motion Concatenation</td>\n",
       "      <td>In this paper, we propose a new method to conc...</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Pranjul Yadav</td>\n",
       "      <td>2009</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>20.0</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>385--394</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1.020</td>\n",
       "      <td>10.1002/cav.v20:2/3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Interaction Patches for Multi-Character Animation</td>\n",
       "      <td>We propose a data-driven approach to automatic...</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Masashi Shiraish...</td>\n",
       "      <td>2008</td>\n",
       "      <td>ACM Transactions on Graphics</td>\n",
       "      <td>27.0</td>\n",
       "      <td>ACM</td>\n",
       "      <td>114:1--114:8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>125</td>\n",
       "      <td>5.414</td>\n",
       "      <td>10.1145/1409060.1409067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Simulating Interactions of Avatars in High Dim...</td>\n",
       "      <td>Efficient computation of strategic movements i...</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Shuntaro Yamazaki</td>\n",
       "      <td>2008</td>\n",
       "      <td>I3D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACM</td>\n",
       "      <td>131--138</td>\n",
       "      <td>8.0</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1145/1342250.1342271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Simulating Interactions of Characters</td>\n",
       "      <td>It is difficult to create scenes where multipl...</td>\n",
       "      <td>Taku Komura  Hubert P H Shum  Edmond S L Ho</td>\n",
       "      <td>2008</td>\n",
       "      <td>MIG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Springer-Verlag</td>\n",
       "      <td>94--103</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1007/978-3-540-89220-5_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Simulating Competitive Interactions using Sing...</td>\n",
       "      <td>It is difficult to create scenes where multipl...</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Shuntaro Yamazaki</td>\n",
       "      <td>2007</td>\n",
       "      <td>VRST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACM</td>\n",
       "      <td>65--72</td>\n",
       "      <td>8.0</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1145/1315184.1315194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Generating Realistic Fighting Scenes by Game Tree</td>\n",
       "      <td>Recently, there have been a lot of researches ...</td>\n",
       "      <td>Hubert P H Shum  Taku Komura</td>\n",
       "      <td>2006</td>\n",
       "      <td>SCA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eurographics Association</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Technical Note: Generating Realistic Fighting ...</td>\n",
       "      <td>Recently, there have been a lot of researches ...</td>\n",
       "      <td>Hubert P H Shum  Taku Komura</td>\n",
       "      <td>2006</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0   Spatio-temporal Manifold Learning for Human Mo...   \n",
       "1   A Quadruple Diffusion Convolutional Recurrent ...   \n",
       "2   GAN-based Reactive Motion Synthesis with Class...   \n",
       "3   A Generic Framework for Editing and Synthesizi...   \n",
       "4   Automatic Sign Dance Synthesis from Gesture-ba...   \n",
       "5   DanceDJ: A 3D Dance Animation Authoring System...   \n",
       "6   Synthesizing Motion with Relative Emotion Stre...   \n",
       "7   Automatic Dance Generation System Considering ...   \n",
       "8   Depth Sensor based Facial and Body Animation C...   \n",
       "9   Multi-layer Lattice Model for Real-Time Dynami...   \n",
       "10  Human Motion Variation Synthesis with Multivar...   \n",
       "11      Topology Aware Data-Driven Inverse Kinematics   \n",
       "12             Natural Preparation Behavior Synthesis   \n",
       "13  Preparation Behaviour Synthesis with Reinforce...   \n",
       "14  Simulating Multiple Character Interactions wit...   \n",
       "15  Real-time Physical Modelling of Character Move...   \n",
       "16                       Manufacturing Video Graphics   \n",
       "17  Simulating Interactions Among Multiple Characters   \n",
       "18       Angular Momentum Guided Motion Concatenation   \n",
       "19  Interaction Patches for Multi-Character Animation   \n",
       "20  Simulating Interactions of Avatars in High Dim...   \n",
       "21              Simulating Interactions of Characters   \n",
       "22  Simulating Competitive Interactions using Sing...   \n",
       "23  Generating Realistic Fighting Scenes by Game Tree   \n",
       "24  Technical Note: Generating Realistic Fighting ...   \n",
       "\n",
       "                                             Abstract  \\\n",
       "0   Data-driven modeling of human motions is ubiqu...   \n",
       "1   Recurrent neural network (RNN) has become popu...   \n",
       "2   Creating realistic characters that can react t...   \n",
       "3   Emotion is considered to be a core element in ...   \n",
       "4   Automatic dance synthesis has become more and ...   \n",
       "5   Dance is an important component of live perfor...   \n",
       "6   With the advancement in motion sensing technol...   \n",
       "7   In recent years, thanks to the development of ...   \n",
       "8   Depth sensors have become one of the most popu...   \n",
       "9   Due to the recent advancement of computer grap...   \n",
       "10  Human motion variation synthesis is important ...   \n",
       "11  Creating realistic human movement is a time co...   \n",
       "12  Humans adjust their movements in advance to pr...   \n",
       "13  When humans perform a series of motions, they ...   \n",
       "14  This paper proposes a new methodology for synt...   \n",
       "15  With the advancement of motion tracking hardwa...   \n",
       "16  An apparatus comprising: a memory system stori...   \n",
       "17  In this thesis, we attack a challenging proble...   \n",
       "18  In this paper, we propose a new method to conc...   \n",
       "19  We propose a data-driven approach to automatic...   \n",
       "20  Efficient computation of strategic movements i...   \n",
       "21  It is difficult to create scenes where multipl...   \n",
       "22  It is difficult to create scenes where multipl...   \n",
       "23  Recently, there have been a lot of researches ...   \n",
       "24  Recently, there have been a lot of researches ...   \n",
       "\n",
       "                                             Author/s  Date  \\\n",
       "0   He Wang  Edmond S L Ho  Hubert P H Shum  Zhanx...  2021   \n",
       "1   Qianhui Men  Edmond S L Ho  Hubert P H Shum  H...  2021   \n",
       "2   Qianhui Men  Hubert P H Shum  Edmond S L Ho  H...  2021   \n",
       "3   Jacky C P Chan  Hubert P H Shum  He Wang  Li Y...  2019   \n",
       "4   Naoya Iwamoto  Hubert P H Shum  Wakana Asahina...  2019   \n",
       "5   Naoya Iwamoto  Takuya Kato  Hubert P H Shum  R...  2017   \n",
       "6      Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi  2017   \n",
       "7   Wakana Asahina  Naoya Iwamoto  Hubert P H Shum...  2016   \n",
       "8   Yijun Shen  Jingtian Zhang  Longzhi Yang  Hube...  2016   \n",
       "9   Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  ...  2015   \n",
       "10  Liuyang Zhou  Lifeng Shang  Hubert P H Shum  H...  2014   \n",
       "11  Edmond S L Ho  Hubert P H Shum  Yiuming Cheung...  2013   \n",
       "12  Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho ...  2013   \n",
       "13  Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho ...  2013   \n",
       "14    Hubert P H Shum  Taku Komura  Shuntaro Yamazaki  2012   \n",
       "15                     Hubert P H Shum  Edmond S L Ho  2012   \n",
       "16                       Taku Komura  Hubert P H Shum  2010   \n",
       "17                                    Hubert P H Shum  2010   \n",
       "18        Hubert P H Shum  Taku Komura  Pranjul Yadav  2009   \n",
       "19  Hubert P H Shum  Taku Komura  Masashi Shiraish...  2008   \n",
       "20    Hubert P H Shum  Taku Komura  Shuntaro Yamazaki  2008   \n",
       "21        Taku Komura  Hubert P H Shum  Edmond S L Ho  2008   \n",
       "22    Hubert P H Shum  Taku Komura  Shuntaro Yamazaki  2007   \n",
       "23                       Hubert P H Shum  Taku Komura  2006   \n",
       "24                       Hubert P H Shum  Taku Komura  2006   \n",
       "\n",
       "                                              Journal  Volume  \\\n",
       "0   IEEE Transactions on Visualization and Compute...    27.0   \n",
       "1   IEEE Transactions on Circuits and Systems for ...    31.0   \n",
       "2                              Computers and Graphics     NaN   \n",
       "3               Computer Animation and Virtual Worlds    30.0   \n",
       "4                                               MIG       NaN   \n",
       "5                                               ACE       NaN   \n",
       "6                                              D2AT       NaN   \n",
       "7                                          SIGGRAPH       NaN   \n",
       "8                                            Motion       NaN   \n",
       "9                             Computer Graphics Forum    34.0   \n",
       "10              Computer Animation and Virtual Worlds    25.0   \n",
       "11                            Computer Graphics Forum    32.0   \n",
       "12              Computer Animation and Virtual Worlds    25.0   \n",
       "13                                             CASA       NaN   \n",
       "14  IEEE Transactions on Visualization and Compute...    18.0   \n",
       "15                                             VRST       NaN   \n",
       "16                                                NaN     NaN   \n",
       "17                                                NaN     NaN   \n",
       "18              Computer Animation and Virtual Worlds    20.0   \n",
       "19                       ACM Transactions on Graphics    27.0   \n",
       "20                                              I3D       NaN   \n",
       "21                                              MIG       NaN   \n",
       "22                                             VRST       NaN   \n",
       "23                                              SCA       NaN   \n",
       "24                                             arXiv      NaN   \n",
       "\n",
       "                            Publisher         Pages  NumberOfPages  Citation  \\\n",
       "0                                IEEE      216--227           12.0        74   \n",
       "1                                IEEE    3417--3432           16.0        13   \n",
       "2                            Elsevier            12           12.0         0   \n",
       "3            John Wiley and Sons Ltd.         e1871           20.0         6   \n",
       "4                                 ACM    18:1--18:9            9.0         0   \n",
       "5                                 NaN      653--670           18.0         4   \n",
       "6                                 NaN             8            8.0         3   \n",
       "7                                 ACM    23:1--23:2            2.0         6   \n",
       "8   Springer International Publishing            16           16.0         1   \n",
       "9            John Wiley and Sons Ltd.       99--109           11.0        14   \n",
       "10           John Wiley and Sons Ltd.      301--309            9.0         6   \n",
       "11           John Wiley and Sons Ltd.        61--70           10.0        26   \n",
       "12           John Wiley and Sons Ltd.      531--542           10.0         1   \n",
       "13           John Wiley and Sons Ltd.            10           10.0         2   \n",
       "14                               IEEE      741--752           12.0        68   \n",
       "15                                ACM        17--24            8.0        68   \n",
       "16           WO Patent WO/2010/057897           NaN            NaN         0   \n",
       "17            University of Edinburgh           151          151.0         1   \n",
       "18           John Wiley and Sons Ltd.      385--394           10.0        11   \n",
       "19                                ACM  114:1--114:8            8.0       125   \n",
       "20                                ACM      131--138            8.0        45   \n",
       "21                    Springer-Verlag       94--103           10.0         0   \n",
       "22                                ACM        65--72            8.0        45   \n",
       "23           Eurographics Association             2            2.0         6   \n",
       "24                                NaN             7            7.0         0   \n",
       "\n",
       "    Impact Factor                            DOI  \n",
       "0           4.579      10.1109/TVCG.2019.2936810  \n",
       "1           4.685     10.1109/TCSVT.2020.3038145  \n",
       "2           1.936      10.1016/j.cag.2021.09.014  \n",
       "3           1.020               10.1002/cav.1871  \n",
       "4             NaN        10.1145/3359566.3360069  \n",
       "5             NaN   10.1007/978-3-319-76270-8_46  \n",
       "6             NaN                            NaN  \n",
       "7             NaN        10.1145/2945078.2945101  \n",
       "8             NaN  10.1007/978-3-319-30808-1_7-1  \n",
       "9           2.078              10.1111/cgf.12749  \n",
       "10          1.020               10.1002/cav.1599  \n",
       "11          2.078              10.1111/cgf.12212  \n",
       "12          1.020               10.1002/cav.1546  \n",
       "13            NaN                            NaN  \n",
       "14          4.579          10.1109/TVCG.2010.257  \n",
       "15            NaN        10.1145/2407336.2407340  \n",
       "16            NaN                            NaN  \n",
       "17            NaN                            NaN  \n",
       "18          1.020            10.1002/cav.v20:2/3  \n",
       "19          5.414        10.1145/1409060.1409067  \n",
       "20            NaN        10.1145/1342250.1342271  \n",
       "21            NaN   10.1007/978-3-540-89220-5_10  \n",
       "22            NaN        10.1145/1315184.1315194  \n",
       "23            NaN                            NaN  \n",
       "24            NaN                            NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = raw_dataset.copy()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spatio-temporal manifold learning for human motions via long-horizon modeling\n",
      "data-driven modeling of human motions is ubiquitous in computer graphics and computer vision applications, such as synthesizing realistic motions or recognizing actions. recent research has shown that such problems can be approached by learning a natural motion manifold using deep learning on a large amount data, to address the shortcomings of traditional data-driven approaches. however, previous deep learning methods can be sub-optimal for two reasons. first, the skeletal information has not been fully utilized for feature extraction. unlike images, it is difficult to define spatial proximity in skeletal motions in the way that deep networks can be applied for feature extraction. second, motion is time-series data with strong multi-modal temporal correlations between frames. on the one hand, a frame could be followed by several candidate frames leading to different motions; on the other hand, long-range dependencies exist where a number of frames in the beginning correlate to a number of frames later.  ineffective temporal modeling would either under-estimate the multi-modality and variance, resulting in featureless mean motion or over-estimate them resulting in jittery motions, which is a major source of visual artifacts. in this paper, we propose a new deep network to tackle these challenges by creating a natural motion manifold that is versatile for many applications. the network has a new spatial component for feature extraction. it is also equipped with a new batch prediction model that predicts a large number of frames at once, such that long-term temporally-based objective functions can be employed to correctly learn the motion multi-modality and variances. with our system, long-duration motions can be predicted/synthesized using an open-loop setup where the motion retains the dynamics accurately. it can also be used for denoising corrupted motions and synthesizing new motions with given control signals. we demonstrate that our system can create superior results comparing to existing work in multiple applications.\n",
      "a quadruple diffusion convolutional recurrent network for human motion prediction\n",
      "recurrent neural network (rnn) has become popular for human motion prediction thanks to its ability to capture temporal dependencies. however, it has limited capacity in modeling the complex spatial relationship in the human skeletal structure. in this work, we present a novel diffusion convolutional recurrent predictor for spatial and temporal movement forecasting, with multi-step random walks traversing bidirectionally along an adaptive graph to model interdependency among body joints. in the temporal domain, existing methods rely on a single forward predictor with the produced motion deflecting to the drift route, which leads to error accumulations over time. we propose to supplement the forward predictor with a forward discriminator to alleviate such motion drift in the long term under adversarial training. the solution is further enhanced by a backward predictor and a backward discriminator to effectively reduce the error, such that the system can also look into the past to improve the prediction at early frames. the two-way spatial diffusion convolutions and two-way temporal predictors together form a quadruple network. furthermore, we train our framework by modeling the velocity from observed motion dynamics instead of static poses to predict future movements that effectively reduces the discontinuity problem at early prediction. our method outperforms the state of the arts on both 3d and 2d datasets, including the human3.6m, cmu motion capture and penn action datasets. the results also show that our method correctly predicts both high-dynamic and low-dynamic moving trends with less motion drift.\n",
      "gan-based reactive motion synthesis with class-aware discriminators for human-human interaction\n",
      "creating realistic characters that can react to the users' or another character's movement can benefit computer graphics, games and virtual reality hugely. however, synthesizing such reactive motions in human-human interactions is a challenging task due to the many different ways two humans can interact. while there are a number of successful researches in adapting the generative adversarial network (gan) in synthesizing single human actions, there are very few on modelling human-human interactions. in this paper, we propose a semi-supervised gan system that synthesizes the reactive motion of a character given the active motion from another character. our key insights are two-fold. first, to effectively encode the complicated spatial-temporal information of a human motion, we empower the generator with a part-based long short-term memory (lstm) module, such that the temporal movement of different limbs can be effectively modelled. we further include an attention module such that the temporal significance of the interaction can be learned, which enhances the temporal alignment of the active-reactive motion pair. second, as the reactive motion of different types of interactions can be significantly different, we introduce a discriminator that not only tells if the generated movement is realistic or not, but also tells the class label of the interaction. this allows the use of such labels in supervising the training of the generator. we experiment with the sbu and the hhoi datasets. the high quality of the synthetic motion demonstrates the effective design of our generator, and the discriminability of the synthesis also demonstrates the strength of our discriminator.\n",
      "a generic framework for editing and synthesizing multimodal data with relative emotion strength\n",
      "emotion is considered to be a core element in performances [1]. in computer animation, both body motions and facial expressions are two popular mediums for a character to express the emotion. however, there has been limited research in studying how to effectively synthesize these two types of character movements using different levels of emotion strength with intuitive control, which is difficult to be modelled effectively. in this work, we explore a common model that can be used to represent the emotion for the applications of body motions and facial expressions synthesis. unlike previous work which encode emotions into discrete motion style descriptors, we propose a continuous control indicator called emotion strength, by controlling which a data-driven approach is presented to synthesize motions with fine control over emotions. rather than interpolating motion features to synthesize new motion as in existing work, our method explicitly learns a model mapping low-level motion features to the emotion strength. since the motion synthesis model is learned in the training stage, the computation time required for synthesizing motions at run-time is very low. we further demonstrate the generality of our proposed framework by editing 2d face images using relative emotion strength. as a result, our method can be applied to interactive applications such as computer games, image editing tools and virtual reality applications, as well as offline applications such as animation and movie production. \n",
      "automatic sign dance synthesis from gesture-based sign language\n",
      "automatic dance synthesis has become more and more popular due to the increasing demand in computer games and animations. existing research generates dance motions without much consideration for the context of the music. in reality, professional dancers make choreography according to the lyrics and music features. in this research, we focus on a particular genre of dance known as sign dance, which combines gesture-based sign language with full body dance motion. we propose a system to automatically generate sign dance from a piece of music and its corresponding sign gesture. the core of the system is a sign dance model trained by multiple regression analysis to represent the correlations between sign dance and sign gesture/music, as well as a set of objective functions to evaluate the quality of the sign dance. our system can be applied to music visualization, allowing people with hearing difficulties to understand and enjoy music. \n",
      "dancedj: a 3d dance animation authoring system for live performance\n",
      "dance is an important component of live performance for expressing emotion and presenting visual context. human dance performances typically require expert knowledge of dance choreography and professional rehearsal, which are too costly for casual entertainment venues and clubs. recent advancements in character animation and motion synthesis have made it possible to synthesize virtual 3d dance characters in real-time. the major problem in existing systems is a lack of an intuitive interfaces to control the animation for real-time dance controls. we propose a new system called the dancedj to solve this problem. our system consists of two parts. the first part is an underlying motion analysis system that evaluates motion features including dance features such as the postures and movement tempo, as well as audio features such as the music tempo and structure. as a pre-process, given a dancing motion database, our system evaluates the quality of possible timings to connect and switch different dancing motions. during run-time, we propose a control interface that provides visual guidance. we observe that disk jockeys (djs) effectively control the mixing of music using the dj controller, and therefore propose a dj controller for controlling dancing characters. this allows djs to transfer their skills from music control to dance control using a similar hardware setup. we map different motion control functions onto the dj controller, and visualize the timing of natural connection points, such that the dj can effectively govern the synthesized dance motion. we conducted two user experiments to evaluate the user experience and the quality of the dance character. quantitative analysis shows that our system performs well in both motion control and simulation quality.\n",
      "synthesizing motion with relative emotion strength\n",
      "with the advancement in motion sensing technology, acquiring high-quality human motions for creating realistic character animation is much easier than before. since motion data itself is not the main obstacle anymore, more and more effort goes into enhancing the realism of character animation, such as motion styles and control. in this paper, we explore a less studied area: the emotion of motions. unlike previous work which encode emotions into discrete motion style descriptors, we propose a continuous control indicator called motion strength, by controlling which a data-driven approach is presented to synthesize motions with fine control over emotions. rather than interpolating motion features to synthesize new motion as in existing work, our method explicitly learns a model mapping low-level motion features to the emotion strength. since the motion synthesis model is learned in the training stage, the computation time required for synthesizing motions at run-time is very low. as a result, our method can be applied to interactive applications such as computer games and virtual reality applications, as well as offline applications such as animation and movie production.\n",
      "automatic dance generation system considering sign language information\n",
      "in recent years, thanks to the development of 3dcg animation editing tools (e.g. mikumikudance), a lot of 3d character dance animation movies are created by amateur users. however, it is very difficult to create choreography from scratch without any technical knowledge. shiratori et al. [2006] produced the dance automatic generation system considering rhythm and intensity of dance motions. however, each segment is selected randomly from database, so the generated dance motion has no linguistic or emotional meanings. takano et al. [2010] produced a human motion generation system considering motion labels. however, they use simple motion labels like 'running' or 'jump', so they cannot generate motions that express emotions. in reality, professional dancers make choreography based on music features or lyrics in music, and express emotion or how they feel in music. in our work, we aim at generating more emotional dance motion easily. therefore, we use linguistic information in lyrics, and generate dance motion.\n",
      "depth sensor based facial and body animation control\n",
      "depth sensors have become one of the most popular means of generating human facial and posture information in the past decade. by coupling a depth camera and computer vision based recognition algorithms, these sensors can detect human facial and body features in real time. such a breakthrough has fused many new research directions in animation creation and control, which also has opened up new challenges. in this chapter, we explain how depth sensors obtain human facial and body information. we then discuss on the main challenge on depth sensor-based systems, which is the inaccuracy of the obtained data, and explain how the problem is tackled. finally, we point out the emerging applications in the field, in which human facial and body feature modeling and understanding is a key research problem.\n",
      "multi-layer lattice model for real-time dynamic character deformation\n",
      "due to the recent advancement of computer graphics hardware and software algorithms, deformable characters have become more and more popular in real-time applications such as computer games. while there are mature techniques to generate primary deformation from skeletal movement, simulating realistic and stable secondary deformation such as jiggling of fats remains challenging. on one hand, traditional volumetric approaches such as the finite element method require higher computational cost and are infeasible for limited hardware such as game consoles. on the other hand, while shape matching based simulations can produce plausible deformation in real-time, they suffer from a stiffness problem in which particles either show unrealistic deformation due to high gains, or cannot catch up with the body movement. in this paper, we propose a unified multi-layer lattice model to simulate the primary and secondary deformation of skeleton-driven characters. the core idea is to voxelize the input character mesh into multiple anatomical layers including the bone, muscle, fat and skin. primary deformation is applied on the bone voxels with lattice-based skinning. the movement of these voxels is propagated to other voxel layers using lattice shape matching simulation, creating a natural secondary deformation. our multi-layer lattice framework can produce simulation quality comparable to those from other volumetric approaches with a significantly smaller computational cost. it is best to be applied in real-time applications such as console games or interactive animation creation. \n",
      "human motion variation synthesis with multivariate gaussian processes\n",
      "human motion variation synthesis is important for crowd simulation and interactive applications to enhance synthesis quality. in this paper, we propose a novel generative probabilistic model to synthesize variations of human motion. our key idea is to model the conditional distribution of each joint via a multivariate gaussian process model, namely semi-parametric latent factor model (slfm). slfm can effectively model the correlations between degrees of freedom (dofs) of joints rather than dealing with each dof separately as implemented in existing methods. a detailed evaluation is performed to show that the proposed approach can effectively synthesize variations of different types of motions. motions generated by our method show a richer variation compared with existing ones. finally, our user study shows that the synthesized motion has a similar level of naturalness to captured human motions. our method is best applied in computer games and animations to introduce motion variations.\n",
      "topology aware data-driven inverse kinematics\n",
      "creating realistic human movement is a time consuming and labour intensive task. the major difficulty is that the user has to edit individual joints while maintaining an overall realistic and collision free posture. previous research suggests the use of data-driven inverse kinematics, such that one can focus on the control of a few joints, while the system automatically composes a natural posture. however, as a common problem of kinematics synthesis, penetration of body parts is difficult to avoid in complex movements. in this paper, we propose a new data-driven inverse kinematics framework that conserves the topology of the synthesizing postures. our system monitors and regulates the topology changes using the gauss linking integral (gli), such that penetration can be efficiently prevented. as a result, complex motions with tight body movements, as well as those involving interaction with external objects, can be simulated with minimal manual intervention. experimental results show that using our system, the user can create high quality human motion in real-time by controlling a few joints using a mouse or a multi-touch screen. the movement generated is both realistic and penetration free. our system is best applied for interactive motion design in computer animations and games.\n",
      "natural preparation behavior synthesis\n",
      "humans adjust their movements in advance to prepare for the forthcoming action, resulting in an efficient and smooth transition. however, traditional computer animation approaches such as motion graphs simply concatenates a series of actions without taking into account the following one. in this paper, we propose a new method to produce preparation behaviours using reinforcement learning. as an offline process, the system learns the optimal way to approach a target and prepare for interaction. a scalar value called the level of preparation is introduced, which represents the degree of transition from the initial action to the interacting action. to synthesize the movements of preparation, we propose a customized motion blending scheme based on the level of preparation, which is followed by an optimization framework that adjusts the posture to keep the balance. during run-time, the trained controller drives the character to move to a target with the appropriate level of preparation, resulting in a human-like behaviour. we create scenes in which the character has to move in a complex environment and interacts with objects, such as crawling under and jumping over obstacles while walking. the method is useful not only for computer animation, but also for real-time applications such as computer games, in which the characters need to accomplish a series of tasks in a given environment.\n",
      "preparation behaviour synthesis with reinforcement learning\n",
      "when humans perform a series of motions, they prepare for the next motion in advance so as to enhance the response time of their movements. this kind of preparation behaviour results in a natural and smooth transition of the overall movement. in this paper, we propose a new method to synthesize the behaviour using reinforcement learning. to create preparation movements, we propose a customized motion blending algorithm that is governed by a single numerical value, which we called the level of preparation. during the offline process, the system learns the optimal way to approach a target, as well as the realistic behaviour to prepare for interaction considering the level of preparation. at run-time, the trained controller indicates the character to move to a target with the appropriate level of preparation, resulting in human-like movements. we synthesized scenes in which the character has to move in a complex environment and interact with objects, such as a character crawling under and jumping over obstacles while walking. the method is useful not only for computer animation, but also for real-time applications such as computer games, in which the characters need to accomplish a series of tasks in a given environment.\n",
      "simulating multiple character interactions with collaborative and adversarial goals\n",
      "this paper proposes a new methodology for synthesizing animations of multiple characters, allowing them to intelligently compete with one another in dense environments, while still satisfying requirements set by an animator. to achieve these two conflicting objectives simultaneously, our method separately evaluates the competition and collaboration of the interactions, integrating the scores to select an action that maximizes both criteria. we extend the idea of min-max search, normally used for strategic games such as chess. using our method, animators can efficiently produce scenes of dense character interactions such as those in collective sports or martial arts. the method is especially effective for producing animations along story lines, where the characters must follow multiple objectives, while still accommodating geometric and kinematic constraints from the environment.\n",
      "real-time physical modelling of character movements with microsoft kinect\n",
      "with the advancement of motion tracking hardware such as the microsoft kinect, synthesizing human-like characters with real-time captured movements becomes increasingly important. traditional kinematics and dynamics approaches perform sub-optimally when the captured motion is noisy or even incomplete. in this paper, we proposed a unified framework to control physically simulated characters with live captured motion from kinect. our framework can synthesize any posture in a physical environment using external forces and torques computed by a pd controller. the major problem of kinect is the incompleteness of the captured posture, with some degree of freedom (dof) missing due to occlusions and noises. we propose to search for a best matched posture from a motion database constructed in a dimensionality reduced space, and substitute the missing dof to the live captured data. experimental results show that our method can synthesize realistic character movements from noisy captured motion. the proposed algorithm is computationally efficient and can be applied to a wide variety of interactive virtual reality applications such as motion-based gaming, rehabilitation and sport training.\n",
      "manufacturing video graphics\n",
      "an apparatus comprising: a memory system storing a plurality of sequences, each sequence comprising data for reproducing a different pattern of interactions between a respective plurality of moving characters and storing a combination data structure defining for each sequence connectability of that sequence with other ones of the plurality of sequences; a processor configured to determine pair-wise combination of stored sequences, by selecting sequences for pair-wise combination that are defined as connectable by the stored combination data structure, wherein each pair-wise combination has in common at least one of their respective plurality of moving characters and configured to use determined pair-wise combinations of the stored sequences to produce and output video graphics comprising a series of sequences in which movable characters repetitively interact in different combinations.\n",
      "simulating interactions among multiple characters\n",
      "in this thesis, we attack a challenging problem in the field of character animation: synthesizing interactions among multiple virtual characters in real-time. although there are heavy demands in the gaming and animation industries, no systemic solution has been proposed due to the difficulties to model the complex behaviors of the characters.we represent the continuous interactions among characters as a discrete markov decision process, and design a general objective function to evaluate the immediate rewards of launching an action. by applying game theory such as tree expansion and min-max search, the optimal actions that benefit the character the most in the future are selected. the simulated characters can interact competitively while achieving the requests from animators cooperatively.since the interactions between two characters depend on a lot of criteria, it is difficult to exhaustively precompute the optimal actions for all variations of these criteria. we design an off-policy approach that samples and precomputes only meaningful interactions. with the precomputed policy, the optimal movements under different situations can be evaluated in real-time.to simulate the interactions for a large number of characters with minimal computational overhead, we propose a method to precompute short durations of interactions between two characters as connectable patches. the patches are concatenated spatially to generate interactions with multiple characters, and temporally to generate longer interactions. based on the optional instructions given by the animators, our system automatically applies concatenations to create a huge scene of interacting crowd.we demonstrate our system by creating scenes with high quality interactions. on one hand, our algorithm can automatically generate artistic scenes of interactions such as the fighting scenes in movies that involve hundreds of characters. on the other hand, it can create controllable, intelligent characters that interact with the opponents for real-time applications such as 3d computer games.\n",
      "angular momentum guided motion concatenation\n",
      "in this paper, we propose a new method to concatenate two dynamic full-body motions such as punches, kicks and flips by using the angular momentum as a cue. through the observation of real humans, we have identified two patterns of angular momentum that make the transition of such motions efficient. based on these observations, we propose a new method to concatenate two full-body motions in a natural manner. our method is useful for applications where dynamic, full-body motions are required, such as 3d computer games and animations.\n",
      "interaction patches for multi-character animation\n",
      "we propose a data-driven approach to automatically generate a scene where tens to hundreds of characters densely interact with each other. during offline processing, the close interactions between characters are precomputed by expanding a game tree, and these are stored as data structures called interaction patches. then, during run-time, the system spatio-temporally concatenates the interaction patches to create scenes where a large number of characters closely interact with one another. using our method, it is possible to automatically or interactively produce animations of crowds interacting with each other in a stylized way. the method can be used for a variety of applications including tv programs, advertisements and movies.\n",
      "simulating interactions of avatars in high dimensional state space\n",
      "efficient computation of strategic movements is essential to control virtual avatars intelligently in computer games and 3d virtual environments. such a module is needed to control non-player characters (npcs) to fight, play team sports or move through a mass crowd. reinforcement learning is an approach to achieve real-time optimal control. however, the huge state space of human interactions makes it difficult to apply existing learning methods to control avatars when they have dense interactions with other characters. in this research, we propose a new methodology to ef?ciently plan the movements of an avatar interacting with another. we make use of the fact that the subspace of meaningful interactions is much smaller than the whole state space of two avatars. we efficiently collect samples by exploring the subspace where dense interactions between the avatars occur and favor samples that have high connectivity with the other samples. using the collected samples, a ?nite state machine (fsm) called interaction graph is composed. at run-time, we compute the optimal action of each avatar by min-max search or dynamic programming on the interaction graph. the methodology is applicable to control npcs in ?ghting and ball-sports games.\n",
      "simulating interactions of characters\n",
      "it is difficult to create scenes where multiple characters densely interact with each other. manually creating the motions of characters is time consuming due to the correlation of the movements between the characters. capturing the motions of multiple characters is also difficult as it requires a huge amount of post-processing of the data. in this paper, we explain the methods we have proposed to simulate close interactions of characters based on singly captured motions. we propose methods to (1) control characters intelligently to cooperatively/competitively interact with the other characters, and (2) generate movements that include close interactions such as tangling the segments with the others by taking into account the topological relationship of the characters.\n",
      "simulating competitive interactions using singly captured motions\n",
      "it is difficult to create scenes where multiple avatars are fighting / competing with each other. manually creating the motions of avatars is time consuming due to the correlation of the movements between the avatars. capturing the motions of multiple avatars is also difficult as it requires a huge amount of post-processing. in this paper, we propose a new method to generate a realistic scene of avatars densely interacting in a competitive environment. the motions of the avatars are considered to be captured individually, which will increase the easiness of obtaining the data. we propose a new algorithm called the temporal expansion approach which maps the continuous time action plan to a discrete space such that turnbased evaluation methods can be used. as a result, many mature algorithms in game such as the min-max search and alpha?beta pruning can be applied.using our method, avatars will plan their strategies taking into account the reaction of the opponent. fighting scenes with multiple avatars are generated to demonstrate the effectiveness of our algorithm. the proposed method can also be applied to other kinds of continuous activities that require strategy planning such as sport games.\n",
      "generating realistic fighting scenes by game tree\n",
      "recently, there have been a lot of researches to synthesize/edit the motion of a single avatar in the virtual environment. however, there has not been so much work of simulating continuous interactions of multiple avatars such as fighting. in this paper, we propose a new method to generate a realistic fighting scene based on motion capture data. we propose a new algorithm called the temporal expansion approach which maps the continuous time action plan to a discrete causality space such that turn-based evaluation methods can be used. as a result, it is possible to use many mature algorithms available in strategy games such as the minimax algorithm and α-β pruning. we also propose a method to generate and use an offense/defense table, which illustrates the spatialtemporal relationship of attacks and dodges, to incorporate tactical maneuvers of defense into the scene. using our method, avatars will plan their strategies taking into account the reaction of the opponent. fighting scenes with multiple avatars are generated to demonstrate the effectiveness of our algorithm. the proposed method can also be applied to other kinds of continuous activities that require strategy planning such as sport games.\n",
      "technical note: generating realistic fighting scenes by game tree\n",
      "recently, there have been a lot of researches to synthesize/edit the motion of a single avatar in the virtual environment. however, there has not been so much work of simulating continuous interactions of multiple avatars such as fighting. in this paper, we propose a new method to generate a realistic fighting scene based on motion capture data. we propose a new algorithm called the temporal expansion approach which maps the continuous time action plan to a discrete causality space such that turn-based evaluation methods can be used. as a result, it is possible to use many mature algorithms available in strategy games such as the minimax algorithm and α-β pruning. we also propose a method to generate and use an offense/defense table, which illustrates the spatialtemporal relationship of attacks and dodges, to incorporate tactical maneuvers of defense into the scene. using our method, avatars will plan their strategies taking into account the reaction of the opponent. fighting scenes with multiple avatars are generated to demonstrate the effectiveness of our algorithm. the proposed method can also be applied to other kinds of continuous activities that require strategy planning such as sport games.\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "#Question 3\n",
    "#e design and implement a solution to find out the 100 most popular words used for the title\n",
    "#and the abstract of the publications. You should define what a “word” means under your design.\n",
    "#For example, such “words” can be of an arbitrary length (single word/double word) and/or they\n",
    "#should be as meaningful as possible.\n",
    "#############################\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    #find the title and convert to lower case text\n",
    "    h1title = soup.find(\"h1\")\n",
    "    titleWithCapitals = h1title.text\n",
    "    title = titleWithCapitals.lower()\n",
    "    \n",
    "    #find the abstract and convert to lower case text\n",
    "    abstractDiv = soup.find(\"div\", attrs = {\"style\": \"margin-left: var(--size-marginleft)\"})\n",
    "    #print(abstractDiv) # check this works\n",
    "    abstractText = abstractDiv.p.text\n",
    "    abstract = abstractText.lower()\n",
    "    \n",
    "    \n",
    "    print(title)\n",
    "    print(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spatiotemporal manifold learning for human motions via longhorizon modeling', 'datadriven modeling of human motions is ubiquitous in computer graphics and computer vision applications such as synthesizing realistic motions or recognizing actions recent research has shown that such problems can be approached by learning a natural motion manifold using deep learning on a large amount data to address the shortcomings of traditional datadriven approaches however previous deep learning methods can be suboptimal for two reasons first the skeletal information has not been fully utilized for feature extraction unlike images it is difficult to define spatial proximity in skeletal motions in the way that deep networks can be applied for feature extraction second motion is timeseries data with strong multimodal temporal correlations between frames on the one hand a frame could be followed by several candidate frames leading to different motions on the other hand longrange dependencies exist where a number of frames in the beginning correlate to a number of frames later  ineffective temporal modeling would either underestimate the multimodality and variance resulting in featureless mean motion or overestimate them resulting in jittery motions which is a major source of visual artifacts in this paper we propose a new deep network to tackle these challenges by creating a natural motion manifold that is versatile for many applications the network has a new spatial component for feature extraction it is also equipped with a new batch prediction model that predicts a large number of frames at once such that longterm temporallybased objective functions can be employed to correctly learn the motion multimodality and variances with our system longduration motions can be predictedsynthesized using an openloop setup where the motion retains the dynamics accurately it can also be used for denoising corrupted motions and synthesizing new motions with given control signals we demonstrate that our system can create superior results comparing to existing work in multiple applications', 'a quadruple diffusion convolutional recurrent network for human motion prediction', 'recurrent neural network rnn has become popular for human motion prediction thanks to its ability to capture temporal dependencies however it has limited capacity in modeling the complex spatial relationship in the human skeletal structure in this work we present a novel diffusion convolutional recurrent predictor for spatial and temporal movement forecasting with multistep random walks traversing bidirectionally along an adaptive graph to model interdependency among body joints in the temporal domain existing methods rely on a single forward predictor with the produced motion deflecting to the drift route which leads to error accumulations over time we propose to supplement the forward predictor with a forward discriminator to alleviate such motion drift in the long term under adversarial training the solution is further enhanced by a backward predictor and a backward discriminator to effectively reduce the error such that the system can also look into the past to improve the prediction at early frames the twoway spatial diffusion convolutions and twoway temporal predictors together form a quadruple network furthermore we train our framework by modeling the velocity from observed motion dynamics instead of static poses to predict future movements that effectively reduces the discontinuity problem at early prediction our method outperforms the state of the arts on both 3d and 2d datasets including the human36m cmu motion capture and penn action datasets the results also show that our method correctly predicts both highdynamic and lowdynamic moving trends with less motion drift', 'ganbased reactive motion synthesis with classaware discriminators for humanhuman interaction', 'creating realistic characters that can react to the users or another characters movement can benefit computer graphics games and virtual reality hugely however synthesizing such reactive motions in humanhuman interactions is a challenging task due to the many different ways two humans can interact while there are a number of successful researches in adapting the generative adversarial network gan in synthesizing single human actions there are very few on modelling humanhuman interactions in this paper we propose a semisupervised gan system that synthesizes the reactive motion of a character given the active motion from another character our key insights are twofold first to effectively encode the complicated spatialtemporal information of a human motion we empower the generator with a partbased long shortterm memory lstm module such that the temporal movement of different limbs can be effectively modelled we further include an attention module such that the temporal significance of the interaction can be learned which enhances the temporal alignment of the activereactive motion pair second as the reactive motion of different types of interactions can be significantly different we introduce a discriminator that not only tells if the generated movement is realistic or not but also tells the class label of the interaction this allows the use of such labels in supervising the training of the generator we experiment with the sbu and the hhoi datasets the high quality of the synthetic motion demonstrates the effective design of our generator and the discriminability of the synthesis also demonstrates the strength of our discriminator', 'a generic framework for editing and synthesizing multimodal data with relative emotion strength', 'emotion is considered to be a core element in performances 1 in computer animation both body motions and facial expressions are two popular mediums for a character to express the emotion however there has been limited research in studying how to effectively synthesize these two types of character movements using different levels of emotion strength with intuitive control which is difficult to be modelled effectively in this work we explore a common model that can be used to represent the emotion for the applications of body motions and facial expressions synthesis unlike previous work which encode emotions into discrete motion style descriptors we propose a continuous control indicator called emotion strength by controlling which a datadriven approach is presented to synthesize motions with fine control over emotions rather than interpolating motion features to synthesize new motion as in existing work our method explicitly learns a model mapping lowlevel motion features to the emotion strength since the motion synthesis model is learned in the training stage the computation time required for synthesizing motions at runtime is very low we further demonstrate the generality of our proposed framework by editing 2d face images using relative emotion strength as a result our method can be applied to interactive applications such as computer games image editing tools and virtual reality applications as well as offline applications such as animation and movie production ', 'automatic sign dance synthesis from gesturebased sign language', 'automatic dance synthesis has become more and more popular due to the increasing demand in computer games and animations existing research generates dance motions without much consideration for the context of the music in reality professional dancers make choreography according to the lyrics and music features in this research we focus on a particular genre of dance known as sign dance which combines gesturebased sign language with full body dance motion we propose a system to automatically generate sign dance from a piece of music and its corresponding sign gesture the core of the system is a sign dance model trained by multiple regression analysis to represent the correlations between sign dance and sign gesturemusic as well as a set of objective functions to evaluate the quality of the sign dance our system can be applied to music visualization allowing people with hearing difficulties to understand and enjoy music ', 'dancedj a 3d dance animation authoring system for live performance', 'dance is an important component of live performance for expressing emotion and presenting visual context human dance performances typically require expert knowledge of dance choreography and professional rehearsal which are too costly for casual entertainment venues and clubs recent advancements in character animation and motion synthesis have made it possible to synthesize virtual 3d dance characters in realtime the major problem in existing systems is a lack of an intuitive interfaces to control the animation for realtime dance controls we propose a new system called the dancedj to solve this problem our system consists of two parts the first part is an underlying motion analysis system that evaluates motion features including dance features such as the postures and movement tempo as well as audio features such as the music tempo and structure as a preprocess given a dancing motion database our system evaluates the quality of possible timings to connect and switch different dancing motions during runtime we propose a control interface that provides visual guidance we observe that disk jockeys djs effectively control the mixing of music using the dj controller and therefore propose a dj controller for controlling dancing characters this allows djs to transfer their skills from music control to dance control using a similar hardware setup we map different motion control functions onto the dj controller and visualize the timing of natural connection points such that the dj can effectively govern the synthesized dance motion we conducted two user experiments to evaluate the user experience and the quality of the dance character quantitative analysis shows that our system performs well in both motion control and simulation quality', 'synthesizing motion with relative emotion strength', 'with the advancement in motion sensing technology acquiring highquality human motions for creating realistic character animation is much easier than before since motion data itself is not the main obstacle anymore more and more effort goes into enhancing the realism of character animation such as motion styles and control in this paper we explore a less studied area the emotion of motions unlike previous work which encode emotions into discrete motion style descriptors we propose a continuous control indicator called motion strength by controlling which a datadriven approach is presented to synthesize motions with fine control over emotions rather than interpolating motion features to synthesize new motion as in existing work our method explicitly learns a model mapping lowlevel motion features to the emotion strength since the motion synthesis model is learned in the training stage the computation time required for synthesizing motions at runtime is very low as a result our method can be applied to interactive applications such as computer games and virtual reality applications as well as offline applications such as animation and movie production', 'automatic dance generation system considering sign language information', 'in recent years thanks to the development of 3dcg animation editing tools eg mikumikudance a lot of 3d character dance animation movies are created by amateur users however it is very difficult to create choreography from scratch without any technical knowledge shiratori et al 2006 produced the dance automatic generation system considering rhythm and intensity of dance motions however each segment is selected randomly from database so the generated dance motion has no linguistic or emotional meanings takano et al 2010 produced a human motion generation system considering motion labels however they use simple motion labels like running or jump so they cannot generate motions that express emotions in reality professional dancers make choreography based on music features or lyrics in music and express emotion or how they feel in music in our work we aim at generating more emotional dance motion easily therefore we use linguistic information in lyrics and generate dance motion', 'depth sensor based facial and body animation control', 'depth sensors have become one of the most popular means of generating human facial and posture information in the past decade by coupling a depth camera and computer vision based recognition algorithms these sensors can detect human facial and body features in real time such a breakthrough has fused many new research directions in animation creation and control which also has opened up new challenges in this chapter we explain how depth sensors obtain human facial and body information we then discuss on the main challenge on depth sensorbased systems which is the inaccuracy of the obtained data and explain how the problem is tackled finally we point out the emerging applications in the field in which human facial and body feature modeling and understanding is a key research problem', 'multilayer lattice model for realtime dynamic character deformation', 'due to the recent advancement of computer graphics hardware and software algorithms deformable characters have become more and more popular in realtime applications such as computer games while there are mature techniques to generate primary deformation from skeletal movement simulating realistic and stable secondary deformation such as jiggling of fats remains challenging on one hand traditional volumetric approaches such as the finite element method require higher computational cost and are infeasible for limited hardware such as game consoles on the other hand while shape matching based simulations can produce plausible deformation in realtime they suffer from a stiffness problem in which particles either show unrealistic deformation due to high gains or cannot catch up with the body movement in this paper we propose a unified multilayer lattice model to simulate the primary and secondary deformation of skeletondriven characters the core idea is to voxelize the input character mesh into multiple anatomical layers including the bone muscle fat and skin primary deformation is applied on the bone voxels with latticebased skinning the movement of these voxels is propagated to other voxel layers using lattice shape matching simulation creating a natural secondary deformation our multilayer lattice framework can produce simulation quality comparable to those from other volumetric approaches with a significantly smaller computational cost it is best to be applied in realtime applications such as console games or interactive animation creation ', 'human motion variation synthesis with multivariate gaussian processes', 'human motion variation synthesis is important for crowd simulation and interactive applications to enhance synthesis quality in this paper we propose a novel generative probabilistic model to synthesize variations of human motion our key idea is to model the conditional distribution of each joint via a multivariate gaussian process model namely semiparametric latent factor model slfm slfm can effectively model the correlations between degrees of freedom dofs of joints rather than dealing with each dof separately as implemented in existing methods a detailed evaluation is performed to show that the proposed approach can effectively synthesize variations of different types of motions motions generated by our method show a richer variation compared with existing ones finally our user study shows that the synthesized motion has a similar level of naturalness to captured human motions our method is best applied in computer games and animations to introduce motion variations', 'topology aware datadriven inverse kinematics', 'creating realistic human movement is a time consuming and labour intensive task the major difficulty is that the user has to edit individual joints while maintaining an overall realistic and collision free posture previous research suggests the use of datadriven inverse kinematics such that one can focus on the control of a few joints while the system automatically composes a natural posture however as a common problem of kinematics synthesis penetration of body parts is difficult to avoid in complex movements in this paper we propose a new datadriven inverse kinematics framework that conserves the topology of the synthesizing postures our system monitors and regulates the topology changes using the gauss linking integral gli such that penetration can be efficiently prevented as a result complex motions with tight body movements as well as those involving interaction with external objects can be simulated with minimal manual intervention experimental results show that using our system the user can create high quality human motion in realtime by controlling a few joints using a mouse or a multitouch screen the movement generated is both realistic and penetration free our system is best applied for interactive motion design in computer animations and games', 'natural preparation behavior synthesis', 'humans adjust their movements in advance to prepare for the forthcoming action resulting in an efficient and smooth transition however traditional computer animation approaches such as motion graphs simply concatenates a series of actions without taking into account the following one in this paper we propose a new method to produce preparation behaviours using reinforcement learning as an offline process the system learns the optimal way to approach a target and prepare for interaction a scalar value called the level of preparation is introduced which represents the degree of transition from the initial action to the interacting action to synthesize the movements of preparation we propose a customized motion blending scheme based on the level of preparation which is followed by an optimization framework that adjusts the posture to keep the balance during runtime the trained controller drives the character to move to a target with the appropriate level of preparation resulting in a humanlike behaviour we create scenes in which the character has to move in a complex environment and interacts with objects such as crawling under and jumping over obstacles while walking the method is useful not only for computer animation but also for realtime applications such as computer games in which the characters need to accomplish a series of tasks in a given environment', 'preparation behaviour synthesis with reinforcement learning', 'when humans perform a series of motions they prepare for the next motion in advance so as to enhance the response time of their movements this kind of preparation behaviour results in a natural and smooth transition of the overall movement in this paper we propose a new method to synthesize the behaviour using reinforcement learning to create preparation movements we propose a customized motion blending algorithm that is governed by a single numerical value which we called the level of preparation during the offline process the system learns the optimal way to approach a target as well as the realistic behaviour to prepare for interaction considering the level of preparation at runtime the trained controller indicates the character to move to a target with the appropriate level of preparation resulting in humanlike movements we synthesized scenes in which the character has to move in a complex environment and interact with objects such as a character crawling under and jumping over obstacles while walking the method is useful not only for computer animation but also for realtime applications such as computer games in which the characters need to accomplish a series of tasks in a given environment', 'simulating multiple character interactions with collaborative and adversarial goals', 'this paper proposes a new methodology for synthesizing animations of multiple characters allowing them to intelligently compete with one another in dense environments while still satisfying requirements set by an animator to achieve these two conflicting objectives simultaneously our method separately evaluates the competition and collaboration of the interactions integrating the scores to select an action that maximizes both criteria we extend the idea of minmax search normally used for strategic games such as chess using our method animators can efficiently produce scenes of dense character interactions such as those in collective sports or martial arts the method is especially effective for producing animations along story lines where the characters must follow multiple objectives while still accommodating geometric and kinematic constraints from the environment', 'realtime physical modelling of character movements with microsoft kinect', 'with the advancement of motion tracking hardware such as the microsoft kinect synthesizing humanlike characters with realtime captured movements becomes increasingly important traditional kinematics and dynamics approaches perform suboptimally when the captured motion is noisy or even incomplete in this paper we proposed a unified framework to control physically simulated characters with live captured motion from kinect our framework can synthesize any posture in a physical environment using external forces and torques computed by a pd controller the major problem of kinect is the incompleteness of the captured posture with some degree of freedom dof missing due to occlusions and noises we propose to search for a best matched posture from a motion database constructed in a dimensionality reduced space and substitute the missing dof to the live captured data experimental results show that our method can synthesize realistic character movements from noisy captured motion the proposed algorithm is computationally efficient and can be applied to a wide variety of interactive virtual reality applications such as motionbased gaming rehabilitation and sport training', 'manufacturing video graphics', 'an apparatus comprising a memory system storing a plurality of sequences each sequence comprising data for reproducing a different pattern of interactions between a respective plurality of moving characters and storing a combination data structure defining for each sequence connectability of that sequence with other ones of the plurality of sequences a processor configured to determine pairwise combination of stored sequences by selecting sequences for pairwise combination that are defined as connectable by the stored combination data structure wherein each pairwise combination has in common at least one of their respective plurality of moving characters and configured to use determined pairwise combinations of the stored sequences to produce and output video graphics comprising a series of sequences in which movable characters repetitively interact in different combinations', 'simulating interactions among multiple characters', 'in this thesis we attack a challenging problem in the field of character animation synthesizing interactions among multiple virtual characters in realtime although there are heavy demands in the gaming and animation industries no systemic solution has been proposed due to the difficulties to model the complex behaviors of the characterswe represent the continuous interactions among characters as a discrete markov decision process and design a general objective function to evaluate the immediate rewards of launching an action by applying game theory such as tree expansion and minmax search the optimal actions that benefit the character the most in the future are selected the simulated characters can interact competitively while achieving the requests from animators cooperativelysince the interactions between two characters depend on a lot of criteria it is difficult to exhaustively precompute the optimal actions for all variations of these criteria we design an offpolicy approach that samples and precomputes only meaningful interactions with the precomputed policy the optimal movements under different situations can be evaluated in realtimeto simulate the interactions for a large number of characters with minimal computational overhead we propose a method to precompute short durations of interactions between two characters as connectable patches the patches are concatenated spatially to generate interactions with multiple characters and temporally to generate longer interactions based on the optional instructions given by the animators our system automatically applies concatenations to create a huge scene of interacting crowdwe demonstrate our system by creating scenes with high quality interactions on one hand our algorithm can automatically generate artistic scenes of interactions such as the fighting scenes in movies that involve hundreds of characters on the other hand it can create controllable intelligent characters that interact with the opponents for realtime applications such as 3d computer games', 'angular momentum guided motion concatenation', 'in this paper we propose a new method to concatenate two dynamic fullbody motions such as punches kicks and flips by using the angular momentum as a cue through the observation of real humans we have identified two patterns of angular momentum that make the transition of such motions efficient based on these observations we propose a new method to concatenate two fullbody motions in a natural manner our method is useful for applications where dynamic fullbody motions are required such as 3d computer games and animations', 'interaction patches for multicharacter animation', 'we propose a datadriven approach to automatically generate a scene where tens to hundreds of characters densely interact with each other during offline processing the close interactions between characters are precomputed by expanding a game tree and these are stored as data structures called interaction patches then during runtime the system spatiotemporally concatenates the interaction patches to create scenes where a large number of characters closely interact with one another using our method it is possible to automatically or interactively produce animations of crowds interacting with each other in a stylized way the method can be used for a variety of applications including tv programs advertisements and movies', 'simulating interactions of avatars in high dimensional state space', 'efficient computation of strategic movements is essential to control virtual avatars intelligently in computer games and 3d virtual environments such a module is needed to control nonplayer characters npcs to fight play team sports or move through a mass crowd reinforcement learning is an approach to achieve realtime optimal control however the huge state space of human interactions makes it difficult to apply existing learning methods to control avatars when they have dense interactions with other characters in this research we propose a new methodology to efciently plan the movements of an avatar interacting with another we make use of the fact that the subspace of meaningful interactions is much smaller than the whole state space of two avatars we efficiently collect samples by exploring the subspace where dense interactions between the avatars occur and favor samples that have high connectivity with the other samples using the collected samples a nite state machine fsm called interaction graph is composed at runtime we compute the optimal action of each avatar by minmax search or dynamic programming on the interaction graph the methodology is applicable to control npcs in ghting and ballsports games', 'simulating interactions of characters', 'it is difficult to create scenes where multiple characters densely interact with each other manually creating the motions of characters is time consuming due to the correlation of the movements between the characters capturing the motions of multiple characters is also difficult as it requires a huge amount of postprocessing of the data in this paper we explain the methods we have proposed to simulate close interactions of characters based on singly captured motions we propose methods to 1 control characters intelligently to cooperativelycompetitively interact with the other characters and 2 generate movements that include close interactions such as tangling the segments with the others by taking into account the topological relationship of the characters', 'simulating competitive interactions using singly captured motions', 'it is difficult to create scenes where multiple avatars are fighting  competing with each other manually creating the motions of avatars is time consuming due to the correlation of the movements between the avatars capturing the motions of multiple avatars is also difficult as it requires a huge amount of postprocessing in this paper we propose a new method to generate a realistic scene of avatars densely interacting in a competitive environment the motions of the avatars are considered to be captured individually which will increase the easiness of obtaining the data we propose a new algorithm called the temporal expansion approach which maps the continuous time action plan to a discrete space such that turnbased evaluation methods can be used as a result many mature algorithms in game such as the minmax search and alphabeta pruning can be appliedusing our method avatars will plan their strategies taking into account the reaction of the opponent fighting scenes with multiple avatars are generated to demonstrate the effectiveness of our algorithm the proposed method can also be applied to other kinds of continuous activities that require strategy planning such as sport games', 'generating realistic fighting scenes by game tree', 'recently there have been a lot of researches to synthesizeedit the motion of a single avatar in the virtual environment however there has not been so much work of simulating continuous interactions of multiple avatars such as fighting in this paper we propose a new method to generate a realistic fighting scene based on motion capture data we propose a new algorithm called the temporal expansion approach which maps the continuous time action plan to a discrete causality space such that turnbased evaluation methods can be used as a result it is possible to use many mature algorithms available in strategy games such as the minimax algorithm and αβ pruning we also propose a method to generate and use an offensedefense table which illustrates the spatialtemporal relationship of attacks and dodges to incorporate tactical maneuvers of defense into the scene using our method avatars will plan their strategies taking into account the reaction of the opponent fighting scenes with multiple avatars are generated to demonstrate the effectiveness of our algorithm the proposed method can also be applied to other kinds of continuous activities that require strategy planning such as sport games', 'technical note generating realistic fighting scenes by game tree', 'recently there have been a lot of researches to synthesizeedit the motion of a single avatar in the virtual environment however there has not been so much work of simulating continuous interactions of multiple avatars such as fighting in this paper we propose a new method to generate a realistic fighting scene based on motion capture data we propose a new algorithm called the temporal expansion approach which maps the continuous time action plan to a discrete causality space such that turnbased evaluation methods can be used as a result it is possible to use many mature algorithms available in strategy games such as the minimax algorithm and αβ pruning we also propose a method to generate and use an offensedefense table which illustrates the spatialtemporal relationship of attacks and dodges to incorporate tactical maneuvers of defense into the scene using our method avatars will plan their strategies taking into account the reaction of the opponent fighting scenes with multiple avatars are generated to demonstrate the effectiveness of our algorithm the proposed method can also be applied to other kinds of continuous activities that require strategy planning such as sport games']\n"
     ]
    }
   ],
   "source": [
    "#Import Regex to remove the punctuation\n",
    "import re\n",
    "\n",
    "TitlesAbstract = []\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    h1title = soup.find(\"h1\")\n",
    "    #only get the text from the title div\n",
    "    titleWithCapitals = h1title.text\n",
    "    #convert the title to lower case\n",
    "    titleLower = titleWithCapitals.lower()\n",
    "    #remove title punctuation\n",
    "    title = re.sub(r'[^\\w\\s]', '', titleLower)\n",
    "    TitlesAbstract.append(title)\n",
    "    \n",
    "    #find the abstract \n",
    "    abstractDiv = soup.find(\"div\", attrs = {\"style\": \"margin-left: var(--size-marginleft)\"})\n",
    "    #print(abstractDiv) # check this works\n",
    "    #only get the text from the abstract div\n",
    "    abstractText = abstractDiv.p.text\n",
    "    #convert the abstract to lower case\n",
    "    abstractLower = abstractText.lower()\n",
    "    #remove punctuation\n",
    "    abstract = re.sub(r'[^\\w\\s]', '', abstractLower)\n",
    "    TitlesAbstract.append(abstract)\n",
    "    \n",
    "\n",
    "print(TitlesAbstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spatiotemporal manifold learning for human motions via longhorizon modeling datadriven modeling of human motions is ubiquitous in computer graphics and computer vision applications such as synthesizing realistic motions or recognizing actions recent research has shown that such problems can be approached by learning a natural motion manifold using deep learning on a large amount data to address the shortcomings of traditional datadriven approaches however previous deep learning methods can be suboptimal for two reasons first the skeletal information has not been fully utilized for feature extraction unlike images it is difficult to define spatial proximity in skeletal motions in the way that deep networks can be applied for feature extraction second motion is timeseries data with strong multimodal temporal correlations between frames on the one hand a frame could be followed by several candidate frames leading to different motions on the other hand longrange dependencies exist where a number of frames in the beginning correlate to a number of frames later  ineffective temporal modeling would either underestimate the multimodality and variance resulting in featureless mean motion or overestimate them resulting in jittery motions which is a major source of visual artifacts in this paper we propose a new deep network to tackle these challenges by creating a natural motion manifold that is versatile for many applications the network has a new spatial component for feature extraction it is also equipped with a new batch prediction model that predicts a large number of frames at once such that longterm temporallybased objective functions can be employed to correctly learn the motion multimodality and variances with our system longduration motions can be predictedsynthesized using an openloop setup where the motion retains the dynamics accurately it can also be used for denoising corrupted motions and synthesizing new motions with given control signals we demonstrate that our system can create superior results comparing to existing work in multiple applications a quadruple diffusion convolutional recurrent network for human motion prediction recurrent neural network rnn has become popular for human motion prediction thanks to its ability to capture temporal dependencies however it has limited capacity in modeling the complex spatial relationship in the human skeletal structure in this work we present a novel diffusion convolutional recurrent predictor for spatial and temporal movement forecasting with multistep random walks traversing bidirectionally along an adaptive graph to model interdependency among body joints in the temporal domain existing methods rely on a single forward predictor with the produced motion deflecting to the drift route which leads to error accumulations over time we propose to supplement the forward predictor with a forward discriminator to alleviate such motion drift in the long term under adversarial training the solution is further enhanced by a backward predictor and a backward discriminator to effectively reduce the error such that the system can also look into the past to improve the prediction at early frames the twoway spatial diffusion convolutions and twoway temporal predictors together form a quadruple network furthermore we train our framework by modeling the velocity from observed motion dynamics instead of static poses to predict future movements that effectively reduces the discontinuity problem at early prediction our method outperforms the state of the arts on both 3d and 2d datasets including the human36m cmu motion capture and penn action datasets the results also show that our method correctly predicts both highdynamic and lowdynamic moving trends with less motion drift ganbased reactive motion synthesis with classaware discriminators for humanhuman interaction creating realistic characters that can react to the users or another characters movement can benefit computer graphics games and virtual reality hugely however synthesizing such reactive motions in humanhuman interactions is a challenging task due to the many different ways two humans can interact while there are a number of successful researches in adapting the generative adversarial network gan in synthesizing single human actions there are very few on modelling humanhuman interactions in this paper we propose a semisupervised gan system that synthesizes the reactive motion of a character given the active motion from another character our key insights are twofold first to effectively encode the complicated spatialtemporal information of a human motion we empower the generator with a partbased long shortterm memory lstm module such that the temporal movement of different limbs can be effectively modelled we further include an attention module such that the temporal significance of the interaction can be learned which enhances the temporal alignment of the activereactive motion pair second as the reactive motion of different types of interactions can be significantly different we introduce a discriminator that not only tells if the generated movement is realistic or not but also tells the class label of the interaction this allows the use of such labels in supervising the training of the generator we experiment with the sbu and the hhoi datasets the high quality of the synthetic motion demonstrates the effective design of our generator and the discriminability of the synthesis also demonstrates the strength of our discriminator a generic framework for editing and synthesizing multimodal data with relative emotion strength emotion is considered to be a core element in performances 1 in computer animation both body motions and facial expressions are two popular mediums for a character to express the emotion however there has been limited research in studying how to effectively synthesize these two types of character movements using different levels of emotion strength with intuitive control which is difficult to be modelled effectively in this work we explore a common model that can be used to represent the emotion for the applications of body motions and facial expressions synthesis unlike previous work which encode emotions into discrete motion style descriptors we propose a continuous control indicator called emotion strength by controlling which a datadriven approach is presented to synthesize motions with fine control over emotions rather than interpolating motion features to synthesize new motion as in existing work our method explicitly learns a model mapping lowlevel motion features to the emotion strength since the motion synthesis model is learned in the training stage the computation time required for synthesizing motions at runtime is very low we further demonstrate the generality of our proposed framework by editing 2d face images using relative emotion strength as a result our method can be applied to interactive applications such as computer games image editing tools and virtual reality applications as well as offline applications such as animation and movie production  automatic sign dance synthesis from gesturebased sign language automatic dance synthesis has become more and more popular due to the increasing demand in computer games and animations existing research generates dance motions without much consideration for the context of the music in reality professional dancers make choreography according to the lyrics and music features in this research we focus on a particular genre of dance known as sign dance which combines gesturebased sign language with full body dance motion we propose a system to automatically generate sign dance from a piece of music and its corresponding sign gesture the core of the system is a sign dance model trained by multiple regression analysis to represent the correlations between sign dance and sign gesturemusic as well as a set of objective functions to evaluate the quality of the sign dance our system can be applied to music visualization allowing people with hearing difficulties to understand and enjoy music  dancedj a 3d dance animation authoring system for live performance dance is an important component of live performance for expressing emotion and presenting visual context human dance performances typically require expert knowledge of dance choreography and professional rehearsal which are too costly for casual entertainment venues and clubs recent advancements in character animation and motion synthesis have made it possible to synthesize virtual 3d dance characters in realtime the major problem in existing systems is a lack of an intuitive interfaces to control the animation for realtime dance controls we propose a new system called the dancedj to solve this problem our system consists of two parts the first part is an underlying motion analysis system that evaluates motion features including dance features such as the postures and movement tempo as well as audio features such as the music tempo and structure as a preprocess given a dancing motion database our system evaluates the quality of possible timings to connect and switch different dancing motions during runtime we propose a control interface that provides visual guidance we observe that disk jockeys djs effectively control the mixing of music using the dj controller and therefore propose a dj controller for controlling dancing characters this allows djs to transfer their skills from music control to dance control using a similar hardware setup we map different motion control functions onto the dj controller and visualize the timing of natural connection points such that the dj can effectively govern the synthesized dance motion we conducted two user experiments to evaluate the user experience and the quality of the dance character quantitative analysis shows that our system performs well in both motion control and simulation quality synthesizing motion with relative emotion strength with the advancement in motion sensing technology acquiring highquality human motions for creating realistic character animation is much easier than before since motion data itself is not the main obstacle anymore more and more effort goes into enhancing the realism of character animation such as motion styles and control in this paper we explore a less studied area the emotion of motions unlike previous work which encode emotions into discrete motion style descriptors we propose a continuous control indicator called motion strength by controlling which a datadriven approach is presented to synthesize motions with fine control over emotions rather than interpolating motion features to synthesize new motion as in existing work our method explicitly learns a model mapping lowlevel motion features to the emotion strength since the motion synthesis model is learned in the training stage the computation time required for synthesizing motions at runtime is very low as a result our method can be applied to interactive applications such as computer games and virtual reality applications as well as offline applications such as animation and movie production automatic dance generation system considering sign language information in recent years thanks to the development of 3dcg animation editing tools eg mikumikudance a lot of 3d character dance animation movies are created by amateur users however it is very difficult to create choreography from scratch without any technical knowledge shiratori et al 2006 produced the dance automatic generation system considering rhythm and intensity of dance motions however each segment is selected randomly from database so the generated dance motion has no linguistic or emotional meanings takano et al 2010 produced a human motion generation system considering motion labels however they use simple motion labels like running or jump so they cannot generate motions that express emotions in reality professional dancers make choreography based on music features or lyrics in music and express emotion or how they feel in music in our work we aim at generating more emotional dance motion easily therefore we use linguistic information in lyrics and generate dance motion depth sensor based facial and body animation control depth sensors have become one of the most popular means of generating human facial and posture information in the past decade by coupling a depth camera and computer vision based recognition algorithms these sensors can detect human facial and body features in real time such a breakthrough has fused many new research directions in animation creation and control which also has opened up new challenges in this chapter we explain how depth sensors obtain human facial and body information we then discuss on the main challenge on depth sensorbased systems which is the inaccuracy of the obtained data and explain how the problem is tackled finally we point out the emerging applications in the field in which human facial and body feature modeling and understanding is a key research problem multilayer lattice model for realtime dynamic character deformation due to the recent advancement of computer graphics hardware and software algorithms deformable characters have become more and more popular in realtime applications such as computer games while there are mature techniques to generate primary deformation from skeletal movement simulating realistic and stable secondary deformation such as jiggling of fats remains challenging on one hand traditional volumetric approaches such as the finite element method require higher computational cost and are infeasible for limited hardware such as game consoles on the other hand while shape matching based simulations can produce plausible deformation in realtime they suffer from a stiffness problem in which particles either show unrealistic deformation due to high gains or cannot catch up with the body movement in this paper we propose a unified multilayer lattice model to simulate the primary and secondary deformation of skeletondriven characters the core idea is to voxelize the input character mesh into multiple anatomical layers including the bone muscle fat and skin primary deformation is applied on the bone voxels with latticebased skinning the movement of these voxels is propagated to other voxel layers using lattice shape matching simulation creating a natural secondary deformation our multilayer lattice framework can produce simulation quality comparable to those from other volumetric approaches with a significantly smaller computational cost it is best to be applied in realtime applications such as console games or interactive animation creation  human motion variation synthesis with multivariate gaussian processes human motion variation synthesis is important for crowd simulation and interactive applications to enhance synthesis quality in this paper we propose a novel generative probabilistic model to synthesize variations of human motion our key idea is to model the conditional distribution of each joint via a multivariate gaussian process model namely semiparametric latent factor model slfm slfm can effectively model the correlations between degrees of freedom dofs of joints rather than dealing with each dof separately as implemented in existing methods a detailed evaluation is performed to show that the proposed approach can effectively synthesize variations of different types of motions motions generated by our method show a richer variation compared with existing ones finally our user study shows that the synthesized motion has a similar level of naturalness to captured human motions our method is best applied in computer games and animations to introduce motion variations topology aware datadriven inverse kinematics creating realistic human movement is a time consuming and labour intensive task the major difficulty is that the user has to edit individual joints while maintaining an overall realistic and collision free posture previous research suggests the use of datadriven inverse kinematics such that one can focus on the control of a few joints while the system automatically composes a natural posture however as a common problem of kinematics synthesis penetration of body parts is difficult to avoid in complex movements in this paper we propose a new datadriven inverse kinematics framework that conserves the topology of the synthesizing postures our system monitors and regulates the topology changes using the gauss linking integral gli such that penetration can be efficiently prevented as a result complex motions with tight body movements as well as those involving interaction with external objects can be simulated with minimal manual intervention experimental results show that using our system the user can create high quality human motion in realtime by controlling a few joints using a mouse or a multitouch screen the movement generated is both realistic and penetration free our system is best applied for interactive motion design in computer animations and games natural preparation behavior synthesis humans adjust their movements in advance to prepare for the forthcoming action resulting in an efficient and smooth transition however traditional computer animation approaches such as motion graphs simply concatenates a series of actions without taking into account the following one in this paper we propose a new method to produce preparation behaviours using reinforcement learning as an offline process the system learns the optimal way to approach a target and prepare for interaction a scalar value called the level of preparation is introduced which represents the degree of transition from the initial action to the interacting action to synthesize the movements of preparation we propose a customized motion blending scheme based on the level of preparation which is followed by an optimization framework that adjusts the posture to keep the balance during runtime the trained controller drives the character to move to a target with the appropriate level of preparation resulting in a humanlike behaviour we create scenes in which the character has to move in a complex environment and interacts with objects such as crawling under and jumping over obstacles while walking the method is useful not only for computer animation but also for realtime applications such as computer games in which the characters need to accomplish a series of tasks in a given environment preparation behaviour synthesis with reinforcement learning when humans perform a series of motions they prepare for the next motion in advance so as to enhance the response time of their movements this kind of preparation behaviour results in a natural and smooth transition of the overall movement in this paper we propose a new method to synthesize the behaviour using reinforcement learning to create preparation movements we propose a customized motion blending algorithm that is governed by a single numerical value which we called the level of preparation during the offline process the system learns the optimal way to approach a target as well as the realistic behaviour to prepare for interaction considering the level of preparation at runtime the trained controller indicates the character to move to a target with the appropriate level of preparation resulting in humanlike movements we synthesized scenes in which the character has to move in a complex environment and interact with objects such as a character crawling under and jumping over obstacles while walking the method is useful not only for computer animation but also for realtime applications such as computer games in which the characters need to accomplish a series of tasks in a given environment simulating multiple character interactions with collaborative and adversarial goals this paper proposes a new methodology for synthesizing animations of multiple characters allowing them to intelligently compete with one another in dense environments while still satisfying requirements set by an animator to achieve these two conflicting objectives simultaneously our method separately evaluates the competition and collaboration of the interactions integrating the scores to select an action that maximizes both criteria we extend the idea of minmax search normally used for strategic games such as chess using our method animators can efficiently produce scenes of dense character interactions such as those in collective sports or martial arts the method is especially effective for producing animations along story lines where the characters must follow multiple objectives while still accommodating geometric and kinematic constraints from the environment realtime physical modelling of character movements with microsoft kinect with the advancement of motion tracking hardware such as the microsoft kinect synthesizing humanlike characters with realtime captured movements becomes increasingly important traditional kinematics and dynamics approaches perform suboptimally when the captured motion is noisy or even incomplete in this paper we proposed a unified framework to control physically simulated characters with live captured motion from kinect our framework can synthesize any posture in a physical environment using external forces and torques computed by a pd controller the major problem of kinect is the incompleteness of the captured posture with some degree of freedom dof missing due to occlusions and noises we propose to search for a best matched posture from a motion database constructed in a dimensionality reduced space and substitute the missing dof to the live captured data experimental results show that our method can synthesize realistic character movements from noisy captured motion the proposed algorithm is computationally efficient and can be applied to a wide variety of interactive virtual reality applications such as motionbased gaming rehabilitation and sport training manufacturing video graphics an apparatus comprising a memory system storing a plurality of sequences each sequence comprising data for reproducing a different pattern of interactions between a respective plurality of moving characters and storing a combination data structure defining for each sequence connectability of that sequence with other ones of the plurality of sequences a processor configured to determine pairwise combination of stored sequences by selecting sequences for pairwise combination that are defined as connectable by the stored combination data structure wherein each pairwise combination has in common at least one of their respective plurality of moving characters and configured to use determined pairwise combinations of the stored sequences to produce and output video graphics comprising a series of sequences in which movable characters repetitively interact in different combinations simulating interactions among multiple characters in this thesis we attack a challenging problem in the field of character animation synthesizing interactions among multiple virtual characters in realtime although there are heavy demands in the gaming and animation industries no systemic solution has been proposed due to the difficulties to model the complex behaviors of the characterswe represent the continuous interactions among characters as a discrete markov decision process and design a general objective function to evaluate the immediate rewards of launching an action by applying game theory such as tree expansion and minmax search the optimal actions that benefit the character the most in the future are selected the simulated characters can interact competitively while achieving the requests from animators cooperativelysince the interactions between two characters depend on a lot of criteria it is difficult to exhaustively precompute the optimal actions for all variations of these criteria we design an offpolicy approach that samples and precomputes only meaningful interactions with the precomputed policy the optimal movements under different situations can be evaluated in realtimeto simulate the interactions for a large number of characters with minimal computational overhead we propose a method to precompute short durations of interactions between two characters as connectable patches the patches are concatenated spatially to generate interactions with multiple characters and temporally to generate longer interactions based on the optional instructions given by the animators our system automatically applies concatenations to create a huge scene of interacting crowdwe demonstrate our system by creating scenes with high quality interactions on one hand our algorithm can automatically generate artistic scenes of interactions such as the fighting scenes in movies that involve hundreds of characters on the other hand it can create controllable intelligent characters that interact with the opponents for realtime applications such as 3d computer games angular momentum guided motion concatenation in this paper we propose a new method to concatenate two dynamic fullbody motions such as punches kicks and flips by using the angular momentum as a cue through the observation of real humans we have identified two patterns of angular momentum that make the transition of such motions efficient based on these observations we propose a new method to concatenate two fullbody motions in a natural manner our method is useful for applications where dynamic fullbody motions are required such as 3d computer games and animations interaction patches for multicharacter animation we propose a datadriven approach to automatically generate a scene where tens to hundreds of characters densely interact with each other during offline processing the close interactions between characters are precomputed by expanding a game tree and these are stored as data structures called interaction patches then during runtime the system spatiotemporally concatenates the interaction patches to create scenes where a large number of characters closely interact with one another using our method it is possible to automatically or interactively produce animations of crowds interacting with each other in a stylized way the method can be used for a variety of applications including tv programs advertisements and movies simulating interactions of avatars in high dimensional state space efficient computation of strategic movements is essential to control virtual avatars intelligently in computer games and 3d virtual environments such a module is needed to control nonplayer characters npcs to fight play team sports or move through a mass crowd reinforcement learning is an approach to achieve realtime optimal control however the huge state space of human interactions makes it difficult to apply existing learning methods to control avatars when they have dense interactions with other characters in this research we propose a new methodology to efciently plan the movements of an avatar interacting with another we make use of the fact that the subspace of meaningful interactions is much smaller than the whole state space of two avatars we efficiently collect samples by exploring the subspace where dense interactions between the avatars occur and favor samples that have high connectivity with the other samples using the collected samples a nite state machine fsm called interaction graph is composed at runtime we compute the optimal action of each avatar by minmax search or dynamic programming on the interaction graph the methodology is applicable to control npcs in ghting and ballsports games simulating interactions of characters it is difficult to create scenes where multiple characters densely interact with each other manually creating the motions of characters is time consuming due to the correlation of the movements between the characters capturing the motions of multiple characters is also difficult as it requires a huge amount of postprocessing of the data in this paper we explain the methods we have proposed to simulate close interactions of characters based on singly captured motions we propose methods to 1 control characters intelligently to cooperativelycompetitively interact with the other characters and 2 generate movements that include close interactions such as tangling the segments with the others by taking into account the topological relationship of the characters simulating competitive interactions using singly captured motions it is difficult to create scenes where multiple avatars are fighting  competing with each other manually creating the motions of avatars is time consuming due to the correlation of the movements between the avatars capturing the motions of multiple avatars is also difficult as it requires a huge amount of postprocessing in this paper we propose a new method to generate a realistic scene of avatars densely interacting in a competitive environment the motions of the avatars are considered to be captured individually which will increase the easiness of obtaining the data we propose a new algorithm called the temporal expansion approach which maps the continuous time action plan to a discrete space such that turnbased evaluation methods can be used as a result many mature algorithms in game such as the minmax search and alphabeta pruning can be appliedusing our method avatars will plan their strategies taking into account the reaction of the opponent fighting scenes with multiple avatars are generated to demonstrate the effectiveness of our algorithm the proposed method can also be applied to other kinds of continuous activities that require strategy planning such as sport games generating realistic fighting scenes by game tree recently there have been a lot of researches to synthesizeedit the motion of a single avatar in the virtual environment however there has not been so much work of simulating continuous interactions of multiple avatars such as fighting in this paper we propose a new method to generate a realistic fighting scene based on motion capture data we propose a new algorithm called the temporal expansion approach which maps the continuous time action plan to a discrete causality space such that turnbased evaluation methods can be used as a result it is possible to use many mature algorithms available in strategy games such as the minimax algorithm and αβ pruning we also propose a method to generate and use an offensedefense table which illustrates the spatialtemporal relationship of attacks and dodges to incorporate tactical maneuvers of defense into the scene using our method avatars will plan their strategies taking into account the reaction of the opponent fighting scenes with multiple avatars are generated to demonstrate the effectiveness of our algorithm the proposed method can also be applied to other kinds of continuous activities that require strategy planning such as sport games technical note generating realistic fighting scenes by game tree recently there have been a lot of researches to synthesizeedit the motion of a single avatar in the virtual environment however there has not been so much work of simulating continuous interactions of multiple avatars such as fighting in this paper we propose a new method to generate a realistic fighting scene based on motion capture data we propose a new algorithm called the temporal expansion approach which maps the continuous time action plan to a discrete causality space such that turnbased evaluation methods can be used as a result it is possible to use many mature algorithms available in strategy games such as the minimax algorithm and αβ pruning we also propose a method to generate and use an offensedefense table which illustrates the spatialtemporal relationship of attacks and dodges to incorporate tactical maneuvers of defense into the scene using our method avatars will plan their strategies taking into account the reaction of the opponent fighting scenes with multiple avatars are generated to demonstrate the effectiveness of our algorithm the proposed method can also be applied to other kinds of continuous activities that require strategy planning such as sport games\n"
     ]
    }
   ],
   "source": [
    "#converting the list of titles and abstracts into one string\n",
    "s = TitlesAbstract\n",
    "AsString = ' '.join([str(elem) for elem in s])\n",
    "print(AsString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spatiotemporal manifold learning human motions longhorizon modeling datadriven modeling human motions ubiquitous computer graphics computer vision applications synthesizing realistic motions recognizing actions recent research shown problems approached learning natural motion manifold deep learning large amount data address shortcomings traditional datadriven previous deep learning methods suboptimal reasons first skeletal information fully utilized feature extraction unlike images difficult define spatial proximity skeletal motions way deep networks applied feature extraction second motion timeseries data strong multimodal temporal correlations frames hand frame followed several candidate frames leading different motions hand longrange dependencies exist number frames beginning correlate number frames later ineffective temporal modeling underestimate multimodality variance featureless mean motion overestimate them jittery motions major source visual artifacts paper deep network tackle challenges creating natural motion manifold versatile applications network spatial component feature extraction equipped batch prediction model predicts large number frames longterm temporallybased objective functions employed correctly learn motion multimodality variances system longduration motions predictedsynthesized openloop setup motion retains dynamics accurately denoising corrupted motions synthesizing motions control signals system create superior results comparing existing work multiple applications quadruple diffusion convolutional recurrent network human motion prediction recurrent neural network rnn become popular human motion prediction thanks its ability capture temporal dependencies limited capacity modeling complex spatial relationship human skeletal structure work present novel diffusion convolutional recurrent predictor spatial temporal movement forecasting multistep random walks traversing bidirectionally along adaptive graph model interdependency among body joints temporal domain existing methods rely single forward predictor produced motion deflecting drift route leads error accumulations time supplement forward predictor forward discriminator alleviate motion drift term under adversarial training solution further enhanced backward predictor backward discriminator effectively reduce error system look past improve prediction early frames twoway spatial diffusion convolutions twoway temporal predictors together form quadruple network train framework modeling velocity observed motion dynamics instead static poses predict future movements effectively reduces discontinuity problem early prediction method outperforms state arts 3d 2d datasets including human36m cmu motion capture penn action datasets results method correctly predicts highdynamic lowdynamic moving trends less motion drift ganbased reactive motion synthesis classaware discriminators humanhuman interaction creating realistic characters react users another characters movement benefit computer graphics games virtual reality hugely synthesizing reactive motions humanhuman interactions challenging task different ways humans interact number successful researches adapting generative adversarial network gan synthesizing single human actions very few modelling humanhuman interactions paper semisupervised gan system synthesizes reactive motion character active motion another character key insights twofold first effectively encode complicated spatialtemporal information human motion empower generator partbased shortterm memory lstm module temporal movement different limbs effectively modelled further include attention module temporal significance interaction learned enhances temporal alignment activereactive motion pair second reactive motion different types interactions significantly different introduce discriminator tells if generated movement realistic tells class label interaction allows labels supervising training generator experiment sbu hhoi datasets high quality synthetic motion demonstrates effective design generator discriminability synthesis demonstrates strength discriminator generic framework editing synthesizing multimodal data relative emotion strength emotion considered core element performances 1 computer animation body motions facial expressions popular mediums character express emotion limited research studying how effectively synthesize types character movements different levels emotion strength intuitive control difficult modelled effectively work explore common model represent emotion applications body motions facial expressions synthesis unlike previous work encode emotions discrete motion style descriptors continuous control indicator emotion strength controlling datadriven approach presented synthesize motions fine control emotions interpolating motion features synthesize motion existing work method explicitly learns model mapping lowlevel motion features emotion strength motion synthesis model learned training stage computation time required synthesizing motions runtime very low further generality framework editing 2d face images relative emotion strength result method applied interactive applications computer games image editing tools virtual reality applications offline applications animation movie production automatic sign dance synthesis gesturebased sign language automatic dance synthesis become popular increasing demand computer games animations existing research generates dance motions without consideration context music reality professional dancers make choreography according lyrics music features research focus particular genre dance known sign dance combines gesturebased sign language full body dance motion system automatically generate sign dance piece music its corresponding sign gesture core system sign dance model trained multiple regression analysis represent correlations sign dance sign gesturemusic set objective functions evaluate quality sign dance system applied music visualization allowing people hearing difficulties understand enjoy music dancedj 3d dance animation authoring system live performance dance important component live performance expressing emotion presenting visual context human dance performances typically require expert knowledge dance choreography professional rehearsal too costly casual entertainment venues clubs recent advancements character animation motion synthesis made possible synthesize virtual 3d dance characters realtime major problem existing systems lack intuitive interfaces control animation realtime dance controls system dancedj solve problem system consists parts first part underlying motion analysis system evaluates motion features including dance features postures movement tempo audio features music tempo structure preprocess dancing motion database system evaluates quality possible timings connect switch different dancing motions during runtime control interface provides visual guidance observe disk jockeys djs effectively control mixing music dj controller therefore dj controller controlling dancing characters allows djs transfer skills music control dance control similar hardware setup map different motion control functions onto dj controller visualize timing natural connection points dj effectively govern synthesized dance motion conducted user experiments evaluate user experience quality dance character quantitative analysis shows system performs motion control simulation quality synthesizing motion relative emotion strength advancement motion sensing technology acquiring highquality human motions creating realistic character animation easier before motion data itself main obstacle anymore effort goes enhancing realism character animation motion styles control paper explore less studied area emotion motions unlike previous work encode emotions discrete motion style descriptors continuous control indicator motion strength controlling datadriven approach presented synthesize motions fine control emotions interpolating motion features synthesize motion existing work method explicitly learns model mapping lowlevel motion features emotion strength motion synthesis model learned training stage computation time required synthesizing motions runtime very low result method applied interactive applications computer games virtual reality applications offline applications animation movie production automatic dance generation system considering sign language information recent years thanks development 3dcg animation editing tools eg mikumikudance lot 3d character dance animation movies created amateur users very difficult create choreography scratch without any technical knowledge shiratori et al 2006 produced dance automatic generation system considering rhythm intensity dance motions segment selected randomly database generated dance motion no linguistic emotional meanings takano et al 2010 produced human motion generation system considering motion labels simple motion labels like running jump cannot generate motions express emotions reality professional dancers make choreography based music features lyrics music express emotion how feel music work aim generating emotional dance motion easily therefore linguistic information lyrics generate dance motion depth sensor based facial body animation control depth sensors become most popular means generating human facial posture information past decade coupling depth camera computer vision based recognition algorithms sensors detect human facial body features real time breakthrough fused research directions animation creation control opened up challenges chapter explain how depth sensors obtain human facial body information then discuss main challenge depth sensorbased systems inaccuracy obtained data explain how problem tackled finally point out emerging applications field human facial body feature modeling understanding key research problem multilayer lattice model realtime dynamic character deformation recent advancement computer graphics hardware software algorithms deformable characters become popular realtime applications computer games mature techniques generate primary deformation skeletal movement simulating realistic stable secondary deformation jiggling fats remains challenging hand traditional volumetric finite element method require higher computational cost infeasible limited hardware game consoles hand shape matching based simulations plausible deformation realtime suffer stiffness problem particles unrealistic deformation high gains cannot catch up body movement paper unified multilayer lattice model simulate primary secondary deformation skeletondriven characters core idea voxelize input character mesh multiple anatomical layers including bone muscle fat skin primary deformation applied bone voxels latticebased skinning movement voxels propagated voxel layers lattice shape matching simulation creating natural secondary deformation multilayer lattice framework simulation quality comparable those volumetric significantly smaller computational cost best applied realtime applications console games interactive animation creation human motion variation synthesis multivariate gaussian processes human motion variation synthesis important crowd simulation interactive applications enhance synthesis quality paper novel generative probabilistic model synthesize variations human motion key idea model conditional distribution joint multivariate gaussian process model namely semiparametric latent factor model slfm slfm effectively model correlations degrees freedom dofs joints dealing dof separately implemented existing methods detailed evaluation performed approach effectively synthesize variations different types motions motions generated method richer variation compared existing ones finally user study shows synthesized motion similar level naturalness captured human motions method best applied computer games animations introduce motion variations topology aware datadriven inverse kinematics creating realistic human movement time consuming labour intensive task major difficulty user edit individual joints maintaining overall realistic collision free posture previous research suggests datadriven inverse kinematics focus control few joints system automatically composes natural posture common problem kinematics synthesis penetration body parts difficult avoid complex movements paper datadriven inverse kinematics framework conserves topology synthesizing postures system monitors regulates topology changes gauss linking integral gli penetration efficiently prevented result complex motions tight body movements those involving interaction external objects simulated minimal manual intervention experimental results system user create high quality human motion realtime controlling few joints mouse multitouch screen movement generated realistic penetration free system best applied interactive motion design computer animations games natural preparation behavior synthesis humans adjust movements advance prepare forthcoming action efficient smooth transition traditional computer animation motion graphs simply concatenates series actions without taking account following paper method preparation behaviours reinforcement learning offline process system learns optimal way approach target prepare interaction scalar value level preparation introduced represents degree transition initial action interacting action synthesize movements preparation customized motion blending scheme based level preparation followed optimization framework adjusts posture keep balance during runtime trained controller drives character move target appropriate level preparation humanlike behaviour create scenes character move complex environment interacts objects crawling under jumping obstacles walking method computer animation realtime applications computer games characters accomplish series tasks environment preparation behaviour synthesis reinforcement learning humans perform series motions prepare next motion advance enhance response time movements kind preparation behaviour results natural smooth transition overall movement paper method synthesize behaviour reinforcement learning create preparation movements customized motion blending algorithm governed single numerical value level preparation during offline process system learns optimal way approach target realistic behaviour prepare interaction considering level preparation runtime trained controller indicates character move target appropriate level preparation humanlike movements synthesized scenes character move complex environment interact objects character crawling under jumping obstacles walking method computer animation realtime applications computer games characters accomplish series tasks environment simulating multiple character interactions collaborative adversarial goals paper proposes methodology synthesizing animations multiple characters allowing them intelligently compete another dense environments still satisfying requirements set animator achieve conflicting objectives simultaneously method separately evaluates competition collaboration interactions integrating scores select action maximizes criteria extend idea minmax search normally strategic games chess method animators efficiently scenes dense character interactions those collective sports martial arts method especially effective producing animations along story lines characters must follow multiple objectives still accommodating geometric kinematic constraints environment realtime physical modelling character movements microsoft kinect advancement motion tracking hardware microsoft kinect synthesizing humanlike characters realtime captured movements becomes increasingly important traditional kinematics dynamics perform suboptimally captured motion noisy even incomplete paper unified framework control physically simulated characters live captured motion kinect framework synthesize any posture physical environment external forces torques computed pd controller major problem kinect incompleteness captured posture some degree freedom dof missing occlusions noises search best matched posture motion database constructed dimensionality reduced space substitute missing dof live captured data experimental results method synthesize realistic character movements noisy captured motion algorithm computationally efficient applied wide variety interactive virtual reality applications motionbased gaming rehabilitation sport training manufacturing video graphics apparatus comprising memory system storing plurality sequences sequence comprising data reproducing different pattern interactions respective plurality moving characters storing combination data structure defining sequence connectability sequence ones plurality sequences processor configured determine pairwise combination stored sequences selecting sequences pairwise combination defined connectable stored combination data structure wherein pairwise combination common least respective plurality moving characters configured determined pairwise combinations stored sequences output video graphics comprising series sequences movable characters repetitively interact different combinations simulating interactions among multiple characters thesis attack challenging problem field character animation synthesizing interactions among multiple virtual characters realtime although heavy demands gaming animation industries no systemic solution difficulties model complex behaviors characterswe represent continuous interactions among characters discrete markov decision process design general objective function evaluate immediate rewards launching action applying game theory tree expansion minmax search optimal actions benefit character most future selected simulated characters interact competitively achieving requests animators cooperativelysince interactions characters depend lot criteria difficult exhaustively precompute optimal actions all variations criteria design offpolicy approach samples precomputes meaningful interactions precomputed policy optimal movements under different situations evaluated realtimeto simulate interactions large number characters minimal computational overhead method precompute short durations interactions characters connectable patches patches concatenated spatially generate interactions multiple characters temporally generate longer interactions based optional instructions animators system automatically applies concatenations create huge scene interacting crowdwe system creating scenes high quality interactions hand algorithm automatically generate artistic scenes interactions fighting scenes movies involve hundreds characters hand create controllable intelligent characters interact opponents realtime applications 3d computer games angular momentum guided motion concatenation paper method concatenate dynamic fullbody motions punches kicks flips angular momentum cue through observation real humans identified patterns angular momentum make transition motions efficient based observations method concatenate fullbody motions natural manner method applications dynamic fullbody motions required 3d computer games animations interaction patches multicharacter animation datadriven approach automatically generate scene tens hundreds characters densely interact during offline processing close interactions characters precomputed expanding game tree stored data structures interaction patches then during runtime system spatiotemporally concatenates interaction patches create scenes large number characters closely interact another method possible automatically interactively animations crowds interacting stylized way method variety applications including tv programs advertisements movies simulating interactions avatars high dimensional state space efficient computation strategic movements essential control virtual avatars intelligently computer games 3d virtual environments module needed control nonplayer characters npcs fight play team sports move through mass crowd reinforcement learning approach achieve realtime optimal control huge state space human interactions makes difficult apply existing learning methods control avatars dense interactions characters research methodology efciently plan movements avatar interacting another make fact subspace meaningful interactions smaller whole state space avatars efficiently collect samples exploring subspace dense interactions avatars occur favor samples high connectivity samples collected samples nite state machine fsm interaction graph composed runtime compute optimal action avatar minmax search dynamic programming interaction graph methodology applicable control npcs ghting ballsports games simulating interactions characters difficult create scenes multiple characters densely interact manually creating motions characters time consuming correlation movements characters capturing motions multiple characters difficult requires huge amount postprocessing data paper explain methods simulate close interactions characters based singly captured motions methods 1 control characters intelligently cooperativelycompetitively interact characters 2 generate movements include close interactions tangling segments others taking account topological relationship characters simulating competitive interactions singly captured motions difficult create scenes multiple avatars fighting competing manually creating motions avatars time consuming correlation movements avatars capturing motions multiple avatars difficult requires huge amount postprocessing paper method generate realistic scene avatars densely interacting competitive environment motions avatars considered captured individually increase easiness obtaining data algorithm temporal expansion approach maps continuous time action plan discrete space turnbased evaluation methods result mature algorithms game minmax search alphabeta pruning appliedusing method avatars plan strategies taking account reaction opponent fighting scenes multiple avatars generated effectiveness algorithm method applied kinds continuous activities require strategy planning sport games generating realistic fighting scenes game tree recently lot researches synthesizeedit motion single avatar virtual environment work simulating continuous interactions multiple avatars fighting paper method generate realistic fighting scene based motion capture data algorithm temporal expansion approach maps continuous time action plan discrete causality space turnbased evaluation methods result possible mature algorithms available strategy games minimax algorithm αβ pruning method generate offensedefense table illustrates spatialtemporal relationship attacks dodges incorporate tactical maneuvers defense scene method avatars plan strategies taking account reaction opponent fighting scenes multiple avatars generated effectiveness algorithm method applied kinds continuous activities require strategy planning sport games technical note generating realistic fighting scenes game tree recently lot researches synthesizeedit motion single avatar virtual environment work simulating continuous interactions multiple avatars fighting paper method generate realistic fighting scene based motion capture data algorithm temporal expansion approach maps continuous time action plan discrete causality space turnbased evaluation methods result possible mature algorithms available strategy games minimax algorithm αβ pruning method generate offensedefense table illustrates spatialtemporal relationship attacks dodges incorporate tactical maneuvers defense scene method avatars plan strategies taking account reaction opponent fighting scenes multiple avatars generated effectiveness algorithm method applied kinds continuous activities require strategy planning sport games\n"
     ]
    }
   ],
   "source": [
    "query = AsString\n",
    "# Articles, sum exist, Pronouns excpet self unsure as this may be conceptually important in autonomy , Coordinating conjunctions, Correlative conjunctions, Conjunctions of time, Modal Nouns, Modal Adjectives, Verbs of perception, subjunctives\n",
    "\n",
    "\n",
    "stopwords = ['the', 'a','an','it', 'each',\n",
    "             'be','is','will','am','are','been','have','were','has','was',\n",
    "             'mine', 'he', 'his', 'her', 'hers', 'you', 'your', 'they', 'theyre', 'their', 'theirs', 'we', 'our','that','this','these',\n",
    "             'for', 'and', 'nor', 'but', 'or', 'yet', 'so', 'by', 'in', 'with', 'of', 'via', 'to','into','due',\n",
    "             'either', 'or', 'not', 'only', 'both', 'whether', 'just', 'the', 'as', 'much', 'sooner', 'rather', 'than', 'which','such','other',\n",
    "             'after', 'soon', 'long', 'now', 'once', 'since', 'till', 'untill', 'when', 'whenever', 'while', 'from','while','during'\n",
    "             'necessity', 'requirement', 'obligation', 'possibility',\n",
    "             'perhaps', 'possibly', 'certainly', 'definitely', 'given',\n",
    "             'suggest', 'seem', 'appear', 'suppose', 'need', 'indicate', 'imply', 'can', 'called','show','reveal','however','demonstrate','propose','proposed','produce', 'resulting', 'approaches',\n",
    "             'also','aditionallly', 'moreover','furthermore','more','however',\n",
    "             'there', 'here','over','near','at','on','by','where','between',\n",
    "             'used','use','using','well','useful','able',\n",
    "             'could','should','would','might', 'may', 'shall',\n",
    "             'many', 'one', 'two', 'three', 'four',\n",
    "             'new',\n",
    "            ]\n",
    "\n",
    "querywords = query.split()\n",
    "\n",
    "resultwords  = [word for word in querywords if word not in stopwords]\n",
    "result = ' '.join(resultwords)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spatiotemporal', 'manifold', 'learning', 'human', 'motions', 'longhorizon', 'modeling', 'datadriven', 'modeling', 'human', 'motions', 'ubiquitous', 'computer', 'graphics', 'computer', 'vision', 'applications', 'synthesizing', 'realistic', 'motions', 'recognizing', 'actions', 'recent', 'research', 'shown', 'problems', 'approached', 'learning', 'natural', 'motion', 'manifold', 'deep', 'learning', 'large', 'amount', 'data', 'address', 'shortcomings', 'traditional', 'datadriven', 'previous', 'deep', 'learning', 'methods', 'suboptimal', 'reasons', 'first', 'skeletal', 'information', 'fully', 'utilized', 'feature', 'extraction', 'unlike', 'images', 'difficult', 'define', 'spatial', 'proximity', 'skeletal', 'motions', 'way', 'deep', 'networks', 'applied', 'feature', 'extraction', 'second', 'motion', 'timeseries', 'data', 'strong', 'multimodal', 'temporal', 'correlations', 'frames', 'hand', 'frame', 'followed', 'several', 'candidate', 'frames', 'leading', 'different', 'motions', 'hand', 'longrange', 'dependencies', 'exist', 'number', 'frames', 'beginning', 'correlate', 'number', 'frames', 'later', 'ineffective', 'temporal', 'modeling', 'underestimate', 'multimodality', 'variance', 'featureless', 'mean', 'motion', 'overestimate', 'them', 'jittery', 'motions', 'major', 'source', 'visual', 'artifacts', 'paper', 'deep', 'network', 'tackle', 'challenges', 'creating', 'natural', 'motion', 'manifold', 'versatile', 'applications', 'network', 'spatial', 'component', 'feature', 'extraction', 'equipped', 'batch', 'prediction', 'model', 'predicts', 'large', 'number', 'frames', 'longterm', 'temporallybased', 'objective', 'functions', 'employed', 'correctly', 'learn', 'motion', 'multimodality', 'variances', 'system', 'longduration', 'motions', 'predictedsynthesized', 'openloop', 'setup', 'motion', 'retains', 'dynamics', 'accurately', 'denoising', 'corrupted', 'motions', 'synthesizing', 'motions', 'control', 'signals', 'system', 'create', 'superior', 'results', 'comparing', 'existing', 'work', 'multiple', 'applications', 'quadruple', 'diffusion', 'convolutional', 'recurrent', 'network', 'human', 'motion', 'prediction', 'recurrent', 'neural', 'network', 'rnn', 'become', 'popular', 'human', 'motion', 'prediction', 'thanks', 'its', 'ability', 'capture', 'temporal', 'dependencies', 'limited', 'capacity', 'modeling', 'complex', 'spatial', 'relationship', 'human', 'skeletal', 'structure', 'work', 'present', 'novel', 'diffusion', 'convolutional', 'recurrent', 'predictor', 'spatial', 'temporal', 'movement', 'forecasting', 'multistep', 'random', 'walks', 'traversing', 'bidirectionally', 'along', 'adaptive', 'graph', 'model', 'interdependency', 'among', 'body', 'joints', 'temporal', 'domain', 'existing', 'methods', 'rely', 'single', 'forward', 'predictor', 'produced', 'motion', 'deflecting', 'drift', 'route', 'leads', 'error', 'accumulations', 'time', 'supplement', 'forward', 'predictor', 'forward', 'discriminator', 'alleviate', 'motion', 'drift', 'term', 'under', 'adversarial', 'training', 'solution', 'further', 'enhanced', 'backward', 'predictor', 'backward', 'discriminator', 'effectively', 'reduce', 'error', 'system', 'look', 'past', 'improve', 'prediction', 'early', 'frames', 'twoway', 'spatial', 'diffusion', 'convolutions', 'twoway', 'temporal', 'predictors', 'together', 'form', 'quadruple', 'network', 'train', 'framework', 'modeling', 'velocity', 'observed', 'motion', 'dynamics', 'instead', 'static', 'poses', 'predict', 'future', 'movements', 'effectively', 'reduces', 'discontinuity', 'problem', 'early', 'prediction', 'method', 'outperforms', 'state', 'arts', '3d', '2d', 'datasets', 'including', 'human36m', 'cmu', 'motion', 'capture', 'penn', 'action', 'datasets', 'results', 'method', 'correctly', 'predicts', 'highdynamic', 'lowdynamic', 'moving', 'trends', 'less', 'motion', 'drift', 'ganbased', 'reactive', 'motion', 'synthesis', 'classaware', 'discriminators', 'humanhuman', 'interaction', 'creating', 'realistic', 'characters', 'react', 'users', 'another', 'characters', 'movement', 'benefit', 'computer', 'graphics', 'games', 'virtual', 'reality', 'hugely', 'synthesizing', 'reactive', 'motions', 'humanhuman', 'interactions', 'challenging', 'task', 'different', 'ways', 'humans', 'interact', 'number', 'successful', 'researches', 'adapting', 'generative', 'adversarial', 'network', 'gan', 'synthesizing', 'single', 'human', 'actions', 'very', 'few', 'modelling', 'humanhuman', 'interactions', 'paper', 'semisupervised', 'gan', 'system', 'synthesizes', 'reactive', 'motion', 'character', 'active', 'motion', 'another', 'character', 'key', 'insights', 'twofold', 'first', 'effectively', 'encode', 'complicated', 'spatialtemporal', 'information', 'human', 'motion', 'empower', 'generator', 'partbased', 'shortterm', 'memory', 'lstm', 'module', 'temporal', 'movement', 'different', 'limbs', 'effectively', 'modelled', 'further', 'include', 'attention', 'module', 'temporal', 'significance', 'interaction', 'learned', 'enhances', 'temporal', 'alignment', 'activereactive', 'motion', 'pair', 'second', 'reactive', 'motion', 'different', 'types', 'interactions', 'significantly', 'different', 'introduce', 'discriminator', 'tells', 'if', 'generated', 'movement', 'realistic', 'tells', 'class', 'label', 'interaction', 'allows', 'labels', 'supervising', 'training', 'generator', 'experiment', 'sbu', 'hhoi', 'datasets', 'high', 'quality', 'synthetic', 'motion', 'demonstrates', 'effective', 'design', 'generator', 'discriminability', 'synthesis', 'demonstrates', 'strength', 'discriminator', 'generic', 'framework', 'editing', 'synthesizing', 'multimodal', 'data', 'relative', 'emotion', 'strength', 'emotion', 'considered', 'core', 'element', 'performances', '1', 'computer', 'animation', 'body', 'motions', 'facial', 'expressions', 'popular', 'mediums', 'character', 'express', 'emotion', 'limited', 'research', 'studying', 'how', 'effectively', 'synthesize', 'types', 'character', 'movements', 'different', 'levels', 'emotion', 'strength', 'intuitive', 'control', 'difficult', 'modelled', 'effectively', 'work', 'explore', 'common', 'model', 'represent', 'emotion', 'applications', 'body', 'motions', 'facial', 'expressions', 'synthesis', 'unlike', 'previous', 'work', 'encode', 'emotions', 'discrete', 'motion', 'style', 'descriptors', 'continuous', 'control', 'indicator', 'emotion', 'strength', 'controlling', 'datadriven', 'approach', 'presented', 'synthesize', 'motions', 'fine', 'control', 'emotions', 'interpolating', 'motion', 'features', 'synthesize', 'motion', 'existing', 'work', 'method', 'explicitly', 'learns', 'model', 'mapping', 'lowlevel', 'motion', 'features', 'emotion', 'strength', 'motion', 'synthesis', 'model', 'learned', 'training', 'stage', 'computation', 'time', 'required', 'synthesizing', 'motions', 'runtime', 'very', 'low', 'further', 'generality', 'framework', 'editing', '2d', 'face', 'images', 'relative', 'emotion', 'strength', 'result', 'method', 'applied', 'interactive', 'applications', 'computer', 'games', 'image', 'editing', 'tools', 'virtual', 'reality', 'applications', 'offline', 'applications', 'animation', 'movie', 'production', 'automatic', 'sign', 'dance', 'synthesis', 'gesturebased', 'sign', 'language', 'automatic', 'dance', 'synthesis', 'become', 'popular', 'increasing', 'demand', 'computer', 'games', 'animations', 'existing', 'research', 'generates', 'dance', 'motions', 'without', 'consideration', 'context', 'music', 'reality', 'professional', 'dancers', 'make', 'choreography', 'according', 'lyrics', 'music', 'features', 'research', 'focus', 'particular', 'genre', 'dance', 'known', 'sign', 'dance', 'combines', 'gesturebased', 'sign', 'language', 'full', 'body', 'dance', 'motion', 'system', 'automatically', 'generate', 'sign', 'dance', 'piece', 'music', 'its', 'corresponding', 'sign', 'gesture', 'core', 'system', 'sign', 'dance', 'model', 'trained', 'multiple', 'regression', 'analysis', 'represent', 'correlations', 'sign', 'dance', 'sign', 'gesturemusic', 'set', 'objective', 'functions', 'evaluate', 'quality', 'sign', 'dance', 'system', 'applied', 'music', 'visualization', 'allowing', 'people', 'hearing', 'difficulties', 'understand', 'enjoy', 'music', 'dancedj', '3d', 'dance', 'animation', 'authoring', 'system', 'live', 'performance', 'dance', 'important', 'component', 'live', 'performance', 'expressing', 'emotion', 'presenting', 'visual', 'context', 'human', 'dance', 'performances', 'typically', 'require', 'expert', 'knowledge', 'dance', 'choreography', 'professional', 'rehearsal', 'too', 'costly', 'casual', 'entertainment', 'venues', 'clubs', 'recent', 'advancements', 'character', 'animation', 'motion', 'synthesis', 'made', 'possible', 'synthesize', 'virtual', '3d', 'dance', 'characters', 'realtime', 'major', 'problem', 'existing', 'systems', 'lack', 'intuitive', 'interfaces', 'control', 'animation', 'realtime', 'dance', 'controls', 'system', 'dancedj', 'solve', 'problem', 'system', 'consists', 'parts', 'first', 'part', 'underlying', 'motion', 'analysis', 'system', 'evaluates', 'motion', 'features', 'including', 'dance', 'features', 'postures', 'movement', 'tempo', 'audio', 'features', 'music', 'tempo', 'structure', 'preprocess', 'dancing', 'motion', 'database', 'system', 'evaluates', 'quality', 'possible', 'timings', 'connect', 'switch', 'different', 'dancing', 'motions', 'during', 'runtime', 'control', 'interface', 'provides', 'visual', 'guidance', 'observe', 'disk', 'jockeys', 'djs', 'effectively', 'control', 'mixing', 'music', 'dj', 'controller', 'therefore', 'dj', 'controller', 'controlling', 'dancing', 'characters', 'allows', 'djs', 'transfer', 'skills', 'music', 'control', 'dance', 'control', 'similar', 'hardware', 'setup', 'map', 'different', 'motion', 'control', 'functions', 'onto', 'dj', 'controller', 'visualize', 'timing', 'natural', 'connection', 'points', 'dj', 'effectively', 'govern', 'synthesized', 'dance', 'motion', 'conducted', 'user', 'experiments', 'evaluate', 'user', 'experience', 'quality', 'dance', 'character', 'quantitative', 'analysis', 'shows', 'system', 'performs', 'motion', 'control', 'simulation', 'quality', 'synthesizing', 'motion', 'relative', 'emotion', 'strength', 'advancement', 'motion', 'sensing', 'technology', 'acquiring', 'highquality', 'human', 'motions', 'creating', 'realistic', 'character', 'animation', 'easier', 'before', 'motion', 'data', 'itself', 'main', 'obstacle', 'anymore', 'effort', 'goes', 'enhancing', 'realism', 'character', 'animation', 'motion', 'styles', 'control', 'paper', 'explore', 'less', 'studied', 'area', 'emotion', 'motions', 'unlike', 'previous', 'work', 'encode', 'emotions', 'discrete', 'motion', 'style', 'descriptors', 'continuous', 'control', 'indicator', 'motion', 'strength', 'controlling', 'datadriven', 'approach', 'presented', 'synthesize', 'motions', 'fine', 'control', 'emotions', 'interpolating', 'motion', 'features', 'synthesize', 'motion', 'existing', 'work', 'method', 'explicitly', 'learns', 'model', 'mapping', 'lowlevel', 'motion', 'features', 'emotion', 'strength', 'motion', 'synthesis', 'model', 'learned', 'training', 'stage', 'computation', 'time', 'required', 'synthesizing', 'motions', 'runtime', 'very', 'low', 'result', 'method', 'applied', 'interactive', 'applications', 'computer', 'games', 'virtual', 'reality', 'applications', 'offline', 'applications', 'animation', 'movie', 'production', 'automatic', 'dance', 'generation', 'system', 'considering', 'sign', 'language', 'information', 'recent', 'years', 'thanks', 'development', '3dcg', 'animation', 'editing', 'tools', 'eg', 'mikumikudance', 'lot', '3d', 'character', 'dance', 'animation', 'movies', 'created', 'amateur', 'users', 'very', 'difficult', 'create', 'choreography', 'scratch', 'without', 'any', 'technical', 'knowledge', 'shiratori', 'et', 'al', '2006', 'produced', 'dance', 'automatic', 'generation', 'system', 'considering', 'rhythm', 'intensity', 'dance', 'motions', 'segment', 'selected', 'randomly', 'database', 'generated', 'dance', 'motion', 'no', 'linguistic', 'emotional', 'meanings', 'takano', 'et', 'al', '2010', 'produced', 'human', 'motion', 'generation', 'system', 'considering', 'motion', 'labels', 'simple', 'motion', 'labels', 'like', 'running', 'jump', 'cannot', 'generate', 'motions', 'express', 'emotions', 'reality', 'professional', 'dancers', 'make', 'choreography', 'based', 'music', 'features', 'lyrics', 'music', 'express', 'emotion', 'how', 'feel', 'music', 'work', 'aim', 'generating', 'emotional', 'dance', 'motion', 'easily', 'therefore', 'linguistic', 'information', 'lyrics', 'generate', 'dance', 'motion', 'depth', 'sensor', 'based', 'facial', 'body', 'animation', 'control', 'depth', 'sensors', 'become', 'most', 'popular', 'means', 'generating', 'human', 'facial', 'posture', 'information', 'past', 'decade', 'coupling', 'depth', 'camera', 'computer', 'vision', 'based', 'recognition', 'algorithms', 'sensors', 'detect', 'human', 'facial', 'body', 'features', 'real', 'time', 'breakthrough', 'fused', 'research', 'directions', 'animation', 'creation', 'control', 'opened', 'up', 'challenges', 'chapter', 'explain', 'how', 'depth', 'sensors', 'obtain', 'human', 'facial', 'body', 'information', 'then', 'discuss', 'main', 'challenge', 'depth', 'sensorbased', 'systems', 'inaccuracy', 'obtained', 'data', 'explain', 'how', 'problem', 'tackled', 'finally', 'point', 'out', 'emerging', 'applications', 'field', 'human', 'facial', 'body', 'feature', 'modeling', 'understanding', 'key', 'research', 'problem', 'multilayer', 'lattice', 'model', 'realtime', 'dynamic', 'character', 'deformation', 'recent', 'advancement', 'computer', 'graphics', 'hardware', 'software', 'algorithms', 'deformable', 'characters', 'become', 'popular', 'realtime', 'applications', 'computer', 'games', 'mature', 'techniques', 'generate', 'primary', 'deformation', 'skeletal', 'movement', 'simulating', 'realistic', 'stable', 'secondary', 'deformation', 'jiggling', 'fats', 'remains', 'challenging', 'hand', 'traditional', 'volumetric', 'finite', 'element', 'method', 'require', 'higher', 'computational', 'cost', 'infeasible', 'limited', 'hardware', 'game', 'consoles', 'hand', 'shape', 'matching', 'based', 'simulations', 'plausible', 'deformation', 'realtime', 'suffer', 'stiffness', 'problem', 'particles', 'unrealistic', 'deformation', 'high', 'gains', 'cannot', 'catch', 'up', 'body', 'movement', 'paper', 'unified', 'multilayer', 'lattice', 'model', 'simulate', 'primary', 'secondary', 'deformation', 'skeletondriven', 'characters', 'core', 'idea', 'voxelize', 'input', 'character', 'mesh', 'multiple', 'anatomical', 'layers', 'including', 'bone', 'muscle', 'fat', 'skin', 'primary', 'deformation', 'applied', 'bone', 'voxels', 'latticebased', 'skinning', 'movement', 'voxels', 'propagated', 'voxel', 'layers', 'lattice', 'shape', 'matching', 'simulation', 'creating', 'natural', 'secondary', 'deformation', 'multilayer', 'lattice', 'framework', 'simulation', 'quality', 'comparable', 'those', 'volumetric', 'significantly', 'smaller', 'computational', 'cost', 'best', 'applied', 'realtime', 'applications', 'console', 'games', 'interactive', 'animation', 'creation', 'human', 'motion', 'variation', 'synthesis', 'multivariate', 'gaussian', 'processes', 'human', 'motion', 'variation', 'synthesis', 'important', 'crowd', 'simulation', 'interactive', 'applications', 'enhance', 'synthesis', 'quality', 'paper', 'novel', 'generative', 'probabilistic', 'model', 'synthesize', 'variations', 'human', 'motion', 'key', 'idea', 'model', 'conditional', 'distribution', 'joint', 'multivariate', 'gaussian', 'process', 'model', 'namely', 'semiparametric', 'latent', 'factor', 'model', 'slfm', 'slfm', 'effectively', 'model', 'correlations', 'degrees', 'freedom', 'dofs', 'joints', 'dealing', 'dof', 'separately', 'implemented', 'existing', 'methods', 'detailed', 'evaluation', 'performed', 'approach', 'effectively', 'synthesize', 'variations', 'different', 'types', 'motions', 'motions', 'generated', 'method', 'richer', 'variation', 'compared', 'existing', 'ones', 'finally', 'user', 'study', 'shows', 'synthesized', 'motion', 'similar', 'level', 'naturalness', 'captured', 'human', 'motions', 'method', 'best', 'applied', 'computer', 'games', 'animations', 'introduce', 'motion', 'variations', 'topology', 'aware', 'datadriven', 'inverse', 'kinematics', 'creating', 'realistic', 'human', 'movement', 'time', 'consuming', 'labour', 'intensive', 'task', 'major', 'difficulty', 'user', 'edit', 'individual', 'joints', 'maintaining', 'overall', 'realistic', 'collision', 'free', 'posture', 'previous', 'research', 'suggests', 'datadriven', 'inverse', 'kinematics', 'focus', 'control', 'few', 'joints', 'system', 'automatically', 'composes', 'natural', 'posture', 'common', 'problem', 'kinematics', 'synthesis', 'penetration', 'body', 'parts', 'difficult', 'avoid', 'complex', 'movements', 'paper', 'datadriven', 'inverse', 'kinematics', 'framework', 'conserves', 'topology', 'synthesizing', 'postures', 'system', 'monitors', 'regulates', 'topology', 'changes', 'gauss', 'linking', 'integral', 'gli', 'penetration', 'efficiently', 'prevented', 'result', 'complex', 'motions', 'tight', 'body', 'movements', 'those', 'involving', 'interaction', 'external', 'objects', 'simulated', 'minimal', 'manual', 'intervention', 'experimental', 'results', 'system', 'user', 'create', 'high', 'quality', 'human', 'motion', 'realtime', 'controlling', 'few', 'joints', 'mouse', 'multitouch', 'screen', 'movement', 'generated', 'realistic', 'penetration', 'free', 'system', 'best', 'applied', 'interactive', 'motion', 'design', 'computer', 'animations', 'games', 'natural', 'preparation', 'behavior', 'synthesis', 'humans', 'adjust', 'movements', 'advance', 'prepare', 'forthcoming', 'action', 'efficient', 'smooth', 'transition', 'traditional', 'computer', 'animation', 'motion', 'graphs', 'simply', 'concatenates', 'series', 'actions', 'without', 'taking', 'account', 'following', 'paper', 'method', 'preparation', 'behaviours', 'reinforcement', 'learning', 'offline', 'process', 'system', 'learns', 'optimal', 'way', 'approach', 'target', 'prepare', 'interaction', 'scalar', 'value', 'level', 'preparation', 'introduced', 'represents', 'degree', 'transition', 'initial', 'action', 'interacting', 'action', 'synthesize', 'movements', 'preparation', 'customized', 'motion', 'blending', 'scheme', 'based', 'level', 'preparation', 'followed', 'optimization', 'framework', 'adjusts', 'posture', 'keep', 'balance', 'during', 'runtime', 'trained', 'controller', 'drives', 'character', 'move', 'target', 'appropriate', 'level', 'preparation', 'humanlike', 'behaviour', 'create', 'scenes', 'character', 'move', 'complex', 'environment', 'interacts', 'objects', 'crawling', 'under', 'jumping', 'obstacles', 'walking', 'method', 'computer', 'animation', 'realtime', 'applications', 'computer', 'games', 'characters', 'accomplish', 'series', 'tasks', 'environment', 'preparation', 'behaviour', 'synthesis', 'reinforcement', 'learning', 'humans', 'perform', 'series', 'motions', 'prepare', 'next', 'motion', 'advance', 'enhance', 'response', 'time', 'movements', 'kind', 'preparation', 'behaviour', 'results', 'natural', 'smooth', 'transition', 'overall', 'movement', 'paper', 'method', 'synthesize', 'behaviour', 'reinforcement', 'learning', 'create', 'preparation', 'movements', 'customized', 'motion', 'blending', 'algorithm', 'governed', 'single', 'numerical', 'value', 'level', 'preparation', 'during', 'offline', 'process', 'system', 'learns', 'optimal', 'way', 'approach', 'target', 'realistic', 'behaviour', 'prepare', 'interaction', 'considering', 'level', 'preparation', 'runtime', 'trained', 'controller', 'indicates', 'character', 'move', 'target', 'appropriate', 'level', 'preparation', 'humanlike', 'movements', 'synthesized', 'scenes', 'character', 'move', 'complex', 'environment', 'interact', 'objects', 'character', 'crawling', 'under', 'jumping', 'obstacles', 'walking', 'method', 'computer', 'animation', 'realtime', 'applications', 'computer', 'games', 'characters', 'accomplish', 'series', 'tasks', 'environment', 'simulating', 'multiple', 'character', 'interactions', 'collaborative', 'adversarial', 'goals', 'paper', 'proposes', 'methodology', 'synthesizing', 'animations', 'multiple', 'characters', 'allowing', 'them', 'intelligently', 'compete', 'another', 'dense', 'environments', 'still', 'satisfying', 'requirements', 'set', 'animator', 'achieve', 'conflicting', 'objectives', 'simultaneously', 'method', 'separately', 'evaluates', 'competition', 'collaboration', 'interactions', 'integrating', 'scores', 'select', 'action', 'maximizes', 'criteria', 'extend', 'idea', 'minmax', 'search', 'normally', 'strategic', 'games', 'chess', 'method', 'animators', 'efficiently', 'scenes', 'dense', 'character', 'interactions', 'those', 'collective', 'sports', 'martial', 'arts', 'method', 'especially', 'effective', 'producing', 'animations', 'along', 'story', 'lines', 'characters', 'must', 'follow', 'multiple', 'objectives', 'still', 'accommodating', 'geometric', 'kinematic', 'constraints', 'environment', 'realtime', 'physical', 'modelling', 'character', 'movements', 'microsoft', 'kinect', 'advancement', 'motion', 'tracking', 'hardware', 'microsoft', 'kinect', 'synthesizing', 'humanlike', 'characters', 'realtime', 'captured', 'movements', 'becomes', 'increasingly', 'important', 'traditional', 'kinematics', 'dynamics', 'perform', 'suboptimally', 'captured', 'motion', 'noisy', 'even', 'incomplete', 'paper', 'unified', 'framework', 'control', 'physically', 'simulated', 'characters', 'live', 'captured', 'motion', 'kinect', 'framework', 'synthesize', 'any', 'posture', 'physical', 'environment', 'external', 'forces', 'torques', 'computed', 'pd', 'controller', 'major', 'problem', 'kinect', 'incompleteness', 'captured', 'posture', 'some', 'degree', 'freedom', 'dof', 'missing', 'occlusions', 'noises', 'search', 'best', 'matched', 'posture', 'motion', 'database', 'constructed', 'dimensionality', 'reduced', 'space', 'substitute', 'missing', 'dof', 'live', 'captured', 'data', 'experimental', 'results', 'method', 'synthesize', 'realistic', 'character', 'movements', 'noisy', 'captured', 'motion', 'algorithm', 'computationally', 'efficient', 'applied', 'wide', 'variety', 'interactive', 'virtual', 'reality', 'applications', 'motionbased', 'gaming', 'rehabilitation', 'sport', 'training', 'manufacturing', 'video', 'graphics', 'apparatus', 'comprising', 'memory', 'system', 'storing', 'plurality', 'sequences', 'sequence', 'comprising', 'data', 'reproducing', 'different', 'pattern', 'interactions', 'respective', 'plurality', 'moving', 'characters', 'storing', 'combination', 'data', 'structure', 'defining', 'sequence', 'connectability', 'sequence', 'ones', 'plurality', 'sequences', 'processor', 'configured', 'determine', 'pairwise', 'combination', 'stored', 'sequences', 'selecting', 'sequences', 'pairwise', 'combination', 'defined', 'connectable', 'stored', 'combination', 'data', 'structure', 'wherein', 'pairwise', 'combination', 'common', 'least', 'respective', 'plurality', 'moving', 'characters', 'configured', 'determined', 'pairwise', 'combinations', 'stored', 'sequences', 'output', 'video', 'graphics', 'comprising', 'series', 'sequences', 'movable', 'characters', 'repetitively', 'interact', 'different', 'combinations', 'simulating', 'interactions', 'among', 'multiple', 'characters', 'thesis', 'attack', 'challenging', 'problem', 'field', 'character', 'animation', 'synthesizing', 'interactions', 'among', 'multiple', 'virtual', 'characters', 'realtime', 'although', 'heavy', 'demands', 'gaming', 'animation', 'industries', 'no', 'systemic', 'solution', 'difficulties', 'model', 'complex', 'behaviors', 'characterswe', 'represent', 'continuous', 'interactions', 'among', 'characters', 'discrete', 'markov', 'decision', 'process', 'design', 'general', 'objective', 'function', 'evaluate', 'immediate', 'rewards', 'launching', 'action', 'applying', 'game', 'theory', 'tree', 'expansion', 'minmax', 'search', 'optimal', 'actions', 'benefit', 'character', 'most', 'future', 'selected', 'simulated', 'characters', 'interact', 'competitively', 'achieving', 'requests', 'animators', 'cooperativelysince', 'interactions', 'characters', 'depend', 'lot', 'criteria', 'difficult', 'exhaustively', 'precompute', 'optimal', 'actions', 'all', 'variations', 'criteria', 'design', 'offpolicy', 'approach', 'samples', 'precomputes', 'meaningful', 'interactions', 'precomputed', 'policy', 'optimal', 'movements', 'under', 'different', 'situations', 'evaluated', 'realtimeto', 'simulate', 'interactions', 'large', 'number', 'characters', 'minimal', 'computational', 'overhead', 'method', 'precompute', 'short', 'durations', 'interactions', 'characters', 'connectable', 'patches', 'patches', 'concatenated', 'spatially', 'generate', 'interactions', 'multiple', 'characters', 'temporally', 'generate', 'longer', 'interactions', 'based', 'optional', 'instructions', 'animators', 'system', 'automatically', 'applies', 'concatenations', 'create', 'huge', 'scene', 'interacting', 'crowdwe', 'system', 'creating', 'scenes', 'high', 'quality', 'interactions', 'hand', 'algorithm', 'automatically', 'generate', 'artistic', 'scenes', 'interactions', 'fighting', 'scenes', 'movies', 'involve', 'hundreds', 'characters', 'hand', 'create', 'controllable', 'intelligent', 'characters', 'interact', 'opponents', 'realtime', 'applications', '3d', 'computer', 'games', 'angular', 'momentum', 'guided', 'motion', 'concatenation', 'paper', 'method', 'concatenate', 'dynamic', 'fullbody', 'motions', 'punches', 'kicks', 'flips', 'angular', 'momentum', 'cue', 'through', 'observation', 'real', 'humans', 'identified', 'patterns', 'angular', 'momentum', 'make', 'transition', 'motions', 'efficient', 'based', 'observations', 'method', 'concatenate', 'fullbody', 'motions', 'natural', 'manner', 'method', 'applications', 'dynamic', 'fullbody', 'motions', 'required', '3d', 'computer', 'games', 'animations', 'interaction', 'patches', 'multicharacter', 'animation', 'datadriven', 'approach', 'automatically', 'generate', 'scene', 'tens', 'hundreds', 'characters', 'densely', 'interact', 'during', 'offline', 'processing', 'close', 'interactions', 'characters', 'precomputed', 'expanding', 'game', 'tree', 'stored', 'data', 'structures', 'interaction', 'patches', 'then', 'during', 'runtime', 'system', 'spatiotemporally', 'concatenates', 'interaction', 'patches', 'create', 'scenes', 'large', 'number', 'characters', 'closely', 'interact', 'another', 'method', 'possible', 'automatically', 'interactively', 'animations', 'crowds', 'interacting', 'stylized', 'way', 'method', 'variety', 'applications', 'including', 'tv', 'programs', 'advertisements', 'movies', 'simulating', 'interactions', 'avatars', 'high', 'dimensional', 'state', 'space', 'efficient', 'computation', 'strategic', 'movements', 'essential', 'control', 'virtual', 'avatars', 'intelligently', 'computer', 'games', '3d', 'virtual', 'environments', 'module', 'needed', 'control', 'nonplayer', 'characters', 'npcs', 'fight', 'play', 'team', 'sports', 'move', 'through', 'mass', 'crowd', 'reinforcement', 'learning', 'approach', 'achieve', 'realtime', 'optimal', 'control', 'huge', 'state', 'space', 'human', 'interactions', 'makes', 'difficult', 'apply', 'existing', 'learning', 'methods', 'control', 'avatars', 'dense', 'interactions', 'characters', 'research', 'methodology', 'efciently', 'plan', 'movements', 'avatar', 'interacting', 'another', 'make', 'fact', 'subspace', 'meaningful', 'interactions', 'smaller', 'whole', 'state', 'space', 'avatars', 'efficiently', 'collect', 'samples', 'exploring', 'subspace', 'dense', 'interactions', 'avatars', 'occur', 'favor', 'samples', 'high', 'connectivity', 'samples', 'collected', 'samples', 'nite', 'state', 'machine', 'fsm', 'interaction', 'graph', 'composed', 'runtime', 'compute', 'optimal', 'action', 'avatar', 'minmax', 'search', 'dynamic', 'programming', 'interaction', 'graph', 'methodology', 'applicable', 'control', 'npcs', 'ghting', 'ballsports', 'games', 'simulating', 'interactions', 'characters', 'difficult', 'create', 'scenes', 'multiple', 'characters', 'densely', 'interact', 'manually', 'creating', 'motions', 'characters', 'time', 'consuming', 'correlation', 'movements', 'characters', 'capturing', 'motions', 'multiple', 'characters', 'difficult', 'requires', 'huge', 'amount', 'postprocessing', 'data', 'paper', 'explain', 'methods', 'simulate', 'close', 'interactions', 'characters', 'based', 'singly', 'captured', 'motions', 'methods', '1', 'control', 'characters', 'intelligently', 'cooperativelycompetitively', 'interact', 'characters', '2', 'generate', 'movements', 'include', 'close', 'interactions', 'tangling', 'segments', 'others', 'taking', 'account', 'topological', 'relationship', 'characters', 'simulating', 'competitive', 'interactions', 'singly', 'captured', 'motions', 'difficult', 'create', 'scenes', 'multiple', 'avatars', 'fighting', 'competing', 'manually', 'creating', 'motions', 'avatars', 'time', 'consuming', 'correlation', 'movements', 'avatars', 'capturing', 'motions', 'multiple', 'avatars', 'difficult', 'requires', 'huge', 'amount', 'postprocessing', 'paper', 'method', 'generate', 'realistic', 'scene', 'avatars', 'densely', 'interacting', 'competitive', 'environment', 'motions', 'avatars', 'considered', 'captured', 'individually', 'increase', 'easiness', 'obtaining', 'data', 'algorithm', 'temporal', 'expansion', 'approach', 'maps', 'continuous', 'time', 'action', 'plan', 'discrete', 'space', 'turnbased', 'evaluation', 'methods', 'result', 'mature', 'algorithms', 'game', 'minmax', 'search', 'alphabeta', 'pruning', 'appliedusing', 'method', 'avatars', 'plan', 'strategies', 'taking', 'account', 'reaction', 'opponent', 'fighting', 'scenes', 'multiple', 'avatars', 'generated', 'effectiveness', 'algorithm', 'method', 'applied', 'kinds', 'continuous', 'activities', 'require', 'strategy', 'planning', 'sport', 'games', 'generating', 'realistic', 'fighting', 'scenes', 'game', 'tree', 'recently', 'lot', 'researches', 'synthesizeedit', 'motion', 'single', 'avatar', 'virtual', 'environment', 'work', 'simulating', 'continuous', 'interactions', 'multiple', 'avatars', 'fighting', 'paper', 'method', 'generate', 'realistic', 'fighting', 'scene', 'based', 'motion', 'capture', 'data', 'algorithm', 'temporal', 'expansion', 'approach', 'maps', 'continuous', 'time', 'action', 'plan', 'discrete', 'causality', 'space', 'turnbased', 'evaluation', 'methods', 'result', 'possible', 'mature', 'algorithms', 'available', 'strategy', 'games', 'minimax', 'algorithm', 'αβ', 'pruning', 'method', 'generate', 'offensedefense', 'table', 'illustrates', 'spatialtemporal', 'relationship', 'attacks', 'dodges', 'incorporate', 'tactical', 'maneuvers', 'defense', 'scene', 'method', 'avatars', 'plan', 'strategies', 'taking', 'account', 'reaction', 'opponent', 'fighting', 'scenes', 'multiple', 'avatars', 'generated', 'effectiveness', 'algorithm', 'method', 'applied', 'kinds', 'continuous', 'activities', 'require', 'strategy', 'planning', 'sport', 'games', 'technical', 'note', 'generating', 'realistic', 'fighting', 'scenes', 'game', 'tree', 'recently', 'lot', 'researches', 'synthesizeedit', 'motion', 'single', 'avatar', 'virtual', 'environment', 'work', 'simulating', 'continuous', 'interactions', 'multiple', 'avatars', 'fighting', 'paper', 'method', 'generate', 'realistic', 'fighting', 'scene', 'based', 'motion', 'capture', 'data', 'algorithm', 'temporal', 'expansion', 'approach', 'maps', 'continuous', 'time', 'action', 'plan', 'discrete', 'causality', 'space', 'turnbased', 'evaluation', 'methods', 'result', 'possible', 'mature', 'algorithms', 'available', 'strategy', 'games', 'minimax', 'algorithm', 'αβ', 'pruning', 'method', 'generate', 'offensedefense', 'table', 'illustrates', 'spatialtemporal', 'relationship', 'attacks', 'dodges', 'incorporate', 'tactical', 'maneuvers', 'defense', 'scene', 'method', 'avatars', 'plan', 'strategies', 'taking', 'account', 'reaction', 'opponent', 'fighting', 'scenes', 'multiple', 'avatars', 'generated', 'effectiveness', 'algorithm', 'method', 'applied', 'kinds', 'continuous', 'activities', 'require', 'strategy', 'planning', 'sport', 'games']\n"
     ]
    }
   ],
   "source": [
    "data = result.split()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter = Counter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_occur = Counter.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('motion', 70), ('characters', 39), ('motions', 38), ('method', 34), ('interactions', 30), ('dance', 27), ('system', 26), ('control', 24), ('character', 22), ('human', 21), ('computer', 20), ('applications', 20), ('games', 20), ('animation', 19), ('avatars', 19), ('multiple', 18), ('movements', 18), ('model', 16), ('realistic', 15), ('paper', 15), ('data', 14), ('synthesis', 14), ('generate', 14), ('realtime', 14), ('scenes', 14), ('emotion', 13), ('synthesizing', 12), ('applied', 12), ('temporal', 12), ('different', 12), ('synthesize', 12), ('preparation', 12), ('movement', 11), ('body', 11), ('time', 11), ('interaction', 11), ('continuous', 11), ('approach', 11), ('sign', 11), ('music', 11), ('algorithm', 11), ('fighting', 11), ('difficult', 10), ('create', 10), ('work', 10), ('effectively', 10), ('action', 10), ('virtual', 10), ('features', 10), ('based', 10), ('captured', 10), ('learning', 9), ('methods', 9), ('existing', 9), ('problem', 9), ('interact', 9), ('quality', 9), ('strength', 9), ('environment', 9), ('datadriven', 8), ('research', 8), ('natural', 8), ('creating', 8), ('framework', 8), ('deformation', 8), ('simulating', 8), ('3d', 7), ('generated', 7), ('facial', 7), ('runtime', 7), ('animations', 7), ('posture', 7), ('level', 7), ('optimal', 7), ('space', 7), ('scene', 7), ('plan', 7), ('modeling', 6), ('information', 6), ('frames', 6), ('hand', 6), ('number', 6), ('network', 6), ('complex', 6), ('reality', 6), ('high', 6), ('discrete', 6), ('result', 6), ('interactive', 6), ('automatically', 6), ('controller', 6), ('game', 6), ('sequences', 6), ('graphics', 5), ('actions', 5), ('spatial', 5), ('prediction', 5), ('results', 5), ('popular', 5), ('joints', 5)]\n"
     ]
    }
   ],
   "source": [
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "#Question 5\n",
    "#Please design and implement the solution to use data analysis and visualization for analysing\n",
    "#how the features of a publication would affect its “citation” (a value that can be found in the\n",
    "#publication detail pages).\n",
    "######################\n",
    "#Begin creating the data file by importing csv\n",
    "import csv\n",
    "#Lists to be used in the data file\n",
    "Title = Titles\n",
    "Author = Authors\n",
    "Date = Dates\n",
    "Journal = Journals\n",
    "Publisher = Publishers\n",
    "No_Pages = NumberOfPages\n",
    "Citation = Citations\n",
    "ImpactFactor = ImpactFactors\n",
    "\n",
    "\n",
    "file = open(\"citationData.csv\", \"w\", newline =\"\")\n",
    "writer = csv.writer(file)\n",
    "\n",
    "for w in range(25):\n",
    "\n",
    "    writer.writerow([Citation[w], Title[w], Author[w], Date[w], Journal[w], Publisher[w], No_Pages[w], \n",
    "                      ImpactFactor[w]])\n",
    "\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = 'citationData.csv'\n",
    "column_names = [ 'Citation','Title', 'Author/s', 'Date', 'Journal', 'Publisher', 'NumberOfPages',\n",
    "                'Impact Factor',]\n",
    "\n",
    "raw_data = pd.read_csv(data, names=column_names,\n",
    "                          na_values='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Citation</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author/s</th>\n",
       "      <th>Date</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>NumberOfPages</th>\n",
       "      <th>Impact Factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>Spatio-temporal Manifold Learning for Human Mo...</td>\n",
       "      <td>He Wang  Edmond S L Ho  Hubert P H Shum  Zhanx...</td>\n",
       "      <td>2021</td>\n",
       "      <td>IEEE Transactions on Visualization and Compute...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>A Quadruple Diffusion Convolutional Recurrent ...</td>\n",
       "      <td>Qianhui Men  Edmond S L Ho  Hubert P H Shum  H...</td>\n",
       "      <td>2021</td>\n",
       "      <td>IEEE Transactions on Circuits and Systems for ...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>GAN-based Reactive Motion Synthesis with Class...</td>\n",
       "      <td>Qianhui Men  Hubert P H Shum  Edmond S L Ho  H...</td>\n",
       "      <td>2021</td>\n",
       "      <td>Computers and Graphics</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>A Generic Framework for Editing and Synthesizi...</td>\n",
       "      <td>Jacky C P Chan  Hubert P H Shum  He Wang  Li Y...</td>\n",
       "      <td>2019</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Automatic Sign Dance Synthesis from Gesture-ba...</td>\n",
       "      <td>Naoya Iwamoto  Hubert P H Shum  Wakana Asahina...</td>\n",
       "      <td>2019</td>\n",
       "      <td>MIG</td>\n",
       "      <td>ACM</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>DanceDJ: A 3D Dance Animation Authoring System...</td>\n",
       "      <td>Naoya Iwamoto  Takuya Kato  Hubert P H Shum  R...</td>\n",
       "      <td>2017</td>\n",
       "      <td>ACE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>Synthesizing Motion with Relative Emotion Stre...</td>\n",
       "      <td>Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi</td>\n",
       "      <td>2017</td>\n",
       "      <td>D2AT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>Automatic Dance Generation System Considering ...</td>\n",
       "      <td>Wakana Asahina  Naoya Iwamoto  Hubert P H Shum...</td>\n",
       "      <td>2016</td>\n",
       "      <td>SIGGRAPH</td>\n",
       "      <td>ACM</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Depth Sensor based Facial and Body Animation C...</td>\n",
       "      <td>Yijun Shen  Jingtian Zhang  Longzhi Yang  Hube...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Motion</td>\n",
       "      <td>Springer International Publishing</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>Multi-layer Lattice Model for Real-Time Dynami...</td>\n",
       "      <td>Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  ...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Computer Graphics Forum</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>Human Motion Variation Synthesis with Multivar...</td>\n",
       "      <td>Liuyang Zhou  Lifeng Shang  Hubert P H Shum  H...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26</td>\n",
       "      <td>Topology Aware Data-Driven Inverse Kinematics</td>\n",
       "      <td>Edmond S L Ho  Hubert P H Shum  Yiuming Cheung...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Computer Graphics Forum</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Natural Preparation Behavior Synthesis</td>\n",
       "      <td>Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho ...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>Preparation Behaviour Synthesis with Reinforce...</td>\n",
       "      <td>Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho ...</td>\n",
       "      <td>2013</td>\n",
       "      <td>CASA</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>68</td>\n",
       "      <td>Simulating Multiple Character Interactions wit...</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Shuntaro Yamazaki</td>\n",
       "      <td>2012</td>\n",
       "      <td>IEEE Transactions on Visualization and Compute...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>68</td>\n",
       "      <td>Real-time Physical Modelling of Character Move...</td>\n",
       "      <td>Hubert P H Shum  Edmond S L Ho</td>\n",
       "      <td>2012</td>\n",
       "      <td>VRST</td>\n",
       "      <td>ACM</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>Manufacturing Video Graphics</td>\n",
       "      <td>Taku Komura  Hubert P H Shum</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WO Patent WO/2010/057897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>Simulating Interactions Among Multiple Characters</td>\n",
       "      <td>Hubert P H Shum</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>University of Edinburgh</td>\n",
       "      <td>151.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11</td>\n",
       "      <td>Angular Momentum Guided Motion Concatenation</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Pranjul Yadav</td>\n",
       "      <td>2009</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>125</td>\n",
       "      <td>Interaction Patches for Multi-Character Animation</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Masashi Shiraish...</td>\n",
       "      <td>2008</td>\n",
       "      <td>ACM Transactions on Graphics</td>\n",
       "      <td>ACM</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>45</td>\n",
       "      <td>Simulating Interactions of Avatars in High Dim...</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Shuntaro Yamazaki</td>\n",
       "      <td>2008</td>\n",
       "      <td>I3D</td>\n",
       "      <td>ACM</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>Simulating Interactions of Characters</td>\n",
       "      <td>Taku Komura  Hubert P H Shum  Edmond S L Ho</td>\n",
       "      <td>2008</td>\n",
       "      <td>MIG</td>\n",
       "      <td>Springer-Verlag</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>45</td>\n",
       "      <td>Simulating Competitive Interactions using Sing...</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Shuntaro Yamazaki</td>\n",
       "      <td>2007</td>\n",
       "      <td>VRST</td>\n",
       "      <td>ACM</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>Generating Realistic Fighting Scenes by Game Tree</td>\n",
       "      <td>Hubert P H Shum  Taku Komura</td>\n",
       "      <td>2006</td>\n",
       "      <td>SCA</td>\n",
       "      <td>Eurographics Association</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>Technical Note: Generating Realistic Fighting ...</td>\n",
       "      <td>Hubert P H Shum  Taku Komura</td>\n",
       "      <td>2006</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Citation                                              Title  \\\n",
       "0         74  Spatio-temporal Manifold Learning for Human Mo...   \n",
       "1         13  A Quadruple Diffusion Convolutional Recurrent ...   \n",
       "2          0  GAN-based Reactive Motion Synthesis with Class...   \n",
       "3          6  A Generic Framework for Editing and Synthesizi...   \n",
       "4          0  Automatic Sign Dance Synthesis from Gesture-ba...   \n",
       "5          4  DanceDJ: A 3D Dance Animation Authoring System...   \n",
       "6          3  Synthesizing Motion with Relative Emotion Stre...   \n",
       "7          6  Automatic Dance Generation System Considering ...   \n",
       "8          1  Depth Sensor based Facial and Body Animation C...   \n",
       "9         14  Multi-layer Lattice Model for Real-Time Dynami...   \n",
       "10         6  Human Motion Variation Synthesis with Multivar...   \n",
       "11        26      Topology Aware Data-Driven Inverse Kinematics   \n",
       "12         1             Natural Preparation Behavior Synthesis   \n",
       "13         2  Preparation Behaviour Synthesis with Reinforce...   \n",
       "14        68  Simulating Multiple Character Interactions wit...   \n",
       "15        68  Real-time Physical Modelling of Character Move...   \n",
       "16         0                       Manufacturing Video Graphics   \n",
       "17         1  Simulating Interactions Among Multiple Characters   \n",
       "18        11       Angular Momentum Guided Motion Concatenation   \n",
       "19       125  Interaction Patches for Multi-Character Animation   \n",
       "20        45  Simulating Interactions of Avatars in High Dim...   \n",
       "21         0              Simulating Interactions of Characters   \n",
       "22        45  Simulating Competitive Interactions using Sing...   \n",
       "23         6  Generating Realistic Fighting Scenes by Game Tree   \n",
       "24         0  Technical Note: Generating Realistic Fighting ...   \n",
       "\n",
       "                                             Author/s  Date  \\\n",
       "0   He Wang  Edmond S L Ho  Hubert P H Shum  Zhanx...  2021   \n",
       "1   Qianhui Men  Edmond S L Ho  Hubert P H Shum  H...  2021   \n",
       "2   Qianhui Men  Hubert P H Shum  Edmond S L Ho  H...  2021   \n",
       "3   Jacky C P Chan  Hubert P H Shum  He Wang  Li Y...  2019   \n",
       "4   Naoya Iwamoto  Hubert P H Shum  Wakana Asahina...  2019   \n",
       "5   Naoya Iwamoto  Takuya Kato  Hubert P H Shum  R...  2017   \n",
       "6      Edmond S L Ho  Hubert P H Shum  He Wang  Li Yi  2017   \n",
       "7   Wakana Asahina  Naoya Iwamoto  Hubert P H Shum...  2016   \n",
       "8   Yijun Shen  Jingtian Zhang  Longzhi Yang  Hube...  2016   \n",
       "9   Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  ...  2015   \n",
       "10  Liuyang Zhou  Lifeng Shang  Hubert P H Shum  H...  2014   \n",
       "11  Edmond S L Ho  Hubert P H Shum  Yiuming Cheung...  2013   \n",
       "12  Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho ...  2013   \n",
       "13  Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho ...  2013   \n",
       "14    Hubert P H Shum  Taku Komura  Shuntaro Yamazaki  2012   \n",
       "15                     Hubert P H Shum  Edmond S L Ho  2012   \n",
       "16                       Taku Komura  Hubert P H Shum  2010   \n",
       "17                                    Hubert P H Shum  2010   \n",
       "18        Hubert P H Shum  Taku Komura  Pranjul Yadav  2009   \n",
       "19  Hubert P H Shum  Taku Komura  Masashi Shiraish...  2008   \n",
       "20    Hubert P H Shum  Taku Komura  Shuntaro Yamazaki  2008   \n",
       "21        Taku Komura  Hubert P H Shum  Edmond S L Ho  2008   \n",
       "22    Hubert P H Shum  Taku Komura  Shuntaro Yamazaki  2007   \n",
       "23                       Hubert P H Shum  Taku Komura  2006   \n",
       "24                       Hubert P H Shum  Taku Komura  2006   \n",
       "\n",
       "                                              Journal  \\\n",
       "0   IEEE Transactions on Visualization and Compute...   \n",
       "1   IEEE Transactions on Circuits and Systems for ...   \n",
       "2                              Computers and Graphics   \n",
       "3               Computer Animation and Virtual Worlds   \n",
       "4                                               MIG     \n",
       "5                                               ACE     \n",
       "6                                              D2AT     \n",
       "7                                          SIGGRAPH     \n",
       "8                                            Motion     \n",
       "9                             Computer Graphics Forum   \n",
       "10              Computer Animation and Virtual Worlds   \n",
       "11                            Computer Graphics Forum   \n",
       "12              Computer Animation and Virtual Worlds   \n",
       "13                                             CASA     \n",
       "14  IEEE Transactions on Visualization and Compute...   \n",
       "15                                             VRST     \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "18              Computer Animation and Virtual Worlds   \n",
       "19                       ACM Transactions on Graphics   \n",
       "20                                              I3D     \n",
       "21                                              MIG     \n",
       "22                                             VRST     \n",
       "23                                              SCA     \n",
       "24                                             arXiv    \n",
       "\n",
       "                            Publisher  NumberOfPages  Impact Factor  \n",
       "0                                IEEE           12.0          4.579  \n",
       "1                                IEEE           16.0          4.685  \n",
       "2                            Elsevier           12.0          1.936  \n",
       "3            John Wiley and Sons Ltd.           20.0          1.020  \n",
       "4                                 ACM            9.0            NaN  \n",
       "5                                 NaN           18.0            NaN  \n",
       "6                                 NaN            8.0            NaN  \n",
       "7                                 ACM            2.0            NaN  \n",
       "8   Springer International Publishing           16.0            NaN  \n",
       "9            John Wiley and Sons Ltd.           11.0          2.078  \n",
       "10           John Wiley and Sons Ltd.            9.0          1.020  \n",
       "11           John Wiley and Sons Ltd.           10.0          2.078  \n",
       "12           John Wiley and Sons Ltd.           10.0          1.020  \n",
       "13           John Wiley and Sons Ltd.           10.0            NaN  \n",
       "14                               IEEE           12.0          4.579  \n",
       "15                                ACM            8.0            NaN  \n",
       "16           WO Patent WO/2010/057897            NaN            NaN  \n",
       "17            University of Edinburgh          151.0            NaN  \n",
       "18           John Wiley and Sons Ltd.           10.0          1.020  \n",
       "19                                ACM            8.0          5.414  \n",
       "20                                ACM            8.0            NaN  \n",
       "21                    Springer-Verlag           10.0            NaN  \n",
       "22                                ACM            8.0            NaN  \n",
       "23           Eurographics Association            2.0            NaN  \n",
       "24                                NaN            7.0            NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citationData = raw_data.copy()\n",
    "citationData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Citation          0\n",
       "Title             0\n",
       "Author/s          0\n",
       "Date              0\n",
       "Journal           2\n",
       "Publisher         3\n",
       "NumberOfPages     1\n",
       "Impact Factor    14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citationData.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "citationData = citationData.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Citation</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author/s</th>\n",
       "      <th>Date</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>NumberOfPages</th>\n",
       "      <th>Impact Factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>Spatio-temporal Manifold Learning for Human Mo...</td>\n",
       "      <td>He Wang  Edmond S L Ho  Hubert P H Shum  Zhanx...</td>\n",
       "      <td>2021</td>\n",
       "      <td>IEEE Transactions on Visualization and Compute...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>A Quadruple Diffusion Convolutional Recurrent ...</td>\n",
       "      <td>Qianhui Men  Edmond S L Ho  Hubert P H Shum  H...</td>\n",
       "      <td>2021</td>\n",
       "      <td>IEEE Transactions on Circuits and Systems for ...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>GAN-based Reactive Motion Synthesis with Class...</td>\n",
       "      <td>Qianhui Men  Hubert P H Shum  Edmond S L Ho  H...</td>\n",
       "      <td>2021</td>\n",
       "      <td>Computers and Graphics</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>A Generic Framework for Editing and Synthesizi...</td>\n",
       "      <td>Jacky C P Chan  Hubert P H Shum  He Wang  Li Y...</td>\n",
       "      <td>2019</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>Multi-layer Lattice Model for Real-Time Dynami...</td>\n",
       "      <td>Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  ...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Computer Graphics Forum</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>Human Motion Variation Synthesis with Multivar...</td>\n",
       "      <td>Liuyang Zhou  Lifeng Shang  Hubert P H Shum  H...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26</td>\n",
       "      <td>Topology Aware Data-Driven Inverse Kinematics</td>\n",
       "      <td>Edmond S L Ho  Hubert P H Shum  Yiuming Cheung...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Computer Graphics Forum</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Natural Preparation Behavior Synthesis</td>\n",
       "      <td>Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho ...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>68</td>\n",
       "      <td>Simulating Multiple Character Interactions wit...</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Shuntaro Yamazaki</td>\n",
       "      <td>2012</td>\n",
       "      <td>IEEE Transactions on Visualization and Compute...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11</td>\n",
       "      <td>Angular Momentum Guided Motion Concatenation</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Pranjul Yadav</td>\n",
       "      <td>2009</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>125</td>\n",
       "      <td>Interaction Patches for Multi-Character Animation</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Masashi Shiraish...</td>\n",
       "      <td>2008</td>\n",
       "      <td>ACM Transactions on Graphics</td>\n",
       "      <td>ACM</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Citation                                              Title  \\\n",
       "0         74  Spatio-temporal Manifold Learning for Human Mo...   \n",
       "1         13  A Quadruple Diffusion Convolutional Recurrent ...   \n",
       "2          0  GAN-based Reactive Motion Synthesis with Class...   \n",
       "3          6  A Generic Framework for Editing and Synthesizi...   \n",
       "9         14  Multi-layer Lattice Model for Real-Time Dynami...   \n",
       "10         6  Human Motion Variation Synthesis with Multivar...   \n",
       "11        26      Topology Aware Data-Driven Inverse Kinematics   \n",
       "12         1             Natural Preparation Behavior Synthesis   \n",
       "14        68  Simulating Multiple Character Interactions wit...   \n",
       "18        11       Angular Momentum Guided Motion Concatenation   \n",
       "19       125  Interaction Patches for Multi-Character Animation   \n",
       "\n",
       "                                             Author/s  Date  \\\n",
       "0   He Wang  Edmond S L Ho  Hubert P H Shum  Zhanx...  2021   \n",
       "1   Qianhui Men  Edmond S L Ho  Hubert P H Shum  H...  2021   \n",
       "2   Qianhui Men  Hubert P H Shum  Edmond S L Ho  H...  2021   \n",
       "3   Jacky C P Chan  Hubert P H Shum  He Wang  Li Y...  2019   \n",
       "9   Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  ...  2015   \n",
       "10  Liuyang Zhou  Lifeng Shang  Hubert P H Shum  H...  2014   \n",
       "11  Edmond S L Ho  Hubert P H Shum  Yiuming Cheung...  2013   \n",
       "12  Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho ...  2013   \n",
       "14    Hubert P H Shum  Taku Komura  Shuntaro Yamazaki  2012   \n",
       "18        Hubert P H Shum  Taku Komura  Pranjul Yadav  2009   \n",
       "19  Hubert P H Shum  Taku Komura  Masashi Shiraish...  2008   \n",
       "\n",
       "                                              Journal  \\\n",
       "0   IEEE Transactions on Visualization and Compute...   \n",
       "1   IEEE Transactions on Circuits and Systems for ...   \n",
       "2                              Computers and Graphics   \n",
       "3               Computer Animation and Virtual Worlds   \n",
       "9                             Computer Graphics Forum   \n",
       "10              Computer Animation and Virtual Worlds   \n",
       "11                            Computer Graphics Forum   \n",
       "12              Computer Animation and Virtual Worlds   \n",
       "14  IEEE Transactions on Visualization and Compute...   \n",
       "18              Computer Animation and Virtual Worlds   \n",
       "19                       ACM Transactions on Graphics   \n",
       "\n",
       "                   Publisher  NumberOfPages  Impact Factor  \n",
       "0                       IEEE           12.0          4.579  \n",
       "1                       IEEE           16.0          4.685  \n",
       "2                   Elsevier           12.0          1.936  \n",
       "3   John Wiley and Sons Ltd.           20.0          1.020  \n",
       "9   John Wiley and Sons Ltd.           11.0          2.078  \n",
       "10  John Wiley and Sons Ltd.            9.0          1.020  \n",
       "11  John Wiley and Sons Ltd.           10.0          2.078  \n",
       "12  John Wiley and Sons Ltd.           10.0          1.020  \n",
       "14                      IEEE           12.0          4.579  \n",
       "18  John Wiley and Sons Ltd.           10.0          1.020  \n",
       "19                       ACM            8.0          5.414  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citationData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-8fe3e43b15ef>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  citationData['NumberOfPages'] = citationData['NumberOfPages'].astype('int')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Citation</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author/s</th>\n",
       "      <th>Date</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>NumberOfPages</th>\n",
       "      <th>Impact Factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>Spatio-temporal Manifold Learning for Human Mo...</td>\n",
       "      <td>He Wang  Edmond S L Ho  Hubert P H Shum  Zhanx...</td>\n",
       "      <td>2021</td>\n",
       "      <td>IEEE Transactions on Visualization and Compute...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>12</td>\n",
       "      <td>4.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>A Quadruple Diffusion Convolutional Recurrent ...</td>\n",
       "      <td>Qianhui Men  Edmond S L Ho  Hubert P H Shum  H...</td>\n",
       "      <td>2021</td>\n",
       "      <td>IEEE Transactions on Circuits and Systems for ...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>16</td>\n",
       "      <td>4.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>GAN-based Reactive Motion Synthesis with Class...</td>\n",
       "      <td>Qianhui Men  Hubert P H Shum  Edmond S L Ho  H...</td>\n",
       "      <td>2021</td>\n",
       "      <td>Computers and Graphics</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>12</td>\n",
       "      <td>1.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>A Generic Framework for Editing and Synthesizi...</td>\n",
       "      <td>Jacky C P Chan  Hubert P H Shum  He Wang  Li Y...</td>\n",
       "      <td>2019</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>20</td>\n",
       "      <td>1.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>Multi-layer Lattice Model for Real-Time Dynami...</td>\n",
       "      <td>Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  ...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Computer Graphics Forum</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>11</td>\n",
       "      <td>2.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>Human Motion Variation Synthesis with Multivar...</td>\n",
       "      <td>Liuyang Zhou  Lifeng Shang  Hubert P H Shum  H...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>9</td>\n",
       "      <td>1.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26</td>\n",
       "      <td>Topology Aware Data-Driven Inverse Kinematics</td>\n",
       "      <td>Edmond S L Ho  Hubert P H Shum  Yiuming Cheung...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Computer Graphics Forum</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>10</td>\n",
       "      <td>2.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Natural Preparation Behavior Synthesis</td>\n",
       "      <td>Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho ...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>10</td>\n",
       "      <td>1.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>68</td>\n",
       "      <td>Simulating Multiple Character Interactions wit...</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Shuntaro Yamazaki</td>\n",
       "      <td>2012</td>\n",
       "      <td>IEEE Transactions on Visualization and Compute...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>12</td>\n",
       "      <td>4.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11</td>\n",
       "      <td>Angular Momentum Guided Motion Concatenation</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Pranjul Yadav</td>\n",
       "      <td>2009</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>John Wiley and Sons Ltd.</td>\n",
       "      <td>10</td>\n",
       "      <td>1.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>125</td>\n",
       "      <td>Interaction Patches for Multi-Character Animation</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Masashi Shiraish...</td>\n",
       "      <td>2008</td>\n",
       "      <td>ACM Transactions on Graphics</td>\n",
       "      <td>ACM</td>\n",
       "      <td>8</td>\n",
       "      <td>5.414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Citation                                              Title  \\\n",
       "0         74  Spatio-temporal Manifold Learning for Human Mo...   \n",
       "1         13  A Quadruple Diffusion Convolutional Recurrent ...   \n",
       "2          0  GAN-based Reactive Motion Synthesis with Class...   \n",
       "3          6  A Generic Framework for Editing and Synthesizi...   \n",
       "9         14  Multi-layer Lattice Model for Real-Time Dynami...   \n",
       "10         6  Human Motion Variation Synthesis with Multivar...   \n",
       "11        26      Topology Aware Data-Driven Inverse Kinematics   \n",
       "12         1             Natural Preparation Behavior Synthesis   \n",
       "14        68  Simulating Multiple Character Interactions wit...   \n",
       "18        11       Angular Momentum Guided Motion Concatenation   \n",
       "19       125  Interaction Patches for Multi-Character Animation   \n",
       "\n",
       "                                             Author/s  Date  \\\n",
       "0   He Wang  Edmond S L Ho  Hubert P H Shum  Zhanx...  2021   \n",
       "1   Qianhui Men  Edmond S L Ho  Hubert P H Shum  H...  2021   \n",
       "2   Qianhui Men  Hubert P H Shum  Edmond S L Ho  H...  2021   \n",
       "3   Jacky C P Chan  Hubert P H Shum  He Wang  Li Y...  2019   \n",
       "9   Naoya Iwamoto  Hubert P H Shum  Longzhi Yang  ...  2015   \n",
       "10  Liuyang Zhou  Lifeng Shang  Hubert P H Shum  H...  2014   \n",
       "11  Edmond S L Ho  Hubert P H Shum  Yiuming Cheung...  2013   \n",
       "12  Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho ...  2013   \n",
       "14    Hubert P H Shum  Taku Komura  Shuntaro Yamazaki  2012   \n",
       "18        Hubert P H Shum  Taku Komura  Pranjul Yadav  2009   \n",
       "19  Hubert P H Shum  Taku Komura  Masashi Shiraish...  2008   \n",
       "\n",
       "                                              Journal  \\\n",
       "0   IEEE Transactions on Visualization and Compute...   \n",
       "1   IEEE Transactions on Circuits and Systems for ...   \n",
       "2                              Computers and Graphics   \n",
       "3               Computer Animation and Virtual Worlds   \n",
       "9                             Computer Graphics Forum   \n",
       "10              Computer Animation and Virtual Worlds   \n",
       "11                            Computer Graphics Forum   \n",
       "12              Computer Animation and Virtual Worlds   \n",
       "14  IEEE Transactions on Visualization and Compute...   \n",
       "18              Computer Animation and Virtual Worlds   \n",
       "19                       ACM Transactions on Graphics   \n",
       "\n",
       "                   Publisher  NumberOfPages  Impact Factor  \n",
       "0                       IEEE             12          4.579  \n",
       "1                       IEEE             16          4.685  \n",
       "2                   Elsevier             12          1.936  \n",
       "3   John Wiley and Sons Ltd.             20          1.020  \n",
       "9   John Wiley and Sons Ltd.             11          2.078  \n",
       "10  John Wiley and Sons Ltd.              9          1.020  \n",
       "11  John Wiley and Sons Ltd.             10          2.078  \n",
       "12  John Wiley and Sons Ltd.             10          1.020  \n",
       "14                      IEEE             12          4.579  \n",
       "18  John Wiley and Sons Ltd.             10          1.020  \n",
       "19                       ACM              8          5.414  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citationData['NumberOfPages'] = citationData['NumberOfPages'].astype('int')\n",
    "citationData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#citationDf = pd.get_dummies(citationData, columns=['Title', 'Author/s', 'Journal', 'Publisher' ], prefix='', prefix_sep='')\n",
    "#citationDf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#sns.lmplot(x=\"Impact Factor\", y=\"Citation\", hue=\"Publisher\", data=citationData);\n",
    "#sns.lmplot(x=\"Date\", y=\"Citation\", hue=\"Publisher\", data=citationData);\n",
    "#sns.lmplot(x=\"NumberOfPages\", y=\"Citation\", hue=\"Publisher\", data=citationData);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Citation</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author/s</th>\n",
       "      <th>Date</th>\n",
       "      <th>Journal</th>\n",
       "      <th>NumberOfPages</th>\n",
       "      <th>Impact Factor</th>\n",
       "      <th>ACM</th>\n",
       "      <th>Elsevier</th>\n",
       "      <th>IEEE</th>\n",
       "      <th>John Wiley and Sons Ltd.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26</td>\n",
       "      <td>Topology Aware Data-Driven Inverse Kinematics</td>\n",
       "      <td>Edmond S L Ho  Hubert P H Shum  Yiuming Cheung...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Computer Graphics Forum</td>\n",
       "      <td>10</td>\n",
       "      <td>2.078</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Natural Preparation Behavior Synthesis</td>\n",
       "      <td>Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho ...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>10</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>68</td>\n",
       "      <td>Simulating Multiple Character Interactions wit...</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Shuntaro Yamazaki</td>\n",
       "      <td>2012</td>\n",
       "      <td>IEEE Transactions on Visualization and Compute...</td>\n",
       "      <td>12</td>\n",
       "      <td>4.579</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11</td>\n",
       "      <td>Angular Momentum Guided Motion Concatenation</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Pranjul Yadav</td>\n",
       "      <td>2009</td>\n",
       "      <td>Computer Animation and Virtual Worlds</td>\n",
       "      <td>10</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>125</td>\n",
       "      <td>Interaction Patches for Multi-Character Animation</td>\n",
       "      <td>Hubert P H Shum  Taku Komura  Masashi Shiraish...</td>\n",
       "      <td>2008</td>\n",
       "      <td>ACM Transactions on Graphics</td>\n",
       "      <td>8</td>\n",
       "      <td>5.414</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Citation                                              Title  \\\n",
       "11        26      Topology Aware Data-Driven Inverse Kinematics   \n",
       "12         1             Natural Preparation Behavior Synthesis   \n",
       "14        68  Simulating Multiple Character Interactions wit...   \n",
       "18        11       Angular Momentum Guided Motion Concatenation   \n",
       "19       125  Interaction Patches for Multi-Character Animation   \n",
       "\n",
       "                                             Author/s  Date  \\\n",
       "11  Edmond S L Ho  Hubert P H Shum  Yiuming Cheung...  2013   \n",
       "12  Hubert P H Shum  Ludovic Hoyet  Edmond S L Ho ...  2013   \n",
       "14    Hubert P H Shum  Taku Komura  Shuntaro Yamazaki  2012   \n",
       "18        Hubert P H Shum  Taku Komura  Pranjul Yadav  2009   \n",
       "19  Hubert P H Shum  Taku Komura  Masashi Shiraish...  2008   \n",
       "\n",
       "                                              Journal  NumberOfPages  \\\n",
       "11                            Computer Graphics Forum             10   \n",
       "12              Computer Animation and Virtual Worlds             10   \n",
       "14  IEEE Transactions on Visualization and Compute...             12   \n",
       "18              Computer Animation and Virtual Worlds             10   \n",
       "19                       ACM Transactions on Graphics              8   \n",
       "\n",
       "    Impact Factor  ACM  Elsevier  IEEE  John Wiley and Sons Ltd.  \n",
       "11          2.078    0         0     0                         1  \n",
       "12          1.020    0         0     0                         1  \n",
       "14          4.579    0         0     1                         0  \n",
       "18          1.020    0         0     0                         1  \n",
       "19          5.414    1         0     0                         0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citationData = pd.get_dummies(citationData, columns=['Publisher'], prefix='', prefix_sep='')\n",
    "citationData.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x7f83e3393820>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(citationData[['Citation', 'Date', 'NumberOfPages', 'Impact Factor']], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = citationData[ ['Date', 'NumberOfPages', 'Impact Factor', 'ACM', 'Elsevier', 'IEEE', 'John Wiley and Sons Ltd.']]\n",
    "y = citationData['Citation']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
