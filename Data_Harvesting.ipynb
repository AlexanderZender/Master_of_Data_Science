{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a9c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "############################################\n",
    "\n",
    "##############################\n",
    "#Get page and import libraries\n",
    "##############################\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "MainUrl = \"https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_characteranimation.htm\"\n",
    "page = requests.get(MainUrl)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02140f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the links through the divs holding images\n",
    "body = soup.find(\"body\", class_=\"PageDefault\")\n",
    "#div = body.find(\"div\", class_ =\"w3-container w3-cell w3-mobile w3-cell-top\")\n",
    "imageDiv = body.find(\"div\", class_ =\"ImgIconPublicationDiv\")\n",
    "url = imageDiv.find(\"a\")\n",
    "print(url)\n",
    "\n",
    "if (url != None):\n",
    "    urlPublication = url[\"href\"]\n",
    "    print(urlPublication)\n",
    "else:\n",
    "    print(\"Cannot find the link in the publication page!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55705a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variable which finds the body and the class we are interested in\n",
    "body = soup.find(\"body\", class_=\"PageDefault\")\n",
    "#Create empty List\n",
    "urlPublications = []\n",
    "#Define variable that finds the title of the movie(the section we are interested in) through tag and class\n",
    "imageDivs = body.find_all(\"div\", class_ =\"ImgIconPublicationDiv\")\n",
    "\n",
    "for imageDiv in imageDivs:\n",
    "    url = imageDiv.find(\"a\")\n",
    "    if (url != None):\n",
    "        urlPublications.append(url[\"href\"])\n",
    "        print(urlPublications[len(urlPublications)-1])\n",
    "    else:\n",
    "        print(\"Cannot find the link in the publication page!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1977d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the host name\n",
    "hostUrl = \"https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_characteranimation.htm\"\n",
    "urlWithoutHttps = hostUrl[8:] # \"www.imdb.com/chart/moviemeter/?sort=i\n",
    "iFirstSlash = urlWithoutHttps.find(\"/\")\n",
    "hostname = \"/hubert.shum/comp42315/\"\n",
    "urlFolder = hostUrl[:8] + urlWithoutHttps[:iFirstSlash]\n",
    "\n",
    "urlGlobal = []\n",
    "for urlPublication in urlPublications:\n",
    "    urlGlobal.append(urlFolder + hostname + urlPublication)\n",
    "    print(urlGlobal[len(urlGlobal)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4deef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in urlGlobal:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2567889",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#Title\n",
    "###################################\n",
    "#Define empty list for the titles\n",
    "Titles = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    #find the h1 (Only the titles are contained in H1 tags in this case) \n",
    "    titleH1 = soup.find(\"h1\")\n",
    "    title = titleH1.text\n",
    "    #print(title)\n",
    "    Titles.append(title)\n",
    "\n",
    "print(Titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c53713",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#Abstract\n",
    "###################################\n",
    "#Define empty list for the abstracts\n",
    "Abstracts = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    #find the abstract \n",
    "    abstractDiv = soup.find(\"div\", attrs = {\"style\": \"margin-left: var(--size-marginleft)\"})\n",
    "    #print(abstractDiv) # check this works\n",
    "    abstract = abstractDiv.p.text\n",
    "    #print(abstract)\n",
    "    Abstracts.append(abstract)\n",
    "print(Abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69f856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#Authors\n",
    "###################################\n",
    "#Define empty list for the authors\n",
    "Authors = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    #find the div where the authors are\n",
    "    authorDiv = soup.find(\"div\",  attrs = {\"style\": \"display:block;min-width:280px;max-width:300px;float:left;text-align:left;overflow:hidden;margin-left:var(--size-marginleft);\"} )\n",
    "    #print(authorDiv) #check this works\n",
    "    authors = authorDiv.p.text\n",
    "    #print(authors) check this works\n",
    "    authorEnding = authors.find(', \"')\n",
    "    listAuthors = authors[:authorEnding]\n",
    "    #print(listAuthors) #check this works #may have to change this later for question about authors, maybe need new line\n",
    "    authorsWithoutComma = listAuthors.replace(',', ' |')\n",
    "    #print(authorsWithoutComma) #check this works\n",
    "    authorList = authorsWithoutComma.replace('and', '|')\n",
    "    \n",
    "    # Removing punctuations in string using regex\n",
    "    import re\n",
    "    # initializing string\n",
    "    test_str = authorList\n",
    "    # Removing punctuations in string\n",
    "    # Using regex\n",
    "    author = re.sub(r'[^\\w\\s]', '', test_str)\n",
    "    Authors.append(author)\n",
    "  \n",
    "    print(Authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#Dates\n",
    "###################################\n",
    "#Define empty list for the dates\n",
    "Dates = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    #find the date\n",
    "    dateDiv = soup.find(\"div\",  attrs = {\"style\": \"display:block;min-width:280px;max-width:300px;float:left;text-align:left;overflow:hidden;margin-left:var(--size-marginleft);\"} )\n",
    "    #print(dateDiv) #check this works\n",
    "    dateWithDot = dateDiv.p.text.split()[-1]\n",
    "    #print(dateWithDot) #check this works\n",
    "    date = dateWithDot[:-1] #remove the fullstop\n",
    "    Dates.append(date)\n",
    "    print(Dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63483228",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#Journal\n",
    "###################################\n",
    "#Define empty list for the journals\n",
    "Journals = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    #find journal article- this is more complex need some if clauses and a list\n",
    "    journalDiv = soup.find(\"div\",  attrs = {\"style\": \"display:block;min-width:280px;max-width:300px;float:left;text-align:left;overflow:hidden;margin-left:var(--size-marginleft);\"} )\n",
    "    #print(journalDiv) #check this works\n",
    "    journals = journalDiv.em\n",
    "    #print(journals)\n",
    "    #remove em\n",
    "    if (journals != None):\n",
    "        journal = journals.text\n",
    "        journal = journal.replace(\"'08:\", \"\")\n",
    "        journal = journal.replace(\"'21:\", \"\")\n",
    "        journal = journal.replace(\"'17:\", \"\")\n",
    "        journal = journal.replace(\"'19:\", \"\")\n",
    "        journal = journal.replace(\"'12:\", \"\")\n",
    "        journal = journal.replace(\"'16:\", \"\")\n",
    "        journal = journal.replace(\"'13:\", \"\")\n",
    "        journal = journal.replace(\"'06:\", \"\")\n",
    "        journal = journal.replace(\"'07:\", \"\")\n",
    "        journal = journal.replace(\"preprint arXiv:2006.11620\", \"\")\n",
    "        Journals.append(journal)\n",
    "    else:\n",
    "        Journals.append(\"NA\")\n",
    "        #Journals.append(journal)\n",
    "print(Journals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0385c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#Volume\n",
    "###################################\n",
    "#Define empty list for the volumes\n",
    "Volumes = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    #Find VolumeNumber\n",
    "    journalDivNumber = soup.find(\"td\")\n",
    "    #print(journalDivNumber)\n",
    "    journalClass = journalDivNumber.find(\"p\", class_=\"TextSmallDefault\")\n",
    "    #print(journalClass)\n",
    "    journalDivText = journalClass.text\n",
    "    #print(journalDivText)\n",
    "    \n",
    "    import re\n",
    "    txt = journalDivText\n",
    "    x = re.findall(\"volume={\", txt)\n",
    "    if (x):\n",
    "        iBeginning = journalDivText.find(\"volume\")\n",
    "        iEnding = journalDivText.find(\"}, number\")\n",
    "        #print(journalDivText[iBeginning+8:iEnding])\n",
    "        jNumber= journalDivText[journalDivText.find(\"volume\")+8:journalDivText.find(\"}, number\")]\n",
    "        #print(jNumber)\n",
    "        split_string = jNumber.split(\"}\", 1)\n",
    "        substring = split_string[0]\n",
    "        #print(substring)\n",
    "        volNum = int(substring)\n",
    "        #print(volNum)\n",
    "        Volumes.append(volNum) \n",
    "    else:\n",
    "        Volumes.append(\"NA\")\n",
    "print(Volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#Publisher\n",
    "###################################\n",
    "#Define empty list for the publishers\n",
    "Publishers = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    #Find Publisher\n",
    "    journalDivPublisher = soup.find(\"td\")\n",
    "    #print(journalDivNumber)\n",
    "    journalPublisherClass = journalDivPublisher.find(\"p\", class_=\"TextSmallDefault\")\n",
    "    #print(journalClass)\n",
    "    publisherDivText = journalPublisherClass.text\n",
    "    #print(publisherDivText)\n",
    "    \n",
    "    import re\n",
    "    text = publisherDivText\n",
    "    i = re.findall(\"publisher={\", text)\n",
    "    \n",
    "    if (i):\n",
    "        iBeginning = publisherDivText.find(\"publisher\")\n",
    "        iEnding = publisherDivText.find(\"},}\")\n",
    "        #print(publisherDivText[iBeginning+8:iEnding])\n",
    "        jPublisher= publisherDivText[publisherDivText.find(\"publisher\")+11:publisherDivText.find(\"}},}\")]\n",
    "        #print(jPublisher)\n",
    "        split_string_pub = jPublisher.split(\"}\", 1)\n",
    "        substring_pub = split_string_pub[0]\n",
    "        #print(substring_pub)\n",
    "        Publishers.append(substring_pub)\n",
    "    else:\n",
    "        Publishers.append(\"NA\")\n",
    "print(Publishers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#Page numbers\n",
    "###################################\n",
    "#Define empty list for the page numbers\n",
    "PageNumbers = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    #Find Pages\n",
    "    journalDivpages = soup.find(\"td\")\n",
    "    #print(journalDivNumber)\n",
    "    journalpagesClass = journalDivpages.find(\"p\", class_=\"TextSmallDefault\")\n",
    "    #print(journalClass)\n",
    "    pagesDivText = journalpagesClass.text\n",
    "    #print(pagesDivText)\n",
    "    \n",
    "    import re\n",
    "    num = pagesDivText\n",
    "    k = re.findall(\"pages={\", num)\n",
    "    \n",
    "    if (k):\n",
    "        iBeginning = pagesDivText.find(\"pages={\")\n",
    "        iEnding = pagesDivText.find(\"}, \")\n",
    "        #print(publisherDivText[iBeginning+8:iEnding])\n",
    "        jpages = pagesDivText[pagesDivText.find(\"pages={\")+7:pagesDivText.find(\"}, \")]\n",
    "        #print(jpages)\n",
    "        split_string_pages = jpages.split(\"},\", 1)\n",
    "        substring_pages = split_string_pages[0]\n",
    "        #print(substring_pages)\n",
    "        PageNumbers.append(substring_pages)\n",
    "    else:\n",
    "        #print(\"NA\")\n",
    "        PageNumbers.append(\"NA\")\n",
    "print(PageNumbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b2d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#Page numbers\n",
    "###################################\n",
    "#Define empty list for the number of page numbers\n",
    "NumberOfPages = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    #Find Number of Pages\n",
    "    journalDivpagesNum = soup.find(\"td\")\n",
    "    #print(journalDivNumber)\n",
    "    journalpagesNumClass = journalDivpagesNum.find(\"p\", class_=\"TextSmallDefault\")\n",
    "    #print(journalClass)\n",
    "    pagesNumDivText = journalpagesNumClass.text\n",
    "    #print(pagesDivText)\n",
    "         \n",
    "        \n",
    "    import re\n",
    "    num = pagesNumDivText\n",
    "    l = re.findall(\"numpages={\", num)\n",
    "    \n",
    "    if (l):\n",
    "        iBeginning = pagesNumDivText.find(\"numpages={\")\n",
    "        iEnding = pagesNumDivText.find(\"}, \")\n",
    "        #print(publisherDivText[iBeginning+8:iEnding])\n",
    "        jpagesNum = pagesNumDivText[pagesNumDivText.find(\"numpages={\")+10:pagesDivText.find(\"}, \")]\n",
    "        #print(jpages)\n",
    "        split_string_pagesNum = jpagesNum.split(\"},\", 1)\n",
    "        substring_pagesNum = split_string_pagesNum[0]\n",
    "        #print(substring_pagesNum)\n",
    "        NumberOfPages.append(substring_pagesNum)\n",
    "    else:\n",
    "        NumberOfPages.append(\"NA\")\n",
    "print(NumberOfPages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a744ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#Citation\n",
    "###################################\n",
    "#Define empty list for the citations\n",
    "Citations = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    citationSpan = soup.find(\"span\", class_=\"TextHighlightDefault\")\n",
    "    #print(citationSpan)\n",
    "    \n",
    "    import re\n",
    "    citationtext = citationSpan.text\n",
    "    #print(citationtext)\n",
    "    x = re.findall(\"Citation:\", citationtext)\n",
    "    \n",
    "    if (x):\n",
    "        iBeginning = citationtext.find(\"Citation:\")\n",
    "        iEnding = citationtext.find(\"##\")\n",
    "        #print(journalDivText[iBeginning+8:iEnding])\n",
    "        cite = citationtext[citationtext.find(\"Citation:\"):citationtext.find(\"##\")]\n",
    "        #print(cite)\n",
    "        split_string = cite.split(\": \", 1)\n",
    "        substring = split_string[1]\n",
    "        #print(substring)\n",
    "        Citations.append(substring)\n",
    "    else:\n",
    "        Citations.append(\"NA\")\n",
    "print(Citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24340708",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#Impact Factor\n",
    "###################################\n",
    "#Define empty list for the impact factor\n",
    "ImpactFactors = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    impactSpan = soup.find(\"span\", class_=\"TextHighlightDefault\")\n",
    "    #print(impactSpan)\n",
    "    \n",
    "    import re\n",
    "    impactText = impactSpan.text\n",
    "    #print(impactText)\n",
    "    x = re.findall(\"Impact Factor\", impactText)\n",
    "    \n",
    "    if (x):\n",
    "        iBeginning = impactText.find(\"Impact Factor\")\n",
    "        iEnding = impactText.find(\"#\")\n",
    "        #print(journalDivText[iBeginning+8:iEnding])\n",
    "        impact = impactText[impactText.find(\"Impact Factor\"):impactText.find(\"#\")]\n",
    "        #print(impact)\n",
    "        split_string = impact.split(\": \", 2)\n",
    "        substring = split_string[1]\n",
    "        #print(substring) \n",
    "        ImpactFactors.append(substring)\n",
    "    else:\n",
    "        ImpactFactors.append(\"NA\")\n",
    "print(ImpactFactors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4278a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#Dois\n",
    "###################################\n",
    "#Define empty list for the dois\n",
    "Dois = []\n",
    "\n",
    "for link in urlGlobal:\n",
    "    # Getting the url upon each iteration\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    #Find Doi\n",
    "    journalDivdoi = soup.find(\"td\")\n",
    "    #print(journalDivNumber)\n",
    "    journaldoiClass = journalDivdoi.find(\"p\", class_=\"TextSmallDefault\")\n",
    "    #print(journalClass)\n",
    "    doiDivText = journaldoiClass.text\n",
    "    #print(doiDivText)\n",
    "    \n",
    "    import re\n",
    "    d = doiDivText\n",
    "    i = re.findall(\"doi={\", d)\n",
    "    \n",
    "    if (i):\n",
    "        iBeginning = doiDivText.find(\"doi={\")\n",
    "        iEnding = doiDivText.find(\"}, \")\n",
    "        #print(publisherDivText[iBeginning+8:iEnding])\n",
    "        jdoi = doiDivText[doiDivText.find(\"doi={\")+5:doiDivText.find(\"}, \")]\n",
    "        #:doiDivText.find(\"}, \")\n",
    "        #print(jdoi)\n",
    "        split_string_doi = jdoi.split(\"},\", 1)\n",
    "        substring_doi = split_string_doi[0]\n",
    "        #print(substring_doi)\n",
    "        Dois.append(substring_doi)\n",
    "    else:\n",
    "        #print(\"NA\")\n",
    "        Dois.append(\"NA\")\n",
    "print(Dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff13fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin creating the data file by importing csv\n",
    "import csv\n",
    "#Lists to be used in the data file\n",
    "Title = Titles\n",
    "Abstract = Abstracts\n",
    "Author = Authors\n",
    "Date = Dates\n",
    "Journal = Journals\n",
    "Volume = Volumes\n",
    "Publisher = Publishers\n",
    "Pages = PageNumbers\n",
    "No_Pages = NumberOfPages\n",
    "Citation = Citations\n",
    "ImpactFactor = ImpactFactors\n",
    "DOI = Dois\n",
    "\n",
    "file = open(\"publicationData.csv\", \"w\", newline =\"\")\n",
    "writer = csv.writer(file)\n",
    "\n",
    "for w in range(25):\n",
    "\n",
    "    writer.writerow([Title[w],Abstract[w], Author[w], Date[w], Journal[w], Volume[w], Publisher[w], Pages[w], No_Pages[w], \n",
    "                     Citation[w], ImpactFactor[w], DOI[w]])\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cae676",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'publicationData.csv'\n",
    "column_names = ['Title', 'Abstract', 'Author/s', 'Date', 'Journal', 'Volume', 'Publisher', 'Pages', 'No.Pages',\n",
    "               'Citation', 'Impact Factor', 'DOI']\n",
    "\n",
    "raw_dataset = pd.read_csv(data, names=column_names,\n",
    "                          na_values='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4339054",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = raw_dataset.copy()\n",
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
